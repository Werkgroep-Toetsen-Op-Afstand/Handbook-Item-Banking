<<<<<<< HEAD
[["index.html", "Handbook Item Banking An itembank in 5 steps Part 1 Introductie 1.1 Inleiding 1.2 Definities en begrippen 1.3 In 5 stappen naar een itembank 1.4 Succesfactoren", " Handbook Item Banking An itembank in 5 steps SURF, Versnellingsplan Updated: 2022-01-28 Part 1 Introductie 1.1 Inleiding Digitale itembanken zijn een prachtige tool om de kwaliteit van toetsen te laten toenemen en de kosten te laten dalen. Wie gezamenlijk toetsvragen ontwikkelt, stemt doelen en processen af. Er is meer expertise en tijd beschikbaar per item (toetsvraag) voor ontwikkeling en kwaliteitscontrole. Dit alles komt de kwaliteit van de toetsen ten goede. Hergebruik van items kan de kosten doen dalen. De coronacrisis heeft daarnaast ook laten zien dat itembanken een rol spelen in situaties waarbij het online toetsen risico’s met zich meebrengt. Met name in als toetsvragen gaan uitlekken of dat studenten ongeoorloofd samenwerken tijdens het maken van de toets. Itembanken bieden de mogelijkheid om deze risico’s te verminderen. Het ontwikkelen, beheren en onderhouden van itembanken vraagt om goed project- management. Met deze uitgave willen we laten zien hoe je een digitaal systeem opzet om items uit te wisselen met collega’s van binnen en buiten de instelling. In dit hand- boek bundelen we de kennis en kunde die instellingen, samenwerkingsverbanden van instellingen en SURF over dit onderwerp hebben opgedaan. Het helpt instellingen op weg die hun voordeel willen behalen met digitale itembanken. Het handboek bestaat uit een introductie, een stappenplan en een verdiepend deel. 1.1.1 Voor wie is deze uitgave? Het handboek is bedoeld voor iedereen die een rol vervult in de processen rondom de ontwikkeling, het beheer en het onderhoud van een itembank en de items. Bijvoorbeeld: Docenten die items beter willen ordenen of willen ontwikkelen en uitwisselen met een collega Onderwijsadviseur die docenten ondersteunt of die trekker is van het inrichten van een itembank Een samenwerkingsverband dat gezamenlijk toetsmateriaal wil uitwisselen of gezamenlijk toetsen wil uitzetten Een landelijk overleg dat de kwaliteit van toetsen wil verhogen Functioneel beheerders van een toetssysteem dat wordt gebruikt voor een itembank 1.1.2 Scope Het handboek richt zich op het ontwikkelen, beheren en gebruiken van itembanken binnen een opleiding, binnen één instelling, instellingsoverstijgend, binnen een bepaald domein of zelfs landelijk. Uitgangspunt is dat de processen in de basis hetzelfde zijn. Het handboek geeft geen uitgebreide beschrijving van bestaande itembanken of toetssystemen. Hiervoor kun je terecht in de SURF thema-uitgave Toets-en vragenbanken in het onderwijs. Het ontwikkelen van itembanken kan onderdeel zijn van een groter implementatie- traject van digitaal toetsen bij instellingen. Voor het implementeren van digitaal toetsen verwijzen wij naar enkele SURF- publicaties, zoals de handreiking Beleid voor digitaal toetsen, het stappenplan Digitaal toetsbeleid schrijven en het werkboek Veilig toetsen. In dit handboek gaan we vooral in op het ontwikkelen van items (het itemontwikkelproces) en het beheer van items in een itembank. Een goede itembank vormt de basis voor goede toetsen. Over hoe je op basis van zo’n itembank goede toetsen ontwikkelt in een toetssysteem, zijn andere bronnen beschikbaar, zoals Toetsen in het hoger onderwijs Van Berkel, Bax, and Joosten-ten Brinke (2017). 1.1.3 Verantwoording Deze uitgave is een bijgewerkte versie van de uitgave uit 2018. De special interest group (SIG) Digitaal toetsen heeft de uitgave – mede in het licht van de ervaringen tijdens de coronacrisis van 2020/2021 – kritisch bekeken, aangevuld en geactualiseerd. De oorspronkelijke publicatie is tot stand gekomen in nauwe samenwerking met itembank-experts uit diverse instellingen, de SIG Digitaal Toetsen, Cito, 10voordeLeraar en Prove2Move. Het handboek is gebaseerd op de ervaringen van de betrokken experts met het ontwikkelen van itembanken. Verder hebben we gebruikgemaakt van verzameld materiaal van onder andere SURF en beschikbare literatuur op dit gebied. Zie het colofon voor een overzicht van alle betrokkenen die hebben bijgedragen aan dit handboek. Neem gerust contact op met de auteurs en/of de deskundigen die een inhoudelijke bijdrage hebben geleverd aan het handboek voor meer informatie. 1.2 Definities en begrippen Voor het gebruik van dit handboek is een goed begrip van een aantal termen essentieel. We beschrijven kort de componenten van een digitaal toetssysteem en laten zien hoe ze zich verhouden tot de itembank. 1.2.1 Wat is een itembank? Een itembank is een verzameling van items ofwel vragen (in het vervolg van dit handboek noemen we ze items). Een itembanksysteem is het digitale systeem waarin de verzameling van items is opgeslagen en waarmee ze beheerd kunnen worden. Een itembanksysteem kan meerdere itembanken bevatten. Als wij in dit werkboek spreken over itembank bedoelen we het itembanksysteem (de digitale itembank). Definitie Een itembank is een verzameling van items voor een bepaald toetsdoel. Een itembank heeft daarvoor een bepaalde ordening, meestal op basis van metadata en in de vorm van een hiërarchische structuur. Een itembanksysteem is het digitale systeem waarin de verzameling van items is opgeslagen en waarmee ze beheerd kunnen worden. Een itembanksysteem kan verscheidene itembanken bevatten. In de itembank worden de vragen logisch ingedeeld. Er vindt versiebeheer plaats en er kunnen allerlei metadata worden opgeslagen over de inhoud, het vraagtype, het niveauen over andere kenmerken. Je kunt er ook psychometrische gegevens in opslaan, zoals de moeilijkheidsgraad en het discriminerend vermogen van items. Soms draaien de programma’s van een itembanksysteem lokaal op een computer. Andere itembanken bestaan uit internetomgevingen. De database en de applicatie worden vanuit de cloud benaderd. Verschillende mensen kunnen tegelijkertijd werken in zo’n cloudsysteem. Nice to know Waarom worden ‘toetsvragen’ door toetsdeskundigen eigenlijk vaak items genoemd? In de jaren ‘30 werd er in de Verenigde Staten gewerkt met gestandaardiseerde toetsen en rudimentaire bestanden met vragen. Al snel ontstond een beroepsgroep met bijbehorend beroepsjargon. ’Test questions’ werd ‘Test items.’ In Nederland hebben we dit woordgebruik overgenomen en ‘vernederlandst’ naar ‘ietums’ in plaats van ‘aitums.’ 1.2.2 De plek van een itembank binnen het digitale toetssysteem Met een digitaal toetssysteem bedoelen we software die het digitale toetsproces faciliteert. We maken onderscheid tussen een aantal kerncomponenten: auteurs- omgeving, itembank, afspeelomgeving en analysetool. Deze componenten bespreken we aan de hand van de toetscyclus. Niet alle stappen uit de toetscyclus worden ondersteund door digitale toetssystemen. In figuur 1.1 is te zien hoe de componenten van een digitaal toetssysteem aansluiten op de toetscyclus. Figure 1.1: Componenten van digitale toetssystemen afgebeeld op de toetscyclus. De itembank vormt de centrale component waarin de items worden opgeslagen. De bewerking en het afspelen van de items vindt plaats in andere systemen, die vervolgens de resultaten teruggeven aan de itembank. Bij de volgende onderdelen van de toetscyclus speelt de itembank een rol: Het ontwerp van de toets (ook wel toetsspecificatie) gaat over aspecten als het doel van de toets, te toetsen onderdelen, de keuze voor formatief of summatief toetsen en ‘wijze van toetsing.’ In het ontwerp worden dus nog geen items gemaakt. Een toets wordt samengesteld vanuit een verzameling items. Hiervoor wordt meestal een specificatietabel gebruikt, ook wel toetsmatrijs of blauwdruk van de toets ge- naamd. Hierin is vastgelegd hoeveel items, welke vraagtypen en welke onderwerpen van welke aard, kennis, of toepassing in de toets komen. De ontwikkeling (constructie) van de items vindt plaats in de auteursomgeving van een toetssysteem. De antwoorden in geval van gesloten vragen worden hierin opgenomen en soms ook die van de open vragen. Toetssoftware biedt voor online toetsafname een afspeelomgeving aan. Deze omgeving controleert de identiteit van de student door middel van een inlogproces, biedt de student een toets aan en slaat de antwoorden van de student op. Het nakijken van de toets hangt af van de gekozen vraagtypen. Antwoorden van gesloten vragen worden doorgaans zonder menselijke tussenkomst beoordeeld en gescoord. De itemontwikkelaar (de docent) heeft hierbij vooraf het correcte antwoord ingegeven. Open vragen moeten worden nagekeken, waarbij systemen bijvoorbeeld ‘blind’ nakijken mogelijk maken en ‘vraag-voor-vraag’ nakijken, eventueel met behulp van ondersteunende software. Sommige softwarepakketten bevatten een analysetool voor toetsanalyse, maar analyseren kan ook met behulp van een download van een gegevensbestand en een bewerking hierna in bijvoorbeeld Excel of SPSS. Toetsanalyse is veelal gericht op het bepalen van de betrouwbaarheid van toetsen en het opsporen van items die van onvoldoende kwaliteit zijn. 1.2.3 Belangrijke begrippen Formatief toetsen: Toetsing waarbij het leren van de toets voorop staat, meestal zonder cijfers, vaak met feedback per toetsvraag of toets. Door met toetsopgaven aan de slag te gaan en te leren van fouten en feedback wordt het leerproces gestimuleerd. Er is geen sprake van een minimaal te behalen niveau. Psychometrische gegevens: Deze data zeggen iets over de moeilijkheidsgraad van vragen en geven aan in welke mate ze onderscheid maken of een student de stof wel of niet beheerst. Psychometrische gegevens kunnen worden opgeslagen in de itembank bij het item waarop ze betrekking hebben. Summatief toetsen: Toetsing waarbij het zo nauwkeurig mogelijk meten van een bepaald niveau van vaardigheid voorop staat. De behaalde score wordt gebruikt voor formele toekenning van een behaald studieresultaat, zoals studiepunten of een diploma. Toetskwaliteit: Het geheel van aspecten rondom toetsing dat de mate van transparantie, betrouwbaarheid en validiteit van toetsing behelst. Transparantie neemt bijvoorbeeld toe als er inzicht is in de wijze waarop items worden gemaakt. Betrouwbaarheid neemt bijvoorbeeld toe als items beter onderscheid maken tussen studenten met verschillende mate van beheersing van de stof of vaardigheid. Validiteit neemt toe als de items beter en vollediger de beoogde kennis of vaardigheid meten. Samenwerkingsverbanden In het handboek refereren we naar een aantal Nederlandse instellingsoverstijgende itembanken. We hebben dankbaar gebruik gemaakt van hun kennis en ervaring. Het gaat om de volgende samenwerkingsverbanden: deKennistoetsenbank MBO Voor de MBO-opleidingsprofielen Zorg en Welzijn, Maatschappelijk Werk en Verpleegkunde/Verzorgende IG heeft deKennistoetsenbank MBO van Prove2Move itembanken beschikbaar van duizenden items elk. DeKennistoetsenbank MBO is een methode-overstijgend digitaal systeem dat leren ondersteunt met goede kennisvragen. Prove2move is een coöperatie van de drie ROC’s (Landstede, Deltion College en ROC van Twente). IVTG De interuniversitaire VoortgangsToets Geneeskunde (iVTG) is een meetinstrument om de kennisprogressie van studenten geneeskunde gedurende hun studie te meten. Het betreft een interuniversitair samenwerkingsverband, bestaande uit zes UMC’s. 10voordeleraar Het programma 10voordeLeraar is onder regie van de Vereniging Hogescholen ontwikkeld en biedt landelijke kennisbases, verplichte kennistoetsen en een peer-review-systematiek voor alle lerarenopleidingen. Sharestats Het doel van dit project is om een vrij toegankelijk uitgebreide verzameling leermaterialen over statistiek te bieden. Door meta-datering van het leermateriaal kunnen docenten uit de vakcommunity naar behoefte een selectie van statistiekopgaven kiezen en gebruiken in het eigen onderwijs. De vijf partners van het projectteam (UvA Psychologie en Pedagogische en Onderwijswetenschappen, VU Sociale Wetenschappen (FSW), VU Gedrags- en Bewegingswetenschappen (FGB), UU Sociale Wetenschappen en EUR Psychologie/Pedagogiek) vormen de kern van de vakcommunity. 1.3 In 5 stappen naar een itembank In de praktijk ontstaan itembanken soms zonder bewust plan. Een uitprobeersel op kleine schaal groeit uit tot iets omvangrijks. Een dergelijke aanpak brengt het risico met zich mee dat de itembankontwikkeling in een later stadium stagneert, bijvoorbeeld door tijdgebrek of personele wisselingen. Het advies in deze is ‘bezint eer ge begint.’ Werk volgens beheerste stappen. Te snel gaan heeft mogelijk later consequenties voor het draagvlak of kan zelfs het project doen mislukken. Wij onderscheiden vijf stappen voor het ontwikkelen en gebruiken van een itembank. Binnen iedere stap moet je voor een aantal zaken zorgen. Deze zaken zullen soms gelijktijdig om aandacht vragen, of in een andere volgorde dan hier beschreven. In elke stap wordt het resultaat, een in gebruik genomen itembank, concreter gemaakt; er wordt van grof naar fijn gewerkt. Deel 2 van het handboek, het Stappenplan, neemt je mee door de verschillende stappen. Soms heb je daarvoor theoretische kennis nodig. Daarom verwijzen we regelmatig naar deel 3 van het handboek, de Verdieping. Dit deel bevat meer achtergrondinformatie voor het opzetten van een itembank. Tip Geen tijd om het hele handboek door te nemen? Hou dan in ieder geval de succesfactoren voor het ontwikkelen, beheren en gebruiken van een itembank uit hoofdstuk 4 bij de hand. 1.3.1 Stappenplan 1 Voorbereiding Formuleer het doel en onderzoek de haalbaarheid van de itembank. Oriënteer je op toetssystemen en uitwisselbaarheid. Verdiep je in omvang en vraagreproductie. Maak een eerste kosten- en batenanalyse. 2 Plan Maak een plan van aanpak, waarin je beschrijft: het doel; het resultaat (wat hebben we als we klaar zijn?); hoe je denkt dit te realiseren. 3 Ontwerp Orden de vragen. Organiseer de processtappen en bepaal rollen en rechten. Stel kwaliteitseisen vast en check of het systeem hieraan voldoet. Kies een toetssysteem. Leg afspraken over omvang en vraagproductie vast. Regel de financiën. Zorg voor een juridische verankering van het project. Organiseer het beheer van de itembank. 4 Pilot Ontwikkel de items. Richt de itembank in en probeer uit. 5 Realisatie Neem de itembank in gebruik. Zorg voor een toekomstbestendige itembank. 1.4 Succesfactoren Het ontwikkelen en gebruiken van een itembank kan complex zijn. De onderstaande succesfactoren leveren een belangrijke bijdrage aan de levensvatbaarheid van een itembank. Neem ze in acht, ook bij een relatief eenvoudige samenwerking. Deze factoren lopen als een rode draad door het handboek. Bezint eer ge begint: neem de tijd bij de start, schat de haalbaarheid goed in en vind de balans tussen een projectmatige aanpak en laten ontstaan. Kies een werkwijze die past bij jouw situatie. Stel een opdrachtgever aan en zorg dat het eigenaarschap van begin af aan goed belegd is. Zonder een opdrachtgever of afspraken over eigenaarschap is de kans op mislukking groot. Bepaal voor het ontwikkelen van de itembank welke stappen je wilt doorlopen en welke resultaten je per stap wilt bereiken. Bepaal het doel van je itembank en geef de itembank een duidelijke rol in het onderwijs. Houd hieraan vast. Draag deze visie regelmatig uit. Wees reëel en manage verwachtingen: de ideale itembank ontstaat niet in één dag en misschien zelfs wel helemaal nooit. De ideale inrichting kun je niet vanachter je bureau bedenken. Durf dingen uit te proberen. Kijk om je heen, er is al veel ontwikkeld. Vind het wiel niet opnieuw uit. Betrek deskundigen. Draagvlak is essentieel. Docenten die elkaar kennen en vertrouwen en die het doel van de itembank delen, zijn een randvoorwaarde voor succes; steek energie in het leren kennen van elkaar. Zorg voor goede begeleiding en ondersteuning van docenten. Het bestaan van een netwerk van mensen die al samenwerken en kennis uitwisselen vergroot de kans op succes aanzienlijk. Begin klein met de itembank en probeer verschillende inrichtingen en metadatamogelijkheden uit. Blijf aanpassen totdat de juiste inrichting en werkwijze is gevonden. Houd het eenvoudig en dicht bij de dagelijkse praktijk. Onderschat de aard en omvang van de werkzaamheden en/of het project niet. Het is moeilijke materie om te doorgronden. Gedrag en werkwijzen van mensen veranderen gaat niet vanzelf. Wees je bewust van technische beperkingen als je over instellingsgrenzen heen aan een itembank werkt. Literatuur "],["stappenplan-1.html", "Part 2 Stappenplan 2.1 Stap 1: Voorbereiding 2.2 Stap 2: Plan van aanpak 2.3 Stap 3: Ontwerp 2.4 Stap 4: Pilot 2.5 Stap 5: Realisatie", " Part 2 Stappenplan 2.1 Stap 1: Voorbereiding Begin met het formuleren van het doel van de itembank. Denk na over de rol van de itembank binnen het onderwijs, de mogelijke fasering en het eigenaarschap van de itembank. Wie heb je nodig? Hoe kun je stapsgewijs een itembank ontwikkelen die meerwaarde heeft voor het onderwijs en langere periode ‘in de lucht’ blijft? Zie ook H3.1. Ga na of het ontwikkelen van een itembank past binnen het beleid van de instelling en of het de onderwijsvisie ondersteunt. Past het binnen het digitale toetsbeleid? Welke kwalitatieve en kwantitatieve doelen wil jouw instelling met digitaal toetsen bereiken? Zorg dat je het doel van jouw itembank laat passen binnen dit beleid. Dit maakt de itembank kansrijker. Om een idee uit te werken zal er commitment nodig zijn in de vorm van tijd en geld. Iedere instelling regelt dit anders. Vaak gaat het om een bestuurlijk traject, waarbij diverse mensen praten/lobbyen op operationeel, tactisch en strategisch niveau. Er zijn meestal richtlijnen en procedures beschikbaar voor het opstarten van een project binnen een instelling. Denk bijvoorbeeld aan innovatiegelden die via onderwijsprijzen of stimuleringsmaatregelen beschikbaar kunnen komen. Mogelijk moet je bijvoorbeeld een startnotitie of projectopdracht formuleren om de middelen te regelen. Het kan zijn dat er al wordt gevraagd naar een initiële inschatting van de kosten ten opzichte van de baten. Het is aan te raden om tijdens de voorbereiding ten minste de volgende zaken te bepalen en te beschrijven: De huidige situatie en de gewenste situatie. Het doel van de itembank. Waarom is het een goed idee? Wat hebben studenten eraan? Maak duidelijk hoe de itembank zich verhoudt tot andere beleids- documenten, bijvoorbeeld het digitaal toetsbeleid en het onderwijsbeleid. Welk resultaat ligt er als het project klaar is? Hoe ziet het onderhoud en beheer er globaal uit? Vanuit welke theorie worden de items ontwikkeld? In de voorbereiding bedenk je op basis van welke theorie je aan de slag gaat: de klassieke testtheorie (KTT) of de item response theorie (IRT). Welke vraagtypes wil je gebruiken en hoe wordt er met feedback omgegaan? Keuzes over feedback en vraagtypes zijn van invloed op het eindresultaat. Zie ook § 3.6.4 en § 3.7.3. Hoeveel tijd, geld en doorlooptijd is er nodig voor dit project? Bepaal de haalbaarheid van het project: voer eventueel een apart haalbaarheidsonderzoek of vooronderzoek uit. Bepaal wie de opdrachtgever en/of de uiteindelijke eigenaar van de itembank is. Stel vast wie je nodig hebt om de itembank te ontwikkelen. Zie ook H3.5. 2.1.1 Haalbaarheidsonderzoek Een mogelijke tussenstap in de voorbereiding van je itembankproject is een onder- zoek naar de haalbaarheid ervan. Hierin werk je één of meerdere opties voor het inrichten van een itembank uit. Kies welke optie het beste is voor jouw situatie. Kijk naar de volgende aspecten: Inhoud: welke overeenkomsten zijn er tussen de items die je in de itembank wilt opnemen? Is er een gezamenlijke ordening te ontdekken? In hoeverre denken de docenten dat zij consensus kunnen bereiken over de inhoud van het vak en de items? Je moet dit weten om je aanpak te bepalen. Bij uiteenlopende meningen is er meer tijd nodig voor dit onderdeel. Zie ook H2.3. Kwaliteit: wat is de kwaliteit van de toetsen eigenlijk op dit moment? Wat zijn de p-waarden en rit-waarden? Hoe acceptabel is dit? Welke verbeterslag willen we realiseren met de itembank? De keuze voor het kopen of zelf ontwikkelen van de items. Wat is er al op de markt aan itembanken? Check oplossingen buiten de instelling, bijvoorbeeld het inkopen van items van een externe commerciële partij. Is er een mogelijkheid om aan te sluiten bij een bestaand samenwerkingsverband dat al een itembank ontwikkelt? De beoogde omvang van de itembank. Hoeveel items zijn er al binnen de instelling die kunnen worden gebruikt? Hoeveel items heb je naar verwachting nodig? Zie ook H3.3. Techniek (toetssystemen en itembanksystemen): Ga na wat er al beschikbaar is binnen de instelling aan itembanksystemen en itembanken die je kunt gebruiken. Zoek de afdeling op die verantwoordelijk is voor het beheer en gebruik van ICTO- applicaties en specifiek de toetsapplicaties. Moet er iets door de instelling worden aangeschaft? Zie ook H3.2. Organisatie: is er voldoende draagvlak voor de optie(s)? Ga in gesprek met organisaties die ervaring hebben met het inrichten van itembanken. Financiën: zet per optie de kosten en baten op een rij en neem de volgende zaken in acht. Kan er voldoende tijd en geld worden vrijgemaakt voor het ontwikkelen van de itembank? In hoeverre zal de itembank de prijs-kwaliteitverhouding van items kunnen verbeteren? Kun je projectgeld of subsidiegeld aantrekken? Zie ook H3.4. 2.1.1.1 Afronding voorbereidingsstap Verwerk alle gegevens tot een opdrachtformulering en verkrijg goedkeuring van je opdrachtgever om een plan van aanpak te maken. Wat heb je aan het eind van deze stap? Resultaat: inzicht in het doel, de haalbaarheid en de globale aanpak van het idee. Een go/no go van je opdrachtgever voor de volgende stap. 2.2 Stap 2: Plan van aanpak In deze stap leg je vast hoe de itembank eruitziet aan het eind van het project. Je maakt een plan van aanpak om dit resultaat te bereiken. Stel jezelf de vraag: wat hebben we als we klaar zijn? Het antwoord is afhankelijk van het doel dat de itembank zal vervullen in het onderwijs. Formuleer met de betrok- kenen eisen en wensen ten aanzien van de itembank. Eisen moeten verplicht worden gerealiseerd; wensen wil men graag realiseren, maar zijn niet noodzakelijk om tot een goed resultaat te komen. In je plan van aanpak zet je beide op een rij. Het kan zijn dat een organisatie projectmatig werken faciliteert en dat er wordt gewerkt aan de hand van een projectmanagement-methode, zoals Prince II. Mogelijk zijn er zelfs projectleiders beschikbaar voor het trekken van grotere projecten. In dat geval zijn er binnen de instelling procedures gangbaar rondom werkwijze, fasering en besluitvorming. Check wat er beschikbaar is binnen de instelling over projectmatig werken. Grijp waar mogelijk terug op standaarddocumenten, zoals een sjabloon voor een plan van aanpak. Geef de stappen uit het stappenplan hierin een plek. Zorg ervoor dat je plan van aanpak ten minste de volgende zaken zo gedetailleerd mogelijk beschrijft: Het doel en resultaat. Hoe je tot het resultaat wilt komen. De rol van opdrachtgever en wie dit op zich neemt. Idealiter is dit de toekomstige eigenaar van de itembank. Plan momenten in om met hem of haar de aanpak en plannen te bespreken. Stel vast waar je zijn of haar hulp nodig hebt. Bedenk wie je wanneer tijdens het project en erna nodig hebt. Beschikken zij over de juiste kennis en kunde? Denk aan de volgende expertises: opdrachtgever, pro- jectleider, toetsdeskundige, psychometrist, functioneel beheerder, jurist en docenten. Bepaal hoe je draagvlak creëert. Hoe ga je ervoor zorgen dat de itembank daad- werkelijk wordt gebruikt? Hoe kun je docenten en studenten ertoe aanzetten om de nieuwe werkwijzen in de praktijk te brengen? Ga hierover met ze in gesprek. Bedenk hoe je het belang van de itembank ook op de tactische en strategische agenda’s krijgt. Wie zijn de belanghebbenden? Kosten en baten: geef een inschatting van wat het ontwikkelen van de itembank kost en wat het de komende jaren zal opleveren. Kwaliteit: hoe kom je tot een itembank van voldoende kwaliteit? Planning: welke fasering heb je voor ogen? Welke doorlooptijd heb je nodig? Bedenk hoe je het gestelde doel behaalt en hoe je het proces om daar te komen kunt beheersen en zo nodig bijstellen. Wat heb je aan het eind van deze stap? Het resultaat van deze stap is een plan van aanpak, waarin het doel, het resultaat en hoe je dit denkt te realiseren zo concreet mogelijk zijn gedefinieerd. Laat het plan van aanpak goedkeuren door de opdrachtgever. Door het plan van aanpak is de benodigde tijd van mensen afgestemd met leidinggevenden. Zo zorg je voor commitment en draagvlak. 2.3 Stap 3: Ontwerp Aan het einde van deze stap ligt er een ontwerp dat aan de opdrachtgever duidelijk maakt wat hij krijgt. Daarbij weten de medewerkers van de volgende stap, het testen in een pilot, wat er moet worden gemaakt. In het ontwerp worden de volgende zaken bepaald en vastgelegd: Ordening van items Itemontwikkelproces Itemkwaliteit Systeem Productie en planning Financiën Juridische aandachtspunten Beheer Leg de afspraken die worden gemaakt per aandachtspunt duidelijk vast. De resultaten van elk onderdeel tellen op naar het totale ontwerp van de itembank en hangen dus ook met elkaar samen. 2.3.1 Ordening van items Breng in kaart in welke mate de vakgebieden overeenkomen. Laat de betrokken do- centen bepalen in hoeverre zij op dezelfde manier naar een vak of vakgebied kijken. Ze gaan met elkaar de diepte in om vast te stellen wat er op onderwerpniveau en vraag- constructie-niveau in de itembank wordt opgenomen. Deze fase is moeilijk, omdat er vaak verschillen zijn. Het is zaak om pragmatisch te blijven. Accepteer dat er mogelijk wat aanpassingen in het onderwijs noodzakelijk zijn, bijvoorbeeld door het kiezen van een nieuwe methode of het aanpassen van het gebruik van specifieke namen voor variabelen of concepten. Vragen die aan de orde komen: Hoe schrijf je de items uit? Wat vind je relevant of opleidingsspecifiek? Hoe zien de toetsmatrijzen eruit? Het kan zijn dat er verschillen zijn op notatie-of illustratieniveau. Hoe ga je hiermee om? Bepaal welke hiërarchische structuur de itembank krijgt en welke metadata wenselijk zijn. Zie ook H3.6. 2.3.2 Itemontwikkelproces Bepaal wie welke stappen in het itemontwikkelproces doorloopt om de toetskwaliteit te borgen. Stel een indicatie op voor de verwachte ontwikkeltijd per item. Kijk naar de huidige processtappen. Scherp ze indien nodig aan of breid ze uit. Moeten al bestaande items opnieuw bekeken worden en worden gekeurd? Hoeveel? Zie ook H3.5. 2.3.3 Itemkwaliteit Bepaal en documenteer vuistregels waaraan de items moeten voldoen. Denk ook aan regels over taalgebruik en bronvermelding. Betrek waar mogelijk item-experts (niet iedereen maakt goede items) of een psychometricus. Plan review-begeleidingssessies onder leiding van een vraagexpert. Laat docenten een flink aantal bestaande voorbeelditems aanleveren. Bespreek gezamenlijk de variatie tussen de vakken, de wijze van vraagstelling en de constructiefouten in deze items. Hierdoor krijgen de betrokkenen ideeën over uniformering van vraagformulering en vraagvormen. Bedenk hoe je de gewenste kwaliteitsverbetering kunt meten. Begin met het einde in gedachten: inventariseer welke gegevens je uit de itembank wilt kunnen halen ten behoeve van het onderwijskundige proces. Zie ook H3.7. 2.3.4 Systeem Bij een eenvoudig itembankproject: bekijk welk toetssysteem de instelling gebruikt. Ondersteunt het systeem het gewenste itemontwikkelproces, de gewenste rollen en rechten, de hiërarchische structuur en metadata? Bij een complex itembankproject: indien er meerdere systemen in gebruik zijn, maak een afweging welk systeem het meest geschikt is voor het doel. Past het gewenste itemontwikkelproces in het systeem? Dan kun je bepalen hoeveel werk het is om in te richten. Past het niet, bepaal dan of je het proces kunt aanpassen, of dat je liever onderzoekt of de structuur van het systeem kan worden aangepast. Geef aan wat er nodig is om de veiligheid van de items in de bank te waarborgen. Hoe schaalbaar moet het systeem zijn, nu en in de toekomst – voor zover je kunt overzien? Zie ook H3.2. 2.3.5 Productie en planning In de voorbereiding heb je grofweg bepaald hoeveel items je nodig hebt in de itembank. Zet dit aantal uit in tijd (korte en lange termijn). Hoeveel vraagt dit van medewerkers? Hoeveel items zijn al beschikbaar bij de verschillende partijen en kunnen met weinig moeite bij elkaar gebracht worden? Bedenk hoe je het vullen van de itembank aanpakt. Bepaal de periode, eventuele deadlines (wanneer vinden de eerste toetsen plaats?) en streefproductie door opschaling (batches). Bepaal wanneer je een kwaliteitsreview doet op basis van toetsstatistieken en psychometrische gegevens. Zie ook H3.3. 2.3.6 Financiën Werk de inschatting van de kosten en baten uit de voorgaande stappen verder uit. Bedenk hoe de initiële ontwikkeling zal worden gefinancierd. Stel vast hoe je de exploitatie van de itembank na afronding van het project gaat financieren. Stel een exploitatieplan op. Mogelijkheden om inkomsten te genereren zijn bijvoorbeeld: Instellingen nemen een abonnement op de itembank, om deze vervolgens kosteloos aan studenten aan te bieden. Studenten betalen voor toegang tot de itembank. Betrek de opdrachtgever van de itembank bij het opstellen van het budget voor exploitatie. Zorg dat hij of zij zich hiervoor verantwoordelijk voelt. Zie ook H3.4. 2.3.7 Juridische aandachtspunten Stel vast wie de eigenaar is van de items in de itembank. Bedenk hoe je wilt omgaan met materiaal van derden dat auteursrechtelijk beschermd is, zoals afbeeldingen bij items. Bepaal wat je nodig hebt op het gebied van de bescherming van persoons- gegevens. Overleg welke afspraken je over de samenwerking met betrokken partijen wilt vastleggen. Leg ten minste vast hoeveel uur wordt geleverd voor itemontwikkeling. Betrek een juridisch expert bij instellingsoverstijgende samenwerkingen om te beoordelen wat de beste samenwerkingsmogelijkheid is en wat hierbij komt kijken. Zie ook [H3.8](#juridische-aandachtspunten]. 2.3.8 Beheer itembank Bedenk hoe je de gebruikers, zoals docenten en studenten, het beste kunt ondersteunen om het eindproduct in gebruik te nemen. Ga hiervoor met hen in gesprek en luister naar argumenten. Laat hen juist bij het ontwerp meedenken. Bedenk wie of wat je nodig hebt om de itembank te beheren. Wie heb je nodig op het gebied van ondersteuning en beheer na het project? Maak instructiemateriaal en plan instructiemomenten. Ga in gesprek met de eventueel beschikbare toets- en beheerorganisatie in de instelling en stem de werkwijze af. Zie ook H3.5. Wat heb je aan het einde van deze stap? Een ontwerp van de itembank. Alle tussenresultaten per onderdeel tellen op tot het ontwerp van de itembank: Structuur: een landkaart/tabel met de onderwerpstructuur die is overeen- gekomen en afspraken omtrent bijvoorbeeld notatiewijzen. Organisatie: aanpak en gedocumenteerde organisatorische afspraken (beschrijving van de toets- en beheerorganisatie) over hoe de itembank zal worden beheerd, inclusief besturing van de itembank, inclusief benodigde kennis en kunde van betrokkenen (een opleidingsplan). Systeem: inrichtingsplan systeem; dit betreft workflow (van het itemontwikkel- proces) en de metastructuur (hiërarchische structuur en metadatastructuur). Kwaliteit: afspraken over de gewenste kwaliteit van de items, onder andere een beschrijving van richtlijnen en criteria waaraan de items moeten voldoen. beschrijving van het werkproces voor itemontwikkeling, inclusief beheer, analyse en de rollen en rechten. een indicatie van de tijd die het vergt om een item te ontwikkelen. een indicatie van de toename en consistentie van de itemkwaliteit. Productie en planning: aanpak en gedocumenteerde organisatorische afspraken hoe de itembank ‘te vullen’ en op niveau te houden. Financieel: een businesscase en exploitatieplan. Juridisch: vastgelegde afspraken over de juridische aspecten van de items en het systeem 2.4 Stap 4: Pilot Het ontwerp is klaar. Nu richt je de itembank in en maak je hem gereed voor gebruik. In deze stap wordt een itembankstructuur opgezet, metadata-categorieën ingevoerd, accounts aangemaakt, bestaande items geconverteerd, rollen en rechten verdeeld. Voer een pilot uit waarbij je een eerste toetsperiode volledig draait. Je test in de pilot of de keten van activiteiten werkt. Maak zoveel items dat je een eerste toetsperiode kan afdekken. Check of de gemaakte afspraken haalbaar zijn. Kloppen de uitgangs- punten? Weet iedereen welke rollen en rechten hij of zij heeft? Test of de systemen en de werkwijze technisch werken. En worden de items door studenten gebruikt zoals beoogd? Worden formatieve toetsen wel gebruikt? Evalueer en pas aan indien nodig. De volgende activiteiten vinden plaats in de pilot: Richt de itembank technisch in en test hem. Indien er sprake is van een conversie van oude items naar de nieuwe situatie: voer de conversie uit met een klein aantal items. Leid medewerkers op. Voer een eerste partij van items in op basis van de afgesproken werkwijze Stel een toets samen met de items. Bied de toets aan in een onderwijsonderdeel. Evalueer het gebruik en de resultaten van de toets. Zet een administratief systeem op voor personen en financiën (bijvoorbeeld als er is gekozen voor een vergoeding per geconstrueerde item of coördinatorschap). Evalueer het gebruik en de werkwijzen voor het aanmaken van items. Kan de kwaliteit beter onder controle worden gebracht? Kloppen de uitgangspunten, bijvoorbeeld de ingeschatte tijd voor vraagontwikkeling? Waar nodig: stuur bij, pas gemaakte afspraken aan, stel uitgangspunten bij. Wat heb je aan het eind van deze stap? Een gedocumenteerde evaluatie van de pilot op de diverse inrichtings- aspecten en vanuit het oogpunt van de gebruikers inclusief een verbeterplan. Een ingerichte en geteste itembank. 2.5 Stap 5: Realisatie In deze stap neem je de itembank in gebruik. Ook onderneem je actie om het gebruik van de itembank te laten toenemen. De pilot heeft een aantal lessen opgeleverd. Op basis hiervan kun je grootschaliger te werk gaan. Tijdens de realisatie worden grotere aantallen items gemaakt en worden de toetsen vaker ingezet. Aandachtspunten: Zorg dat de eigenaar van de itembank zich blijft bezig houden het volgen en bewaken van de doelstellingen en resultaten. Bespreek de borging regelmatig met de eigenaar van de itembank. Maken de itemontwikkelaars voldoende items? Zijn de items van goede kwaliteit? Hoe kun je zorgen voor zo min mogelijk variatie in kwaliteit? Is iedereen tevreden met het proces? Kan de docent gemakkelijk toetsen samenstellen? Vinden studenten de ontwikkelde formatieve items en feedback aantrekkelijk? Ga na of de businesscase haalbaar blijft. Zie ook H3.7. 2.5.1 Nazorg De itembank is geïmplementeerd. Het proces is in werking, iedereen vervult zijn taak en gebruikt de itembank op de afgesproken manier. Er is een formele eigenaar die het doel van de itembank in de gaten houdt. De itembank heeft een plek gekregen in de organisatie: er zijn mensen die ervoor zorgen dat de itembank blijft bestaan en goed wordt gebruikt. De onderwijskundig meerwaarde is duidelijk voor nu en in de toekomst. Aandachtspunten: Zorg dat het onderhoud van de itembank is belegd, bijvoorbeeld bij een functioneel beheerder en de redacteur(s). Zorg dat de financiële administratie functioneert zoals voorzien voor de exploitatiefase. Blijf in de gaten houden of de exploitatie een passende toekomstige begroting oplevert. Zorg voor goede documentatie, zoals: een beschrijving van de inzet van de itembank in de cursus. een beschrijving van het onderhoud van de itembank voor de toets- en beheerorganisatie. een beschrijving van het onderhoud van de itembank voor een eventueel coördinatieteam en voor elk auteursteam per itembank. Zie § 3.7.2.3. een standaard instructieprogramma voor gebruik van het itembanksysteem voor nieuwe medewerkers. aanpassingen aan het instructieprogramma bij updates van en aanpassingen aan het itembanksysteem. "],["verdieping.html", "Part 3 Verdieping 3.1 Waarom (samenwerken aan) een itembank? 3.2 Technologie: Systemen 3.3 Omvang van de itembank 3.4 Kosten en baten 3.5 Organisatie: processtappen, rollen en rechten 3.6 De ordening van een itembank 3.7 Didactiek: kwaliteit van items 3.8 Juridische aandachtspunten", " Part 3 Verdieping 3.1 Waarom (samenwerken aan) een itembank? Tijdens de voorbereiding leg je het doel van de itembank vast. In dit hoofdstuk beschrijven we een aantal mogelijke manieren waarop een itembank impact kan hebben op het onderwijs. We gaan ook nader in op de voordelen van samenwerken aan een itembank. Ten slotte bekijken we welke factoren binnen het vakgebied bijdragen aan een succesvol gezamenlijk itembankproject. 3.1.1 Waarom een itembank? Bepaal welke rol de itembank zal gaan spelen in het leer- en toetsproces. Vaak wordt een onderwijskundige impact nagestreefd met de inrichting van een itembank, zoals kwaliteitsverbetering of een verhoging van de efficiency. Voorbeelden van beoogde impact zijn: Psychometrisch: het verhogen van betrouwbaarheid en validiteit van toetsen. Verantwoording en transparantie: door de komst van de itembank komen er meer mogelijkheden om de toetskwaliteit transparant te verantwoorden. Er is onder andere goed zicht op de totale itembank en de match met de toetsmatrijs. Borging: een beter controleerbaar en minder foutgevoelig productieproces van items, bijvoorbeeld door controleerbaar versiebeheer van items. Verduurzaming: items worden docent-onafhankelijk beheerd in de itembank. Ze verdwijnen dus ook niet als een docent vertrekt. Toetsveiligheid: het is simpel om meerdere toetsversies te produceren en zo spieken tegen te gaan. Toetsveiligheid: het wordt gemakkelijker om toetsen op een veilige manier te ontwikkelen, bijvoorbeeld doordat items niet meer via e-mail worden doorgestuurd. Efficiency: door hergebruik van items nemen de kosten af. Items hoeven maar eenmaal te worden ingetypt, redactie en verbetering gaan veel sneller en items kunnen gemakkelijker worden geselecteerd voor opname in een toets. Onderwijskwaliteit: door hergebruik van items kunnen docenten meer tijd aan onderwijs besteden. Onderwijskwaliteit: er ontstaan meer oefenmogelijkheden en mogelijkheden voor niveaudifferentiatie. Onderwijskwaliteit: er kunnen snellere herkansings- en betere feedback- mogelijkheden worden gerealiseerd. Onderwijskwaliteit: het studiesucces wordt hoger, doordat studenten kunnen oefenen en de stof daardoor beter beheersen. Utiliteit: investeringen in al ontwikkelde items worden effectiever terugverdiend. 3.1.2 Waarom samenwerken aan een itembank? Zowel binnen de instellingen als tussen instellingen valt veel profijt te behalen uit samenwerking. Het gezamenlijk opzetten en gebruiken van itembanken kan zelfs op korte termijn leiden tot een aanzienlijke kwalitatieve en kwantitatieve winst. Hoe komt dat? Ten eerste moeten docenten die samen een itembank inrichten het gesprek aangaan in eenduidige bewoordingen. Ze bepalen samen hoe ze over itemontwikkeling spreken en welke minimale eisen ze stellen ten aanzien van kwaliteit. Dit is op zich al kwaliteitsverhogend. Ten tweede ligt bij samenwerking de inzet van toetsdeskundigen voor de hand. Zij kunnen een bijdrage leveren aan de toetsinhoud, maar ook op het gebied van bijvoorbeeld toets- kundigheid, redactie, opmaak. Ook dit zal de kwaliteit van items verhogen. Zie § 3.7.2 over verschillende rollen bij itemontwikkeling. Ten derde maak je bij samenwerking meer afspraken over het proces van itemontwikkeling. Dat betekent dat de itemontwikkelaars zich bewuster worden van de verschillende ontwikkelingsfasen, de verantwoordelijkheid en de borging van het proces. Als itemontwikkelaars zich aan deze afspraken houden, leidt dat tot kwaliteitsverhoging. Er is betere grip op kwaliteit doordat het proces is geborgd. Tot slot kan samenwerking zorgen voor lagere kosten. In eerste instantie zal een samenwerking meer kosten met zich meebrengen, maar als eenmaal ontwikkelde items vaker en door meer docenten worden ingezet en dus door meer studenten worden gebruikt, wordt het gebruik van de items per saldo goedkoper. Voor een verdere toelichting zie Hoofdstuk 3.4. 3.1.3 Kansrijke vakgebieden voor een succesvolle samenwerking Een gedeelde visie op het vakgebied kan eerder leiden tot een succesvolle itembank. Hoe meer mensen in een vakgebied het eens zijn over de indeling van de leerstof, hoe meer het voor de hand ligt om aan itembanken te werken. Vakgebieden met een gemeenschappelijke Body of Knowledge (BoK), zoals de lerarenopleidingen of het medisch domein, zijn hier voorbeelden van. Ook vakgebieden waarbij de meeste opleidingen inleidende cursussen verzorgen, zijn geschikt voor het inrichten van gezamenlijke itembanken. Denk bijvoorbeeld aan Inleiding Psychologie, Inleiding in de Sociologie, Inleiding Neurobiologie, en Inleiding programmeren. Stabiele vakgebieden lenen zich goed voor gezamenlijke itembanken die je met weinig onderhoud kan inrichten. Klassieke vakgebieden zoals logica, wiskunde, statistiek, mechanica, boekhouden en basiskennis microbiologie lenen zich goed voor een dergelijke samenwerking. Eenmaal ontwikkelde vragen verouderen niet of niet snel. Bij vakgebieden die zich gestaag ontwikkelen door nieuwe inzichten is het onderhoud van de items belangrijk. Verouderde items moeten worden opgespoord, verwijderd of aangepast. Dat proces moet systematisch worden ingericht om de kwaliteit van de vragenbank te kunnen borgen. Eenmaal per jaar gaat bijvoorbeeld de stofkam door alle items. In een samenwerkingsverband zijn hierover afspraken nodig. Voorbeelden van vakgebieden waarvoor dit geldt, zijn het medisch vakgebied, verpleegkunde, toerisme en geschiedenis. Samenwerking om kosten te delen en kwaliteit te verhogen, is hier een interessante driver. Vakgebieden die inhoudelijk voortdurend in ontwikkeling zijn (volatiele vakgebieden), vragen nog meer nadruk op de inrichting van het itembanksysteem en het borgings- proces. Denk aan vakken waarbij recente wetgeving altijd van belang is, zoals Rechten, waar steeds aanpassing aan protocollen plaatsvindt, zoals Verpleegkunde of waar thematisch wordt gewerkt. Er is continue aandacht nodig voor screening, aanpassing, verwijdering of aanvulling. Hiervoor moet een proces worden ingericht. In vakgebieden waarin veel studenten worden onderwezen, zijn itembanken ook geschikter om gezamenlijk aan te werken, omdat de investeringskosten en onderhoudskosten per student dan veel lager zijn. 3.2 Technologie: Systemen Tijdens de voorbereiding van het project doe je onderzoek naar het te gebruiken itembanksysteem. Bij een klein project is dit waarschijnlijk het centrale toetssysteem van de instelling. Zoek binnen de instelling de afdeling op die verantwoordelijk is voor het beheer en gebruik van ICTO-applicaties en specifiek de toetssystemen en bespreek je wensen en eisen ten aanzien van de itembank. Primair moeten itembanksystemen het mogelijk maken om: items goed te kunnen invoeren, ordenen, zoeken en selecteren aan kwaliteitsborging van de items te werken. Toetssystemen zijn er in vele soorten en maten en bieden een grote hoeveelheid functionaliteiten die tegemoetkomen aan deze eisen. De kerncomponenten van een digitaal toetssysteem zijn de auteursomgeving, de itembank, de afspeelomgeving en de analysetool. In Hoofdstuk 3.5 gaan we dieper in op de achterliggende processen van deze componenten. 3.2.1 Veel gebruikte toetssystemen voor onderwijs Deze paragraaf geeft een overzicht van de meest gebruikte toetssystemen voor het hoger onderwijs. De lijst is samengesteld door experts die betrokken zijn bij de ont- wikkeling van dit handboek. De markt van toetssysteem-leveranciers is dynamisch. Daarom is deze lijst niet compleet. Veel gebruikte toetssystemen in Nederland: TestVision RemindoToets MapleTA Cirrus Assessment QM iQualify Ans Delft Toetssystemen in Learning Management Systemen met beperkte itembankfunctionaliteit: Canvas Blackboard Desire2Learn BrightSpace Moodle Cumlaude Itembanksystemen (Stand Alone programma’s) met directe itemexportfunctionaliteit: Respondus ExamView Toetssystemen met vragen: SOWISO (Wiskunde) Grasple (Statistiek) Zeer Actieve Psychologie R/Exams (Statistiek) Drillster Quizlet ProProfs Itembanken van uitgeverijen, vaak behorende bij een specifiek studieboek of online leeromgeving (bijv. Pearson MyLab series, (McGraw Hill methoden, WebAssign e.d.) 3.2.2 Toetsitems uitwisselen tussen systemen Niet iedere onderwijsinstelling in Nederland gebruikt dezelfde toetssoftware. Hoe werk je dan samen aan itembanken? Vaak is het mogelijk om eenvoudige meerkeuzevragen te converteren van het ene naar het andere systeem. Instelling A maakt bijvoorbeeld de vragen en instellingen B en C converteren ze naar hun eigen toetssysteem. In het ideale geval werk je echter in dezelfde auteursomgeving, hanteer je een gezamenlijk itemontwikkelproces en vloeien alle gemaakte vragen en data over de gemaakte items terug naar één itembank. 3.2.2.1 QTI-standaard In de afgelopen jaren is onder meer in SURF-verband onderzoek gedaan naar de interoperabiliteit en uitwisselbaarheid van vragen tussen verschillende systemen. In internationaal verband wordt voortdurend gewerkt aan de IMS QTI-standaard (Question and Test Interoperability) voor de uitwisseling tussen toetssystemen. Deze standaard zal niet op korte termijn alle uitwisselingsvraagstukken oplossen, omdat sommige systemen vraagtypen hebben, die andere niet hebben, zoals de meerdere punten in een afbeelding aanwijzen of ongelijke-matchvragen. Tegelijkertijd werkt QTI al wel in veel gevallen voor multiple-choice toetsen. In de praktijk zien we dan ook dat een complexere samenwerking van meerdere instellingen voor de meeste deelnemende partijen betekent dat ze moeten werken in een ander systeem dan hun eigen instellingssysteem. Als je bij een samenwerking niet in één itembanksysteem kan werken, beperk dan het te gebruiken vraagtype tot de meerkeuzevraag. Deze vorm is zo eenvoudig dat uitwisseling via QTI of platte tekst meestal probleemloos verloopt. Ga ervan uit dat metadata en media (afbeeldingen, geluid) niet (goed) meekomen als je items ex- en importeert. Voer in de voorbereiding een proef uit met het uitwisselen van items. Zo kun je bepalen of het overzetten goed gaat of dat er conversiegereedschappen moeten worden gemaakt. Probeer zo veel mogelijk in hetzelfde systeem te werken, ook vanuit verschillende instellingen. 3.2.2.2 LTI en SCORM standaard In de toetswereld wordt vaak gewerkt met de standaarden IMS LTI (Learning Tools Interoperability standard) en SCORM (Sharable Content Object Reference Model). Dit zijn géén uitwisselstandaarden voor items. Deze standaarden worden gebruikt om toetssystemen te koppelen aan andere systemen, zoals digitale leeromgevingen (Leer-managementsystemen (LMS) zoals Blackboard of Canvas). Daarbij worden met name docent- en studentgegevens tussen de systemen automatisch uitgewisseld, kunnen toetsen vanuit het LMS worden gestart en worden de scores op toetsen teruggevoerd naar de cijferlijstfuncties in het LMS. Voor de eindgebruikers ontstaat een meer eenduidige beleving van het LMS en ze hoeven niet extra in te loggen. 3.2.3 Veiligheid en schaalbaarheid van itembanken Vooral summatieve items moeten in veilige omstandigheden worden ontwikkeld en opgeslagen in itembanken. Hoe hoger het belang van de toetsen, hoe meer aan- dacht er nodig is voor veiligheid. Ook tijdens de afname van toetsen wil je verspreiding van items tegengaan, vooral bij gekalibreerde vragen en kleine itembanken. Bij computerafname is het belangrijk om een afgeschermde omgeving te hebben (secure browser), maar ook goed te controleren op telefoons, brillen, horloges en andere apparatuur die foto’s kunnen maken van het scherm of van papier. Professionele surveillanten blijven noodzakelijk. Tegelijkerijd is tijdens de coronaperiode van 2020-2021 een enorme sprong gemaakt in toetsin via online proctoring. Met online proctoring wordt de student op afstand gemonitord door het verzamelen van beelden van de student via de webcam en van het scherm. Meer informatie is te vinden in het SURF werkboek Veilig toetsen. Schaalbaarheid hangt nauw samen met de gewenste omvang van de itembank. Houd rekening met het toevoegen van eventuele externe auteurs, de performance van de auteursomgeving, een mogelijke groei in het aantal vragen en toe te voegen onder- werpen en metadata. 3.3 Omvang van de itembank Hoe bepaal je het aantal items van een itembank? Tijdens de voorbereiding denk je al na over de benodigde omvang, omdat dit richting geeft aan de hoeveelheid tijd en middelen die nodig zijn om de bank te realiseren. Soms worden indrukwekkend aantal items genoemd, maar in feite is de benodigde omvang afhankelijk van veel factoren. In de eerste plaats moet je weten hoeveel items je in een toets wilt opnemen. Mag en kan je de items hergebruiken? Wil je de items in een summatieve of formatieve toets gebruiken? Dit kan verschil maken voor het benodigde aantal. In de volgende paragrafen schetsen we scenario’s voor verschillende toetsen waarvoor je de items wilt gebruiken. 3.3.1 Summatieve toetsen met een gelijktijdig toetsmoment voor alle studenten De meeste toetsen in het hoger onderwijs bestaan uit klassieke tentamens. Eén docent of team stelt een toets samen voor een groep kandidaten voor één cursus van één instelling. Alle studenten maken de toets op hetzelfde moment. Welke situatie treedt dan op? Toetsvragen vrijgegeven Toetsvragen niet vrijgegeven Stel dat de toetsvragen wel worden vrijgegeven na afloop van de toets, dat je drie verschillende varianten van de toets samenstelt en dat de toets uit vijftig items bestaat. Dan is een itembank nodig waarbij de omvang bij elke toetsafname moet groeien met 150 items. Na vijf jaar zitten er 750 items in de itembank. Instellingen leggen vaak vast in hun toetsbeleid wat wel en niet toegestaan is ten aanzien van het hergebruik van items. Stel dat de toetsvragen niet worden vrijgegeven na afloop van de toets, dat je drie verschillende varianten van de toets samenstelt en dat de toets uit vijftig items bestaat. Dan is een itembank van 150 items afdoende. Stel dat de toetsvragen wel worden vrijgegeven na afloop van de toets, dat er voor elke positie van een vraag in de toets random gekozen wordt voor een van de twee items voor die positie en dat de toets uit vijftig items bestaat. Dan is een itembank nodig waarbij de omvang bij elke toetsafname moet groeien met honderd items. Na vijf jaar heb je een itembank met 500 items (Draaijer and Klinkenberg 2015). Stel dat de toetsvragen niet worden vrijgegeven na afloop van de toets, dat er voor elke positie van een vraag in de toets willekeurig gekozen wordt voor een van de twee items voor die positie en dat de toets uit vijftig items bestaat. Een itembank van honderd items is afdoende. 3.3.2 Summatieve toetsen die op elk moment kunnen worden afgenomen Er zijn ook toetsen die worden afgenomen op aanvraag van de student. Daarbij bestaat het gevaar dat studenten items gaan verzamelen en met elkaar gaan delen. Of dat studenten zich vaker voor de toets inschrijven in de hoop dezelfde items te beantwoorden waarop ze dan het correcte antwoord weten. Over dergelijke toetsen zegt men soms dat het niet erg is dat de toetsvragen bekend zijn, als de bank maar groot genoeg is. Immers, als studenten kunnen oefenen met alle items, leren ze vanzelf de stof. Er zal echter ook een groep studenten proberen de correcte antwoorden letterlijk uit het hoofd te leren, waardoor er geen echte kennis of begrip wordt verworven. Als ze hierin slagen, maakt dit de toets minder valide. Hoeveel items er nodig zijn om dit tegen te gaan, is niet te zeggen. Itembanken van ten minste 1000 tot 2000 items lijken noodzakelijk. Het is in alle gevallen aan te raden studenten nieuwe vragen te laten beantwoorden. Een minimale variant is dat alternatieven uit meerkeuzevragen variëren van inhoud of van positie. Daarnaast zou gebruik kunnen worden gemaakt van geparametriseerde items. Dit zijn vragen waarbij getallen, objecten, concepten of begrippen worden getrokken uit een verzameling (Fattoh 2014). Bij de start van het itembankproject van de Open Universiteit gold de richtlijn dat er vijftien keer zoveel items in de bank moesten zitten als het aantal items in een toets. Deze eis was afgeleid van drie tentamenmogelijkheden per jaar en de aanname dat studenten binnen vijf jaar de opleiding zouden afronden. Voor docenten was het bijna onhaalbaar om zoveel items te maken, hoewel het voor ongeveer vijftig itembanken is gelukt. De OU heeft de programmatuur laten aanpassen, zodat studenten pas na vijf keer weer eenzelfde item voorgelegd konden krijgen. Daarmee kon de eis van vijftien keer verlaagd worden naar zes keer; een opluchting voor docenten. Een andere eis was dat de itembank na een jaar psychometrisch nuttige analyses kon opleveren. Elk item moest dan ten minste dertig keer beantwoord zijn. Voor sommige banken was echter de eis van vijftien keer al veel te hoog, bijvoorbeeld wanneer er maar tien studenten op jaarbasis het tentamen aflegden in een afstudeerrichting. 3.3.3 Formatieve diagnostische toetsen Formatieve diagnostische toetsen die het klassieke tentamen representeren, komen overeen met het eenmalig maken van één tot drie representatieve toetsen. Het maakt niet uit of de toetsvragen uitlekken. Het is immers de verantwoordelijkheid van de student zelf om al dan niet serieus een diagnostische toets te maken. Is de insteek dat studenten heel vaak een diagnostische toets kunnen afnemen? Dan lijkt de verstandigste keus om te bepalen hoeveel van dergelijke toetsen je wilt aanbieden. Indien er twee beschikbaar zijn, bekijken studenten doorgaans de eerste in het begin van de onderwijsperiode om het gewenste eindniveau in te schatten en de tweede net voor het summatieve tentamen om voor zichzelf te toetsen of ze voldoende beheersing hebben. Het aantal items die nodig is voor summatieve toetsen die op elk moment kunnen worden afgenomen is hier niet nodig. 3.3.4 Formatieve oefentoetsen Formatieve oefentoetsen vragen minder items per onderwerp. De student heeft vooral goede instructieve items nodig over belangrijke, moeilijke onderwerpen. Per onderwerp zijn wellicht een stuk of twintig items voldoende. Als een vakgebied uit twintig onderwerpen bestaat, zijn er ongeveer 400 items nodig. 3.4 Kosten en baten Tijdens de voorbereiding schat je de financiële haalbaarheid van je project in. Je maakt een globale inschatting van de te verwachten investeringskosten en exploitatiekosten van de itembank. Daarnaast kijk je naar de kwantitatieve en kwalitatieve voordelen of baten die de itembank mogelijk oplevert. Je probeert antwoord te vinden op de vragen ‘waarom investeren in een itembank?’ en ‘wegen de kosten op tegen de baten?’ Het uitwerken van een kosten-batenanalyse biedt hiervoor uitkomst. Bij grote investeringsplannen wordt steeds vaker een businesscase gevraagd. Een businesscase geeft antwoord op de vraag ‘waarom willen we dit eigenlijk?’ Het is een onderbouwing van toekomstig besluit en draagt eraan bij dat iedereen begrijpt waarom het project belangrijk is. Het ontwikkelen van itembanken kan onderdeel zijn van een groter implementatie- traject van digitaal toetsen bij instellingen. Hier ligt vaak een businesscase aan ten grondslag. De kosten en baten van een te ontwikkelen itembank is dan een van de onderdelen. 9 Voor meer informatie over de businesscase zie de thema-uitgave Digitale toets- en itembanken in het onderwijs, jan 2017 (hoofdstuk Kosten en baten van een itembank, bladzijde 12). 3.4.1 Kosten Voor de ontwikkeling van een itembank is een investering nodig. Dit zijn de project- kosten. Aansluitend heb je geld nodig voor exploitatie voor het doorontwikkelen en het beheer. 3.4.1.1 Projectkosten De initiële investering bestaat uit het creëren van een samenwerkingsverband, het ontwikkelen van een plan, het opzetten van de organisatie, het overleggen en onder- handelen over de inrichting van de bank en werkwijzen, het licenseren en opzetten van het beheer van een itembanksysteem, het ontwikkelen van een eerste aantal items voor de itembank en het afnemen van de eerste toetsen met de items uit de itembank. Voor het ontwikkelen van items worden verschillende tijdsbegrotingen aangehouden. Voor eenvoudige kennisvragen in meerkeuzevorm, zeker als het bovendien gemakkelijk is om afleiders te formuleren, kunnen er soms vier items per uur ontwikkeld worden. Vragen met casuïstiek of toepassing kosten doorgaans meer tijd, tot wel gemiddeld 60 minuten per vraag. Deze tijd kan nog oplopen als een item ook audiovisueel materiaal en feedback krijgt. Indien vragen meermaals gebruikt gaan worden, is het verstandig om de kwaliteit te bekijken na de eerste afname. De ervaring leert dat na de eerste afname soms wel de helft van de items nog aangepast moet worden. Dit kan gaan om een tekstuele aanpassing of een inhoudelijke. Bij de voortgangstoets geneeskunde zijn er behoorlijk wat mensen vooraf betrokken bij de ontwikkeling van een item. Ondanks deze zorgvuldigheid blijkt zo’n één procent van de items achteraf inhoudelijk niet te voldoen. Neem voor kosten de uren van de projectleden op en vermenigvuldig die met hun uurtarief. Neem kosten van externen op, net als kosten van middelen (zoals systeem, ruimtes, reiskosten, opleidingen, materialen). Neem ook een post onvoorzien op, die je kunt onderbouwen met mogelijke risico’s. Een aantal mogelijkheden om de ontwikkeling van complexere itembanken te financieren: Bij de ontwikkeling van de itembanken wordt ‘met gesloten beurzen’ gewerkt. Er worden afspraken gemaakt wie hoeveel items maakt en reviewt en in welke termijnen. Onderling worden dus geen rekeningen verstuurd. Itemontwikkelaars krijgen een vergoeding per geconstrueerd en goedgekeurd item. Dan maakt het niet zoveel uit welke docent of instelling de meeste items maakt. In dit geval zal er wel out-of-pocket-geld beschikbaar moeten zijn om item- ontwikkelaars te bekostigen. De coördinerende instelling of docent krijgt een aparte vergoeding, omdat de coördinator relatief meer tijd kwijt is en een grotere verantwoordelijkheid heeft. 3.4.1.2 Exploitatie Na afloop van het project moet de itembank worden onderhouden en door- ontwikkeld. Tijdens het project maak je een plan hoe dat eruit ziet. In dit zogenoemde exploitatieplan leg je onder andere vast dat medewerkers tijd krijgen om items te blijven ontwikkelen, de itembank in de lucht te houden en te beheren. Ga ervan uit dat eens in de zoveel jaar alle items worden nagelopen, bijvoorbeeld om ze te hercoderen vanwege curriculumwijzigingen, nieuwe boeken et cetera. Bedenk dat zonder een gezamenlijke itembank de kosten voor het ontwikkelen van items veelal niet transparant zijn. Het kost tijd en geld om elke keer op individuele basis items te maken, die mogelijk van variërende kwaliteit zijn. 3.4.2 Baten Wat zijn de baten van de itembank? In § 3.1.1 staat een opsomming van mogelijke vormen van impact van de itembank. Probeer die kwaliteitsdoelen zo te formuleren dat ze in financiële zin kunnen worden uitgedrukt. Vaak is dat niet goed mogelijk en komt het aan op de kracht van argumenten. Baten kunnen bestaan uit structurele exploitatie, waarbij financiering wordt geborgd door bijvoorbeeld lump-sum financiering, contributie of financiering op basis van toetsafname door studenten of docenten. Breng vooraf in kaart welke wijze van bekostiging je wilt hanteren. Zorg ervoor dat partijen zich al in de voorbereiding hieraan committeren. Baten kunnen ook bestaan uit lagere kosten en toegenomen kwaliteit. Eenmaal ontwikkelde items worden immers gebruikt door meerdere docenten en vooral door meerdere studenten. Dat betekent dat er weliswaar kosten zijn gemoeid met het ontwikkelen van een itembank, maar dat per saldo de verhouding tussen kosten en kwaliteit per afgenomen toets per student beter is. 3.5 Organisatie: processtappen, rollen en rechten In het ontwerp van de itembank organiseer je de ontwikkeling en het beheer. Je bepaalt wie welke taken uitvoert en hoe dat gebeurt. Inzicht in de globale processtap- pen die bij een itembank horen, kan helpen om deze taken op een rijtje te krijgen. 3.5.1 Componenten en processtappen Itembanksystemen bestaan vaak uit verschillende componenten met bijbehorende functies. Kerncomponenten zijn meestal een auteursomgeving, een itembank, een afspeelomgeving en een analysegereedschap. Binnen elk van die kerncomponenten zijn processtappen te onderscheiden. Neem de functionaliteiten van de itembank. Hierin onderscheiden we processen om items te kunnen invoeren, controleren, wijzigen, voorzien van metadata, ordenen en selecteren. Hieronder staat een uitwerking van een aantal processtappen die binnen de verschillende componenten van een toetssysteem kunnen plaatsvinden. Auteursomgeving Auteur schrijft item Review door collega reviewer Redactie door toetsexpert Specialist metadateert item Gatekeeper geeft items vrij Afname omgeving Toetsbureau zet toets klaar Analysetool Docent analyseert toets met toetsexpert 3.5.2 Rollen en rechten Wie mag bij welke informatie in de itembank? Wie mag wat veranderen of toevoegen? Dat bepaal je door het toekennen van rollen en rechten. In de meeste itembanksystemen liggen de mogelijke rollen en rechten vast. Je kunt er zelf personen aan koppelen. Sommige systemen maken het mogelijk om zelf rollen te definiëren. Bepaal bij voorkeur eerst welke rollen en rechten je wilt kunnen onderscheiden, voordat je een keuze maakt voor een systeem. Voorbeelden van mogelijke rollen: Beheer Super user: mag bij alle items en kan rechten instellen voor alle gebruikers. Coördinator: heeft toegang tot een deel van de itembank en een deel van de gebruikers en kan hier wijzigingen in aan brengen. Itemontwikkeling Auteur: mag items toevoegen en bewerken. Controleur: mag items wijzigen en van commentaar voorzien. Psychometrisch specialist: mag metadata toevoegen en bewerken en commentaar geven op items. Redacteur: zorgt voor controle op taal- en spelfouten, uniform taalgebruik en uniforme opmaak van items. Illustrator/multimediaspecialist: zorgt voor uniforme en goed bruikbare afbeeldingen en voegt deze in de items. Copyright controleur: zorgt dat gebruik van afbeeldingen en teksten juridisch gelegitimeerd is. Gatekeeper: zorgt voor een finale check van een item. Lid examencommissie of lid visitatiecommissie: kan kijken in de itembank om vast te stellen dat een eventueel itembankbeleid in een opleidingsplan wordt uitgevoerd. Externe validator: inspecteert de itembank als iemand van buiten het ontwikkelteam. Voorbeelden van verschillende rechten in een itembanksysteem: Toegang tot alle items. Toegang tot delen van de itembank. Leesrechten, bewerkrechten, verwijderrechten, verplaatsrechten, toevoegrechten, commentaarrechten, metadateringsrechten. Rechten om toetsen samen te stellen. Rechten om toetsen klaar te zetten voor afname. Rechten om toetsanalyses uit te voeren. Tip Wees spaarzaam met het toekennen van verschillende rollen en rechten. Hoe complexer het aantal rollen en rechten en de workflow, des te meer tijd en energie het redactieteam kwijt is aan de inrichting en het beheer hiervan. Een gebrek aan goed beheer zal de gebruiksvriendelijkheid verminderen en het proces van itemontwikkeling stagneren. 3.5.3 Geautomatiseerde workflow De huidige toetssystemen bieden in meer of mindere mate werkprocesondersteuning voor samenwerking aan items. Dit betreft het proces dat een item ondergaat van start tot gereed product en verwijdering. De inrichting van het werkproces voor itemontwikkeling ligt dus al grotendeels vast. Let er bij de keuze voor een systeem op dat meerdere personen tegelijkertijd kunnen werken in het systeem en gelijktijdig aan dezelfde items kunnen werken. In webgebaseerde toetssystemen is dit meestal standaard mogelijk. Via een systeem van rollen en rechten kunnen items sequentieel aan verschillende personen ter beschikking worden gesteld voor revisie, controle of accordering. Dit is de geautomatiseerde workflow. In een workflow kan worden vastgelegd dat een item pas naar een volgende fase kan als een voorgaande fase, bijvoorbeeld grammaticale controle, is doorlopen. In een toetssysteem kan van elk item een geschiedenis worden bijgehouden. Zo wordt inzichtelijk wie wanneer aan welk item heeft gewerkt. Als er fouten zijn ontstaan tijdens het proces, kan zo nodig een roll-back worden uitgevoerd. Tip Als de kwaliteit bij inzet al hoog is: richt een uitgebreider workflowproces in met diverse controlemomenten. Daarmee optimaliseer je de mogelijkheden om de kwaliteit van de items te verhogen. In § 3.7.2.2 lees je meer over gecontroleerde itemontwikkeling. 3.6 De ordening van een itembank Een goede ordening van een itembank geeft inzicht in de opbouw ervan en in de mate waarin deze geschikt is om toetsen mee samen te stellen. Ook helpt een goede ordening bij het efficiënt beheren van items. Tegelijkertijd wijst de praktijk uit dat ordening nooit een geheel sluitend systeem kan opleveren. Voor één specifieke toets is het wel mogelijk om een sluitend systeem te creëren. Gebruik je één itembank om items voor toetsen voor meerdere vakgebieden te selecteren, dan is het een vrijwel onmogelijke opgave. Wat is de ordening van een itembank? Het vastleggen van wat iedereen bedoelt met de term is van groot belang om spraakverwarring te voorkomen. Maak goed onder- scheid tussen de structuur van een itembank en metadata. In dit hoofdstuk bespreken we deze begrippen en indelingsprincipes. 3.6.1 Metadata Metadata zijn toegevoegde stukjes informatie aan items. Metadata van items zijn bijvoorbeeld het hoofdonderwerp van de vraag, een subonderwerp van de vraag, de status van het item (concept, ter revisie, ter redactiecontrole, goedgekeurd, afgekeurd), de moeilijkheidsgraad of het discriminerend vermogen, de soort vraag, de taxonomische aanduiding (bijvoorbeeld kennis, toepassing, inzicht), de functie van de vraag, wie het item heeft gemaakt of bewerkt en in welke toetsen het item wordt gebruikt. Figure 3.1: Een item met enkele metadatavelden ingevuld. 3.6.2 Structuur De structuur van een itembank is de opdeling van de itembank in onafhankelijke, vaak hiërarchisch geordende eenheden. Dit is vergelijkbaar met een mappenstructuur zoals je die vindt in de verkenner van een computer. Over het algemeen hebben gebruikers behoefte aan deze duidelijke hiërarchische structuur. Om inzicht te krijgen in de opbouw van een itembank, is deze structuur beter bruikbaar dan een structuur alleen gebaseerd op metadata. In de praktijk werkt het het beste om eerst de hiërarchische structuur te kiezen en dan op vraagniveau metadata toe te voegen. De structuur en metadata zijn vaak op verschillende wijzen technisch met elkaar verweven per itembanksysteem. Gebruikers kunnen de structuur en de metadata apart gebruiken voor indeling en selectie van items, maar soms ook in combinatie. Dit maakt het doordenken van structuur en metadata extra moeilijk. Enkele voorbeelden van bestaande itembankstructuren: Het VUmc heeft een aparte itembank binnen het itembanksysteem van Questionmark Perception. Figure 3.2: De hoofdstructuur van het VUmc volgt de afzonderlijke cursusafhankelijke toetsen (CAT) die worden afgenomen in het curriculum. Figure 3.3: De hoofdstructuur voor een itembank over statistiek van de Faculteit der Sociale Wetenschappen van de VU. Deze itembank bevindt zich in een cursus in Blackboard. De itembank volgt de onderwerpindeling binnen statistiek met verbijzondering naar Advanced/Basic en Moeilijkheidsgraad. DeKennistoetsenbank bevat itembanken voor gebruikers van Pedagogisch Werk (PW), Maatschap- pelijke Zorg (MZ) en Verpleegkundige en Verzorgende opleidingen MBO Zorg en Welzijn. Figure 3.4: Deze itembanken zijn ingedeeld naar: vraagtypen, de structuur van de Body of Knowledge (BoK), het Kwalificatie Dossier (KD) en bijzondere kenmerken. In de afbeelding is de inrichting van de itembank van Pedagogisch Werk (PW) weergegeven. Figure 3.5: Een docent van de cursus Inleiding Microbiologie heeft een itembank in QuestionmarkLive gemaakt. De indeling volgt exact de hoofdstukindeling van het studieboek dat hij gebruikt. Indeling vragenbank LUMC. In het itembanksysteem RemindoToets heeft het Leids Universitair Medisch Centrum (LUMC) een itembank. Figure 3.6: In eerste instantie koos het LUMC voor een themagerichte indeling op basis van disciplines uit de landelijke Voortgangstoets. Deze structuur leidde tot problemen vanwege de gebrekkige vind- baarheid van items voor docenten en het beheer van de vragen door docenten. Figure 3.7: Inmiddels is voor een meer pragmatische ordening naar opleidingen en onderwijs- onderdelen gekozen. Structuur van de itembank van Toets &amp; Leer. Figure 3.8: Iedere itembank binnen het itembanksysteem RemindoToets van Toets &amp; Leer is uit drie niveaus (categorieën) opgebouwd: Onderwerp &gt; Leerdoel &gt; Toetsterm. Hierdoor klap je slechts drie niveaus uit om een toetsvraag te zoeken, namelijk onderwerp, leerdoel en toetsterm. Het is de kunst geweest om de lijst met onderwerpen niet te algemeen, maar ook niet te gedetailleerd te maken. 3.6.3 Structuur en metadata gebruiken Hoe moet je de structuur en metadata van een specifieke itembank inrichten? In de eerste plaats helpt de inrichting het doel van de toetsen – die worden samengesteld op basis van de items – zo efficiënt mogelijk te realiseren. Hiervoor is met name de toetsmatrijs leidend. Is die niet aanwezig, neem dan als uitgangspunt een sequentie van items waarmee studenten kunnen oefenen. De structuur en de metadata helpen om de items te selecteren, vindbaar te maken en ‘in te laden’ in een toets. Ze maken het mogelijk om het grote aantal items zodanig te ordenen dat de gebruiker er goed mee kan werken en er bijvoorbeeld een toets met de gewenste eigenschappen mee kan samenstellen. Tip Voor metadata geldt het adagium: garbage in = garbage out. Als er te weinig tijd of geld is om metadata consequent en precies bij te houden, dan wordt de bruikbaarheid van de metadata teniet gedaan. Houd liever minder metadata bij en zorg dat ze van goede kwaliteit zijn. Een basisprincipe voor de inrichting van de structuur en de hoeveelheid metadata is het streven naar spaarzaamheid. Houd een complexiteit aan die net genoeg is om het doel te bereiken dat de toetsen dienen en de basisstappen voor kwaliteitscontrole te borgen. Ten tweede ondersteunt de inrichting van de metadata het proces van item- ontwikkeling en kwaliteitsborging. De metadata in de vorm van de processtappen voor ontwikkeling zijn dus van belang. Zie ook § 3.5.1. Hetzelfde geldt voor metadata waarmee je eventueel verouderde items kunt traceren of items afkomstig van een bepaalde auteur kunt selecteren. Op de derde plaats moet je de metadata zo inrichten dat ze de basis kunnen vormen voor zinvolle rapportages en feedback voor studenten. Is het doel van een specifieke toets dat studenten feedback krijgen over hoe zij per cel van een toetsmatrijs scoren? Schep hiervoor dan de mogelijkheid in de metadatastructuur. Is dit doel niet vooraf bepaald? Laat een dergelijke indeling achterwege. Tip Als in je vakgebied sprake is van een breed gedragen Body of Knowledge (BoK, kennisbasis) of (inter)nationale standaarden, zoals bijvoorbeeld CanMeds of bij de Interuniversitaire Voortgangstoets Geneeskunde (iVGT), dan ligt het voor de hand om deze indeling als hoofdstructuur te kiezen. Zeker als die structuur ook al hiërarchisch is geordend. Het gebruik ervan kan nog effectiever zijn als de structuur van het curriculum van een opleiding er ook nauw bij aansluit. Als het curriculum (vakindeling) er niet nauw bij aansluit, wat helaas vaak het geval is, levert het kiezen van de BoK als hoofdstructuur in de praktijk problemen op. Vaak is het dan efficiënter om als hoofdstructuur de curriculumindeling (vakken) te volgen en daar BoK-gegevens als metadata aan te koppelen. Figure 3.9: Een itembank met als hoofdstructuur een indeling van hoofdstukken. Vragen kunnen worden gefilterd op basis van verschillende categorieën van metadata. Tot slot, accepteer dat er verschillende itembanken met verschillende ordeningen en metadatastructuren binnen een enkel itembanksysteem worden gehanteerd. Binnen hogeronderwijsinstellingen leeft vaak de wens om één uniforme ordening en metadatastuctuur te realiseren voor alle items. De praktijk laat zien dat dit niet kan. Dat is niet zo vreemd: itembanken worden in het hoger onderwijs gebruikt voor verschillende doeleinden en vakgebieden en vereisen dus een andere ordening en metadatastructuur. Probeer wel om het aantal varianten te beperken. Tip Het ‘op papier’ verzinnen van een benodigde structuur om een bepaald toetsdoel te realiseren, is heel moeilijk. Het beste kun je deze structuur bedenken door een pilot uit te voeren. Ontwikkel een matrijs en een toets met behulp van het itembanksysteem. Itereer daarna naar een optimale oplossing voor de gegeven context en het doel. Zorg ervoor dat de gebruikers van het systeem zich bewust zijn van de voor- en nadelen van de gekozen indeling, zodat er voldoende draag- vlak is. Hier zit een ‘politiek’ aspect aan, omdat deelgebieden in een vakgebied graag hun deelgebied in de juiste ‘zwaarte’ vertegenwoordigd zien in een toets. Zeker als je toetsresultaten over een langere termijn met elkaar wilt vergelijken (zoals bijvoorbeeld bij de voortgangstoetsen in het medisch domein), is het belangrijk om de toetsmatrijs niet te vaak te veranderen. Tip Vaak willen docenten bij een item kunnen aangeven met welke andere items in de itembank het betreffende item niet gelijktijdig in een toets mag voorkomen. Het aantal mogelijkheden dat hiervoor wordt nagelopen, neemt snel toe als de itembank groeit. Dit vergt een voortdurende handmatige handeling. Het is eenvoudiger om te onderzoeken of de toetsen zo kunnen worden samengesteld en gefinetuned dat, na de eerste selectie van items, eventueel van elkaar afhankelijke items handmatig kunnen worden verwijderd. Tip Het is verstandig om elk item in een itembank van een unieke code te voorzien, die zelf de hiërarchische of metadatastructuur in zich herbergt. Items worden namelijk soms per ongeluk verplaatst. Na een toetsafname waarbij alleen ruwe data overblijft, maken de codes het terugvinden van individuele items in de bank weer mogelijk. Het unieke technische ID van een item op systeemniveau zegt vaak niet veel. Een voorbeeld van zo’n code: B2_AF12-3_RegcoefT is dan een item uit het Bachelor2-jaar over Hoofdstuk 12 uit het studieboek van Agresti en Finlay, met als onderwerp de regressiecoëfficiënt. Dit vergt wel uiterst precies werken door degene die items invoeren en controle- ren. Het onderstreept het belang van vindbaarheid en het gebruik van te veel of te weinig tags. 3.6.4 Feedback en vraagtypes Afhankelijk van het doel van je itembank maak je een keuze over het type items en de aard van de feedback van de items. 3.6.4.1 Feedback Voor formatieve toetsen is het ontwikkelen van feedback per item heel belangrijk. Feedback kan de student belangrijke studeeraanwijzingen geven. Het is verstandig om vroeg in het project na te gaan welke soort feedback je wilt ontwikkelen. Zo kun je een goede inschatting maken van de benodigde ontwikkeltijd per vraag. Vooral bij oefentoetsen is het belangrijk dat er bij iedere vraag inhoudelijke feed- back wordt gegeven. Feedback moet voldoende specifiek en gedetailleerd zijn en zo mogelijk studeeraanwijzingen voor de student bevatten of een verwijzing naar studiemateriaal om de stof opnieuw te bestuderen. Je kunt niet vooraf bedenken waarom studenten correcte dan wel incorrecte antwoorden kiezen. Noem die dan ook niet. Formuleer feedback zorgvuldig en bij voorkeur neutraal. Het mag studenten niet demotiveren, maar uitnodigen om verder te studeren. 3.6.4.2 Vraagtypes In een itembank kun je items maken op basis van verschillende vraagtypes. De meeste toets- en itembanksystemen ondersteunen meer dan tien types. Zie bijvoorbeeld dit overzicht van alle QTI-gedefinieerde vraagtypes. Zie ook § 3.2.2.1. Voor oefentoetsen is het gebruik van verschillende vraagtypes aan te raden. Dit maakt de toetsen aantrekkelijker. Met deze vraagtypes kun je bijvoorbeeld meerdere deelonderwerpen tegelijkertijd bevragen of gebruikmaken van de voordelen van computerafname, zoals het verslepen of aanklikken van objecten. Dat elke vraag kan resulteren in een andere score, maakt weinig uit voor oefentoetsen. Voor summatieve en diagnostische toetsen is vanuit statistisch oogpunt het gebruik van verschillende vraagtypes af te raden. Kies voor meerkeuzevragen én geef deze items allemaal een gelijk gewicht (bijv. 1 scorepunt). Deze keuze zorgt ervoor dat er geen problemen optreden met verschillende scoremogelijkheden of met de wijze van beantwoording. Dit zorgt voor meer flexibiliteit in het willekeurig kunnen trekken van items uit een itembank. Variëren in het aantal alternatieven (1-uit-3-, 1-uit-4- of 1-uit-5-vragen) levert over het algemeen geen problemen op. De scores vindt immers plaats op basis van 1 punt voor het juiste antwoord en 0 voor een afleider. Bij meerkeuzevragen wordt de een-uit-vijfvraag veel door Amerikanen gebruikt. De een-uit-viervraag en een-uit-drievraag worden in Nederland veel toegepast. De raadkans verandert bij deze varianten, maar soms is het lastig om afleiders te bedenken. Een-uit-viervragen zijn geschikt voor vakgebieden waar bepaalde antwoorden echt fout zijn. Denk hierbij aan exacte vakken zoals wiskunde, maar ook biologie. In vakgebieden waar verbanden belangrijk zijn of waar dingen niet snel fout zijn, is de een-uit-drievraag te verkiezen. Voorbeelden zijn communicatie en de meeste vakken van psychologie. Ga terughoudend om met items waarbij een casus wordt beschreven met daarbij deelvragen. Een dergelijk vraagtype maakt het moeilijker om flexibel items te trekken, omdat het aantal deelvragen kan verschillen per casus. Dat heeft invloed op de maximaal te behalen score. 3.7 Didactiek: kwaliteit van items Het verhogen van de kwaliteit van toetsing is meestal één van de belangrijkste redenen om gezamenlijk aan itembanken te werken. Zie § 3.1.2. De kwaliteit van toetsing neemt toe als de kwaliteit van de items toeneemt. Maar hoe zorgt een tool als een gezamenlijke itembank voor een hogere itemkwaliteit? Daarover gaat dit hoofdstuk. Er zijn veel definities voor de kwaliteit van toetsing (Joosten-ten Brinke and Draaijer 2014). De kwaliteit van een itembanksysteem betreft de mate waarin het systeem het mogelijk maakt om op een effectieve, efficiënte en prettige manier items te ontwikkelen, te beheren en te selecteren voor opname in een toets. De kwaliteit van de itembank neemt toe naar mate de items beter de leerdoelen uit de toetsmatrijs representeren. De kwaliteit neemt ook toe als er meer items in de itembank zitten die van hogere kwaliteit zijn. 3.7.1 Kwaliteit van items Wil je de kwaliteit van items verbeteren, focus je dan op twee soorten basiskwaliteit van toetsing: validiteit en betrouwbaarheid. Met behulp van itembanken kun je in beide een verbeterslag maken. Dit geldt zowel voor formatieve toetsing als voor summatieve toetsing. De validiteit van een toets gaat over de mate waarin een toets meet wat je beoogt te meten. Voor validiteit is het belangrijk dat de items relevant zijn voor een bepaald thema of leerdoel. Kwaliteitsverhoging treedt op wanneer docenten vaardiger worden in het maken van dit soort items. Training en samenwerking helpt daarbij. Itembank- systemen ondersteunen dit proces onder andere door mogelijkheden te bieden om direct commentaar te leveren, het faciliteren van samenwerking en de opzet van een verbetercyclus via een workflow. Zie § 3.5.1 en 3.5.2. De betrouwbaarheid gaat over de mate waarin het toetsresultaat (de score op een toets) reproduceerbaar is en zo min mogelijk afhangt van het toeval. Voor een hoge betrouwbaarheid moet de ‘ruis’ of ‘meetfout’ zo klein mogelijk zijn. De betrouwbaarheid van toetsen kan op de eerste plaats groter worden door meer items op te nemen in toetsen. Door gezamenlijk aan itembanken te werken vergroot je de voorraad aan items. Het aantal items waarmee een docent toetsen kan samenstellen, neemt toe. De betrouwbaarheid wordt ook groter door items te construeren die beter discrimineren tussen de studenten die voldoende en onvoldoende beheersing hebben van de studiestof. Beter discriminerende items verhogen over het algemeen ook de validiteit: je meet beter wat je wilt meten. Formuleer items in de eerste plaats zo helder, efficiënt en objectief mogelijk om beter te discrimineren. Er zijn veel handboeken en lijstjes met vuistregels om de meest bekende oorzaken voor onduidelijke items te voorkomen en daarmee de ruis. Zie bijvoorbeeld Van Berkel, Bax, and Joosten-ten Brinke (2017) Toetsen in het Hoger Onderwijs. 3.7.2 Hoe realiseer je een kwaliteitsverbetering? Je verhoogt de kwaliteit van items door fluctuatie in kwaliteit van items te verminderen en de vraagkwaliteit op een hoger niveau te krijgen. Dit inzicht komt voort uit het vakgebied van de operations management, zoals geïllustreerd in figuur 3.10. In het linker plaatje zie je dat de kwaliteit van een product of dienst gemiddeld vrij laag is, dan weer eens hoog en dan weer eens laag, maar niet stijgend. Het rechter- plaatje laat zien dat de kwaliteit gestaag toeneemt en de fluctuatie kleiner wordt. Om een dergelijke kwaliteitsverbetering te realiseren, is een doelgerichte benadering nodig. Figure 3.10: Grafische weergave van kwaliteitsontwikkeling. Links: de kwaliteit varieert heel erg, de gemiddelde kwaliteit is relatief laag. Rechts: de kwaliteit wordt constanter en de kwaliteit neemt systematisch toe. Er zijn twee manieren om de kwaliteit van nieuwe items te verhogen: ofwel door een grotere vaardigheid te ontwikkelen in het construeren van items, ofwel door review tijdens het constructieproces. Itembanksystemen bieden hiervoor workflow-onder- steuning. Zo werk je systematisch aan kwaliteitsverhoging. Om de kwaliteit van bestaande items in de itembank te verbeteren, kun je verder analyses uitvoeren na afname van toetsen en aanpassingen doorvoeren op basis van interpretaties van die analyses. Dit is in feite een cyclisch proces, dat onderdeel uit- maakt van het onderhoud en beheer van een itembank. We werken dit in de volgende paragrafen uit. 3.7.2.1 Voorlichting en training Om goede items te kunnen construeren zijn voorlichting, training en een goede feedbackloop in itemontwikkeling onontbeerlijk. Geef bij de start van een itembank training aan de beoogde itemontwikkelaars. Dit helpt om een gezamenlijk gevoel van kwaliteit te ontwikkelen en te laten groeien. Betrek toetsexperts bij de opzet van de training. Wanneer de itemontwikkeling eenmaal op gang is, kunnen meerdere trainingen volgen, bijvoorbeeld tijdens redactievergaderingen van het auteursteam. Zie ook § 3.7.2.3. Het diepgaand bespreken van items op basis van toets- en item- analyses is in feite impliciete training. Training is daarmee een continu terugkerend onderdeel van het onderhoud en beheer van de itembank. 3.7.2.2 Gecontroleerde itemontwikkeling door review Door een stapsgewijze itemontwikkeling met ingebouwde reviewmomenten kun je veelvoorkomende valkuilen of onduidelijkheden voortijdig signaleren en oplossen. Het welbekende vierogenprincipe is hiervoor bedoeld. Review vindt plaats door het inschakelen van vakgenoten of experts. Voor twee samenwerkende docenten hoef je het reviewproces en de verdeling van rollen niet uitgebreid te definiëren, zeker niet in technische zin. Tegenwoordig wordt wel geëist dat elk item in ieder geval éénmaal door een collega-docent is bekeken. Ook moet duidelijk zijn wie een item goedkeurt. Onderschat dit niet: de consequenties van een toets die door één docent is ontwikkeld zijn even groot als die van een landelijke toets. Een student die de toets niet heeft gehaald, kan niet afstuderen. Itembanken voor toetsen waarmee grote belangen gemoeid zijn vereisen meer rollen en rechten. Stel dat op basis van de itembank toetsen worden ontwikkeld voor een selectie of toetsen zonder herkansingsmogelijkheid. De stakeholders, zoals de politiek, het publiek, bestuurders en studentenbonden, stellen hoge eisen aan transparantie en de kwaliteit van toetsing. Denk bijvoorbeeld aan landelijke toetsen, zoals de PABO-rekentoets, de toets in het kader van 10voordeleraar of de centrale toetsing in het mbo. Er is een uitgebreid, transparant reviewproces nodig. Voor een beschrijving van mogelijke rollen in dit proces, zie § 3.5.1. Hoe omvangrijker het team van betrokkenen, des te belangrijker is de aansturing van teams en project- ondersteuning door een coördinator. 3.7.2.3 Itembankonderhoud en beheer Onderdeel van het onderhoud en beheer van de itembank is het bespreken van items die niet zo goed functioneren. Uit psychometrische analyse van deze items blijkt dat ze lage rit-waarden of hoge of lage proportiewaarden hebben. Voor een uitgebreide uitleg van deze termen, zie § 3.7.3.2. Probeer er gezamenlijk achter te komen wat de oorzaak is en wat je kunt doen om de items te verbeteren. Regelmatig zul je besluiten om items te verwijderen en geheel opnieuw te construeren. Deze besprekingen zijn wellicht aanleiding om de regels en richtlijnen voor items met het team aan te scherpen. Soms blijkt dat het probleem niet in de items, maar in het onderwijs zit. In dat geval zul je moeten bekijken hoe onderwijs en items beter op elkaar kunnen aansluiten. Het komt voor dat er een herbeoordeling van items nodig is of een aanpassing van de ordeningen. Er zijn bijvoorbeeld nieuwe inzichten in het vakgebied, wijzigingen in onderwerpen of leerdoelen of in de metadata en de toetsmatrijs. Je kunt je dan afvragen of er nieuwe sets van items voor nieuwe onderwerpen nodig zijn. Moeten items worden gehercodeerd? Zet gerichte ontwikkelopdrachten uit voor de itemontwikkelaars. Redactieteam Het team van itemontwikkelaars komt gedurende het ontwikkelproces regelmatig bij elkaar om de voortgang van de itemontwikkeling in de gaten te houden. Maak één persoon verantwoordelijk voor het itemontwikkelproces. Noem deze persoon de coördinator of hoofdredacteur. Kom met elkaar overeen dat deze persoon het mandaat heeft om mensen aan te spreken op de afspraken over het ontwikkelen van items. De hoofdredacteur plant regelmatig een overleg in met alle betrokkenen. Werk in deze overleggen aan het onderling vertrouwen en het bespreken van problemen rondom het proces van de itemontwikkeling. 3.7.3 Toetsgegevens Een krachtig middel om de itemkwaliteit te verbeteren, is het verzamelen van gegevens over items van afgenomen toetsen. In combinatie met de kwaliteitsverhoging door itembanken heb je een dubbele verbeterslag te pakken. Deze gegevens zeggen namelijk iets over de kwaliteit van de items en de mate waarin studenten presteren of over de kwaliteit van het onderwijs. Het is belangrijk om beschrijvende en psychometrische gegevens goed te onderscheiden. Zij hebben een heel verschillend karakter, dat moet passen bij het doel dat je nastreeft met de itembank. 3.7.3.1 Beschrijvende data Eenvoudige beschrijvende data van items gaan om zaken zoals het aantal keren dat een item is afgenomen of beantwoord. Op het niveau van een toets als geheel kunnen beschrijvende data gaan over hoe vaak een student een poging heeft ondernomen om een toets te maken, wanneer de poging is geweest, hoe de student scoort, wat de slagingspercentages zijn enzovoort. Deze data zijn afkomstig uit toetsservicesystemen. De data kun je vertalen naar een grafische overzichtspagina voor docenten of studenten. Docenten krijgen een overzicht van de progressie van een groep studenten en de onderwerpen waarmee ze moeite hebben. Studenten krijgen inzicht in hun progressie en hun positie ten opzichte van groepsgenoten. Dit levert in feite managementinformatie op. Gebruik van data op deze wijze noemt men ook wel learning analytics. 3.7.3.2 Psychometrische data Psychometrische gegevens zeggen iets over de moeilijkheidsgraad van items en in welke mate ze onderscheid maken of een student de stof wel of niet beheerst. Daarbij maakt het uit of je gebruikmaakt van de klassieke testtheorie (KTT) of de item-response-theorie (IRT). In deze paragraaf leggen we beide theorieën kort uit. 3.7.3.2.1 De klassieke testtheorie (KTT) De klassieke testtheorie gaat ervan uit dat de score op een toets bestaat uit de werkelijke score en een fout in de meting (ruis). Door statistische bewerkingen kan van één toets worden bepaald wat de betrouwbaarheid is van de toets, dat wil zeggen, in welke bandbreedte de werkelijke score van de student met een bepaalde mate van zekerheid ligt. De belangrijkste psychometrische gegevens voor items binnen het concept van de klassieke testtheorie zijn: Betrouwbaarheid: de mate waarin de toets als geheel, dus alle items gezamenlijk, goed onderscheid maken in de mate van beheersingsgraad van de stof. Het is een maat van meetnauwkeurigheid en de mate waarin de score op een toets niet aan het toeval is te wijten. Volgens de literatuur moeten tentamens ten minste een waarde van 0,7 of hoger hebben en bijvoorbeeld selectietoetsen een waarde van 0,9 of hoger. p-waarde: de proportie correcte antwoorden van de studentenpopulatie. De p-waarde wordt vaak aangeduid als de moeilijkheidsgraad van een item, maar is in feite een graad van gemakkelijkheid. Bij open items wordt een optimum van 0,5 en voor meerkeuzevragen met vier alternatieven is dit 0,67. Hier kan ook nog de a-waarde worden genoemd. Dat is de proportie studenten uit de groep die een bepaalde afleider heeft gekozen. Rit-waarde: de correlatie tussen de score van de studenten op de item en de score op de toets als geheel. De rit-waarde is een maat voor het discriminerend vermogen van een vraag. Dat is de mate waarin het item onderscheid maakt tussen de studenten die de stof beter of slechter beheersen. De rir-waarde is een wat strengere maat voor het discriminerend vermogen, omdat de invloed van het item zelf op het discriminerend vermogen wordt weggelaten. Volgens de literatuur ligt het streven op een rit-waarde van ten minste 0,3. Waarden lager dan 0,1 worden als slecht beschouwd. Negatieve waarden verdienen direct aandacht. Hier komt de rat-waarde van pas. Dat is de mate waarin de keuze voor een bepaald incorrect alternatief samenhangt met de toetsscore. Het kan waardevol zijn om psychometrische gegevens bij de items op te slaan. Sommige toetssystemen ondersteunen zelfs een automatische update van deze gegevens na iedere beantwoording door een student. Op zich aantrekkelijk, ware het niet dat ze alleen interpreteerbaar zijn per toetsafname. Onderling kun je ze alleen vergelijken als de afnamecondities identiek zijn. Psychometrische gegevens per toetsafname zijn wel zinnig. 3.7.3.2.2 De item-response-theorie (IRT) Wil je weten wat de meer absolute moeilijkheidsgraad en het discriminerend vermogen van items zijn, dan biedt de item-response-theorie (IRT) uitkomst. IRT levert zogenaamde gekalibreerde items op. Om die te verkrijgen, worden de items afgenomen bij een grote steekproef onder een groep studenten met veel spreiding in kennis en vaardigheid. In het kalibratieproces wordt heel precies gemeten hoe items onderscheid maken tussen studenten op specifieke niveaus van moeilijkheid. Itembanken met items die op deze manier zijn ontwikkeld kun je gebruiken in zogenaamde adaptieve toetssystemen. De achterliggende technieken zijn zo complex dat ze alleen met ruime ontwikkelbudgetten kunnen worden gemaakt. Voorbeelden hiervan zijn de PABO-rekentoets (Wiscat), de Rekentuin (primair onderwijs) en de in ontwikkeling zijnde computer-adaptieve versie van de iVTG (interuniversitaire voortgangstoets geneeskunde). Ook geldt dat de items aan strengere eisen moeten voldoen dan bij klassieke toetsen, vooral voor wat betreft discriminerend vermogen (Linden, Linden, and Glas 2000). Soms kiezen docenten in de voorbereiding voor adaptief toetsen, zonder zich voldoende te realiseren dat hiervoor complexe en kostbare IRT-technieken nodig zijn. Kies hier alleen voor als de projectgelden toereikend zijn. Voor de ordening van de itembank heeft de keuze voor IRT niet zo heel veel consequenties. Het maken van een adaptieve itembank vergt meer kennis en middelen om de kalibratie uit te kunnen voeren, maar het resultaat kan worden opgenomen in een ‘gewone’ itembank. Er zijn voldoende items nodig, verspreid over de verschillende moeilijkheidsniveaus, met meer moeilijke items dan items met een gemiddelde moeilijkheidsgraad. Voor een wat uitgebreidere handzame uitleg van de klassieke testtheorie en item-respons theorie, zie De Gruijter, D. N. M. (2008). Toetsing en toetsanalyse. 3.8 Juridische aandachtspunten Als je als instelling een itembank wilt ontwikkelen, hetzij alleen, hetzij samen met andere instellingen, krijg je te maken met een aantal juridische aandachtspunten. Hoe borg je het eigendom van de items en de itembank? Wat moet je regelen op het gebied van bescherming van persoonsgegevens? En met betrekking tot samenwerkingsvormen? Hoe ga je het verspreiden van items tegen? In dit hoofdstuk komen deze zaken aan de orde. Weet je niet zeker hoe het bij jouw instelling in elkaar steekt? Vraag het na bij de afdeling juridische zaken of het juridisch bureau van je instelling. Werk je aan een instellingsoverstijgende itembank? In de praktijk is het een behoorlijke klus dit juridisch goed doortimmerd te krijgen. Betrek een jurist met kennis van zaken. Neem de tijd om te zoeken naar iemand met deze expertise. 3.8.1 Eigendom items en itembank Maak afspraken over de rechten over de items. Hiermee borg je dat deze beschikbaar blijft voor de instelling en/of het samenwerkingsverband. Items zijn beschermd door het auteursrecht. Dit recht ontstaat bij het maken van het werk. Volgens de Auteurswet heeft de auteur het exclusieve recht om te publiceren en te dupliceren. Let op de volgende zaken: Voor iedere docent met een arbeidsovereenkomst geldt dat de auteursrechten toekomen aan de werkgever indien het materiaal is gemaakt bij de uitoefening van de onderwijstaken, tenzij partijen anders zijn overeengekomen. Indien er sprake is van een samenwerkingsverband: maak de items eigendom van het samenwerkingsverband. Specificeer dit in de samenwerkingsovereenkomst òf stel de items onder Creative Commons Licentie beschikbaar. Let op: indien de items summatief worden ingezet, is dit laatste wellicht niet wenselijk, want dan wil je juist niet dat iedereen de items kan gebruiken. Regel bij een samenwerking dat bij vertrek van één van de partners het eigenaarschap van de items bij het samenwerkingsverband blijft. Leg dit vast in de samenwerkingsovereenkomst. Als je gebruikmaakt van afbeeldingen bij items, heb je te maken met beeldbankenrecht. Het is verboden beeldmateriaal te kopiëren en te gebruiken. Dit moet de instelling of het samenwerkingsverband afkopen per beeld of er een abonnement voor afsluiten. Tip: definieer een extra stap binnen je itemontwikkelproces, waarin de reviewer van een item checkt of het gebruikte beeld en geluid is toegestaan. Bepaal of er databankenrecht ontstaat. De Databankenwet beschermt tegen overnemen of herhaaldelijk opvragen van gegevens uit een databank zonder toestemming van de producent. De gegevens die in de databank staan zijn als ‘verzameling’ beschermd door het databankenrecht. Als je als partij een investering doet in de databank bij het aanleggen ontstaat er automatisch databankenrecht. Degene die investeert, mag ook exploiteren. Spreek met de betrokken commerciële marktpartij af hoe je content kunt terugkrijgen bij een exit-scenario. Wat voor tool levert de marktpartij? Laat je goed adviseren als je een commerciële partij betrekt, zodat jouw belangen zijn afgedekt. Voorkom dat een andere partij ervandoor gaat met de waardevolle inhoud van de itembank. 3.8.2 Misbruik van itembankmateriaal Instellingen willen meestal voorkomen dat de items ‘op straat’ terecht komen, vooral als het summatieve items betreft. Vaak komen toetsen en items terecht op https://www.studeersnel.nl, https://www.stuvia.nl en https://www.knoowy.nl. Spreek de website aan op het auteursrecht. Zij zijn geen eigenaar van de items en mogen ze dus niet publiceren. Als je ze vraagt om de items te verwijderen, moeten zij hieraan gehoor geven. 3.8.3 Het gebruik van materiaal van andere partijen in itembanken Je kan niet zomaar allerlei video’s en afbeeldingen van anderen overnemen in items. Hieraan zijn wettelijke regels verbonden. Wel zijn er veel mogelijkheden om recht- matig bestaand materiaal te gebruiken en anders is er de mogelijkheid om sommige afbeeldingen of video’s speciaal voor je itembank opnieuw of na te maken. Neem hiervoor budget op in je project. Ga na of je instelling, of de instelling waarmee je wilt samenwerken, een regeling heeft getroffen met Stichting Pro (Publicatie en Reproductie Organisatie). Dit orgaan monitort auteursrechten binnen het onderwijs en beschermt de bij haar aangesloten uitgeverijen. Pro controleert instellingen op het gebruik en hergebruik van publicaties van commerciële partijen. Instellingen kunnen ervoor kiezen om Stichting Pro jaarlijks een afkoopbedrag te betalen. Daardoor hoeven deze instellingen geen administratie bij te houden voor korte stukken tekst die worden overgenomen. Als je instelling géén afkoopbedrag betaalt, moet je expliciet rekening houden met auteursrecht. Soms moet je voor een overgenomen bron betalen, maar vaak ook niet. Hieronder sommen we de belangrijkste vuistregels op voor het gebruik van beeld en geluid in items. Deze informatie is ontleend aan https://www.auteursrechten.nl. 3.8.3.1 Linken mag altijd Je mag linken naar afbeeldingen, beeld- en geluidsfragmenten op het internet. Openbaarmaking op het internet moet echter wel rechtmatig zijn. Een beeld- of geluidsfragment stream je rechtstreeks. Downloaden en vertonen is verboden, tenzij de licentie dit toestaat. Voor summatief toetsen kan streamen problematisch zijn. Bij toetsing wordt vaak gebruikgemaakt van materiaal dat juist niet openbaar toegankelijk mag zijn. Bovendien wil je niet dat het materiaal nèt van het internet is verdwenen op het moment van de toets. 3.8.3.2 Licentie voor hergebruik Afbeeldingen en audiovisuele werken met een licentie die hergebruik toestaat, bijvoorbeeld een Creative Commons-licentie, kun je zonder problemen downloaden en vertonen. Afhankelijk van de gebruikte CC-licentie zijn de mogelijkheden ruimer of beperkter. Check daarom altijd even de betekenis van de licentie op https://creativecommons.nl/uitleg/. 3.8.3.3 Het citaatrecht biedt veel mogelijkheden Het citaatrecht maakt het mogelijk om afbeeldingen of audiovisuele werken te gebruiken. Je moet aan de volgende voorwaarden voldoen: De citaten dienen ter ondersteuning van de inhoud van het onderwijs. Je mag ze niet gebruiken ter versiering. Ook mag je geen wijzigingen aanbrengen in het te citeren fragment. De omvang van het citaat is gerelateerd aan het doel dat je ermee nastreeft. In de praktijk komt dit neer op: gebruik altijd korte fragmenten. Afbeeldingen mag je wel in hun geheel ‘citeren.’ Je doet aan bronvermelding. 3.8.3.4 Afbeelding of audiovisueel werk uit de eigen bibliotheek] Mogelijk heeft je instelling gebruikslicenties afgesloten, bijvoorbeeld voor Academia, die gebruik van afbeeldingen en audiovisuele werken binnen het onderwijs van je instelling toestaan, zonder dat je voor ieder gebruik aparte toestemming moet vragen en een vergoeding betalen. Ga na binnen je instelling of er dergelijke licenties zijn. 3.8.3.5 Complete audio(visuele) werken Je mag een compleet audiovisueel werk gratis afspelen of vertonen zonder toestemming, mits: het afspelen een educatief doel tijdens een les of werkgroep dient en deel uitmaakt van het onderwijsprogramma. het afspelen fysiek in de onderwijsinstelling plaatsvindt. het afspelen plaatsvindt in het kader van onderwijs zonder winstoogmerk. Een kopie van het werk in een toetsapplicatie zetten, zodat studenten het thuis kunnen afspelen, mag niet. Je moet hiervoor toestemming vragen aan de maker. 3.8.3.6 Delen van audio(visuele) werken Je mag delen van audiovisuele werken plaatsen in een toets, mits: de vertoning/het afspelen uitsluitend dient ter toelichting bij niet-commercieel onderwijs. Het is dus aanvullend en niet onderwijsvervangend. de plaatsing gebeurt in een besloten omgeving, waartoe alleen studenten toegang hebben. Loggen studenten in voor de toets? Dan voldoe je aan deze voorwaarde. er een billijke vergoeding wordt betaald aan de rechthebbenden. Neem hiervoor contact met ze op. De vertoning hoeft niet fysiek in het leslokaal plaats te vinden. Studenten mogen de werken thuis zien of afspelen. 3.8.3.7 Bronvermelding Als je gebruikmaakt van materiaal van derden waarvoor je moet betalen, neem je contact op met de rechthebbenden. Ook als je materiaal namaakt. Het contact verloopt via de volgende organisaties: VIDEMA BUMA/STEMRA PICTORIGHT Meer informatie: https://www.auteursrechten.nl/ http://www.onderwijsenauteursrecht.nl/ https://IUSmentis.com https://creativecommons.nl/ 3.8.4 Bescherming persoonsgegevens Vanwege de Algemene Verordening Gegevensbescherming (AVG) heb je verwerkersovereenkomsten nodig om de privacy van docenten, medewerkers en studenten te borgen. Zodra medewerkers of studenten een account aanmaken voor bijvoorbeeld een toetsapplicatie en de instelling of leverancier toegang heeft tot deze persoons- gegevens, moet er een verwerkersovereenkomst worden getekend tussen de instelling en de leverancier. Let op de volgende zaken: Het gebruik van een SAAS (software-as-a-service)-dienst vraagt om extra handelingen. De instelling of het samenwerkingsverband moet zorgen voor een vewerkersovereenkomst met de leverancier. Bij het inloggen in de applicatie staat er dan vaak: ‘er zijn gebruiksvoorwaarden van toepassing.’ De gebruikers kunnen dit wel of niet accepteren, waarmee de privacy is geborgd. De instelling of het samenwerkingsverband neemt zelf voldoende passende maatregelen. Er is afstemming met de leverancier nodig in een contract en medewerkers moeten weten waar ze aan toe zijn. Als het gaat om een applicatie op de eigen servers, dan is de instelling of het samenwerkingsverband zelf verantwoordelijk voor het systeem en de gegevens. Zorg ook in dat geval voor duidelijke communicatie met medewerkers. Lees meer over de Algemene Verordening Gegevensbescherming op de SURF thema-pagina Algemene Verordening Gegevensbescherming (AVG). 3.8.5 Rechtsvormen voor samenwerkingsverbanden Ben je van plan om met meerdere instellingen een itembank te ontwikkelen? Leg de rechtsvorm en de onderlinge verhoudingen vast. Stem ook af wie wat op welk moment aanlevert. Dit leg je bijvoorbeeld vast in statuten en of een samenwerkingsovereenkomst. Literatuur "],["bijlagen.html", "Part 4 Bijlagen 4.1 Begrippenkader 4.2 Workflows voor itemontwikkeling", " Part 4 Bijlagen 4.1 Begrippenkader 4.1.1 Adaptieve toets Een toets waarbij de kandidaten gemakkelijker of moeilijker opgaven moeten beantwoorden afhankelijk van de vaardigheid van de kandidaat, die wordt ingeschat op basis van voorgaand items. Bij een op IRT gebaseerde adaptieve toets is sprake van gekalibreerde items waarvan die op een gegeven vaardigheidsniveau van de kandidaten het meest nauwkeurig kunnen meten. Bij een computer-adaptieve toets worden geen items meer aan de kandidaat aangeboden als het niveau van vaardigheid met de gewenste nauwkeurigheid is vastgesteld. Zie ook: item response. 4.1.2 Afspeelomgeving De component van een toetssysteem waarmee toetsen en items aan de kandidaten worden aangeboden en de antwoorden geregistreerd worden. zorgt dat studenten na inloggen de toets kunnen maken; wordt vaak gecombineerd met een zogenaamde secure browser, die ervoor zorgt dat de student niet ongeoorloofd software kan gebruiken om op internet te zoeken, met derden te communiceren, etc.; registreert de antwoorden en stuurt deze terug naar de itembank. 4.1.3 Auteursomgeving De component van een toetssysteem waarmee items kunnen worden ingevoerd. is een interface waarin items kunnen worden ontwikkeld en voorzien van metadata; biedt keuze uit allerlei vraagtypen (meerkeuze, aanwijsvraag, open vraag, etc.); geeft de mogelijkheid multimediale componenten op te nemen; ondersteunt (meestal) de workflow en legt het reviewproces vast; ondersteunt het reviseren van items (bijvoorbeeld na de eerste toetsafname) 4.1.4 Betrouwbaarheid De mate waarin de score op een toets niet van het toeval afhankelijk is. De mate waarin een toets bij gelijke condities dezelfde uitslag oplevert. In statische termen wordt betrouwbaarheid beschreven in de mate waarin een meting vrij is van meetfouten (ook wel ruis of error genoemd). Zie ook: Misclassificaties. 4.1.5 Body of Knowledge (BoK) Een Body of Knowledge (BoK) is een gemeenschappelijke kennisbasis voor een vakgebied of sector waaraan de professional zijn theoretische en praktische kennis, inzichten en methoden ontleent. Het gaat niet alleen om theorie, maar ook om beproefde inzichten en methoden van de beroepsgroep in kwestie. 4.1.6 Businesscase Een businesscase of een haalbaarheidsstudie beschrijft de afweging om een project of taak te beginnen of te stoppen. In de business- case worden de kosten tegen de baten afgewogen, rekening houdend met de risico’s. Een businesscase is niet bedoeld om uitsluitend een financiële afweging te maken. Juist ook kwalitatieve overwegingen worden in een goede businesscase nadrukkelijk in de beoordeling betrokken. Een aspect als kwaliteitsverbetering kan voor een instelling heel belangrijk zijn, terwijl het zuiver financiële gewin daarvan nauwelijks te bepalen is. Bekijk ook de whitepaper De businesscase van digitaal toetsen. 4.1.7 Cronbach’s alpha Een psychometrische maat die bij een niet-herhaalde toets (dus éénmalige afname zoals een tentamen) een waarde geeft waarin de meting onderhevig is aan meetfouten. Cronbach’s alpha is geschikt voor polytoom gescoorde items. Vragen waar naast correct-incorrect (0 of 1) ook met bijvoorbeeld 0, 1, 2, 3 etc. punten kan worden gerekend. Zie ook: KR20. 4.1.8 Diagnostische toetsen (onderdeel van formatieve toetsen) Een formatieve toets waarbij een kandidaat inzicht krijgt in zijn progressie door de leerstof. Een diagnostische toets is representatief voor wat betreft inhoud en niveau met een eventueel volgende summatieve eindtoets. Zie ook: Oefentoets. 4.1.9 Digitaal toetssysteem Dit is het cyclische proces om van leerdoelen, via items, toetsen, afname en analyse toetsing te realiseren en voortdurend te verbeteren. Vaak wordt in het centrum van deze cyclus de ontwikkeling en beheer van de itembank gesitueerd. Figure 4.1: Componenten van digitale toetssystemen afgebeeld op de toetscyclus. 4.1.10 Formatieve feedback Feedback waarbij de opbrengst voor het leerproces voorop staat. De feedback is bedoeld om studenten te stimuleren om zich verder te verdiepen in de stof. Bij incorrecte antwoorden worden vaak aan wijzingen gegeven om het correct antwoord te kunnen achterhalen of simpelweg het correcte antwoord. Bij correcte antwoorden wordt soms verdiepende stof of een ander voorbeeld besproken. 4.1.11 Formatieve toetsing Toetsing waarbij het leren van de toets voorop staat. Vanuit de literatuur wordt hierbij aanbevolen om af te zien van het geven van cijfers . Door met toetsopgaven aan de slag te gaan en te leren van fouten en feedback wordt het leerproces gestimuleerd. Er wordt niet gestreefd naar een bepaald minimaal te behalen niveau. Soms ook aangeduid met Assessment for Learning of Assessment as Learning. 4.1.12 Geparametriseerde vraag Een item bestaand uit een vaste hoofdstructuur met variabele elementen (getallen, objecten, concepten, principes), zodat ze elke keer een nieuw item vormen. Sommige itembanksystemen genereren elke keer een uniek nieuw item voor de docent of een unieke set van waarden voor elke student. 4.1.13 IMS Een IMS-standaard is vastgelegd door het IMS Global Learning Consortium, een community van hogeronderwijsinstellingen, leveranciers en overheidsinstellingen die samen uitwisselingsstandaarden ontwikkelen. Toen IMS in 1997 begon was de officiële naam Instructional Management Systems (IMS)-project. 4.1.14 Item-response theorie (IRT) Item-response theorie (IRT) is een methode waarbij er vanuit wordt gegaan dat de kans dat een kandidaat een item correct beantwoordt bepaald wordt door de vaardigheid van de kandidaat als functie van het moeilijkheidsniveau van het item en het onderscheidend vermogen van het item. IRT is een theorie die uitgaat van de karakteristiek van items, terwijl de klassieke testtheorie uitgaat van de karakteristiek van een toets. Om IRT zinvol te kunnen toepassen moet op basis van een groot aantal toetsafnames bij een populatie waarin alle vaardigheidsniveaus aanwezig zijn deze karakteristieken voorafgaand aan een toets worden bepaald (zgn. kalbireren). Zie ook: adaptieve toets. 4.1.15 Item Een opdracht/taak waarop een kandidaat het correcte antwoord moet geven. Wordt ook wel aangeduid als vraag, toetsvraag of toetsitem. 4.1.16 Itembank Een verzameling van items voor een bepaald toetsdoel. Een itembank heeft daarvoor een bepaalde ordening, meestal op basis van metadata en in de vorm van een metadata en in de vorm van een hiërarchische structuur. 4.1.17 Itembanksysteem Een software om itembanken in op te kunnen slaan en bewerken. Itembanksystemen kunnen losse systemen zijn of onderdeel van een toetssysteem. In een toetssysteem zijn ook functies opgenomen om toetsen samen te stellen, toetsen af te nemen, toetsen te scoren en toetsanalyses uit te voeren. 4.1.18 Klassieke testtheorie De klassieke testtheorie (KTT) heeft als basisaanname dat de geobserveerde score op een toets bestaat uit de werkelijke score plus een meetfout. Uitgaande van deze basisveronderstellingen kunnen grootheden zoals Cronbach’s alpha worden berekend, die een schatting is voor de grootte van de meetfout. 4.1.19 KR20 Een psychometrische maat die bij een niet herhaalde toets (dus éénmalige afname zoals een tentamen) een waarde geeft waarin de meting onderhavig is aan meetfouten. Het enige verschil tussen KR20 en Cronbachs alfa is dat er bij KR20 uitsluitend met dichotoom gescoorde items kan worden gerekend, zoals meerkeuzevragen. Dus items die correct of incorrect zijn en 0 of 1 punt opleveren. Zie ook: Cronbachs alfa. 4.1.20 Learning analytics Learning analytics is het verzamelen en analyseren van studiedata die een student tijdens het online leren genereert. De studiedata worden omgezet naar waardevolle informatie en kan bijdragen aan de verbetering van de onderwijskwaliteit. Zie ook: https://www.surf.nl/onderwijs-ict/learning-analytics. 4.1.21 LTI LTI (Learning Tools Interoperability standard) is een IMS-standaard om toetssystemen te koppelen aan andere systemen zoals digitale leeromgevingen (LMSen zoals Blackboard of Canvas). Daarbij worden docent- en studentgegevens tussen de systemen automatisch uitgewisseld, kunnen toetsen vanuit de LMS worden gestart en worden de scores op toetsen teruggevoerd naar de cijferlijstfuncties in de LMS. Zie ook: SCORM. 4.1.22 Meetfout De meetfout bij een toets is de afwijking van de score op een toets die niet verklaard kan worden. De werkelijke score is de geobserveerde score minus de meetfout. Zie ook: ruis en error. 4.1.23 Metadata Toegevoegde gegevens aan items die aanvullende karakeristieken beschrijven. Die gegevens kunnen gebruikt worden om de items te structureren (zie structuur), zoeken, filteren en selecteren. 4.1.24 Misclassificaties Het aandeel kandidaten dat onterecht gezakt danwel geslaagd is op een toets. Toetsen met lage betrouwbaarheid hebben een hoge mate van misclassificaties. Zie ook: betrouwbaarheid. 4.1.25 Oefentoetsen (onderdeel van formatieve toetsen) Een toets waarbij een kandidaat kan oefenen met de leerstof. De kandidaat kan zelf onderwerpen en niveau kiezen. Vaak worden deze toetsen gebruikt om vooral met moeilijke begrippen of concepten te oefenen. Soms wordt enige adaptiviteit ingebouwd zodat de afspeelomgeving items op een best passend niveau wordt aangeboden aan de kandidaat. Zie ook: diagnostische toets. 4.1.26 Psychometrische data Psychometrische data betreft de te berekenen gegevens van afgenomen toetsen en items die iets zeggen over de kwaliteit van de toetsing. Enerzijds zeggen zij iets over de betrouwbaarheid en moeilijkheidsgraad van toetsen als geheel. Anderzijds zeggen ze iets over het discriminerend vermogen en de moeilijkheidsgraad van afzonderlijke items uit die toetsen. Zie ook: klassieke testtheorie, item-response theorie, Cronbach’s alpha, p-waarde, rit-waarde. 4.1.27 P-waarde De proportie van de kandidaten die een item correct heeft beantwoord. Wordt ook de moeilijkheidsgraad van een item genoemd. Hoe hoger de p-waarde, des te gemakkelijker is het item. P-waarde wordt alleen gebruik in relatie tot de klassieke testtheorie. 4.1.28 QTI QTI (Question and Test Interoperability) is een IMS-standaard voor de uitwisseling van items en toetsen tussen toetssystemen. 4.1.29 Rit-waarde De correlatie tussen de score van de kandidaten op de item en de score op de toets als geheel minus de betreffende item zelf. De rir-waarde is een wat strengere maat voor het discriminerend vermogen dan de rit-waarde omdat bij de rir-waarde de invloed van het item zelf op het discriminerend vermogen wordt uitgesloten. Rir-waarde wordt alleen gebruik in relatie tot de klassieke testtheorie. Zie ook: Rit-waarde en klassieke testtheorie. 4.1.30 SCORM SCORM (Sharable Content Object Reference Model) is een IMS-standaard om toetssystemen te koppelen aan andere systemen zoals digitale leeromgevingen (LMSen zoals Blackboard of Canvas). Daarbij worden met name docent- en studentgegevens tussen de systemen automatisch uitgewisseld, kunnen toetsen vanuit de LMS worden gestart en worden de scores op toetsen teruggevoerd naar de cijferlijstfuncties in de LMS. Zie ook: LTI. 4.1.31 Stabiel domein Een vakgebied dat weinig veranderingen meer ondergaat. Zie ook: volatiel domein. 4.1.32 Structuur De manier waarop een verzameling van items logisch geordend zijn, bijvoorbeeld in een boomstructuur. 4.1.33 Summatieve toetsing Toetsing waarbij het zo nauwkeurig mogelijk meten van een bepaald niveau van vaardigheid voorop staat. De behaalde score op een dergelijke meting wordt gebruikt voor formele toerekening van een behaald studieresultaat zoals studiepunten of een diploma. Denk bijvoorbeeld aan examens of een tentamen. Soms ook aangeduid met assessment of learning. 4.1.34 Toetsitem Zie: item. 4.1.35 Toetstransparantie Het geheel van aspecten rondom toetsing dat de mate van transparantie, betrouwbaarheid en validiteit van toetsing behelst. Transparantie kan bijvoorbeeld toenemen als inzicht is in de wijze waarop items worden gemaakt, betrouwbaarheid kan toenemen als items beter onderscheid maken tussen studenten met verschillende mate van beheersing van de stof of vaardigheid, validiteit kan toenemen als de items beter en vollediger de beoogde kennis of vaardigheid meten. 4.1.36 Toetsmatrijs Een toetsmatrijs is een overzicht hoe toetsopgaven in een toets verdeeld zijn over de stof. Vaak wordt per onderdeel van de stof tevens de gewenste type vaardigheid, zoals kennis, inzicht of toepassing, aangegeven. Soms ook aangeduid als toetsmatrix, toetplan, specificatietabel of blueprint. 4.1.37 Toetsveiligheid De mate waarin correcte antwoorden items en toetsen niet op een ongeoorloofde manier door kandidaten gegeven worden (fraude). Bijvoorbeeld doordat items zijn gestolen of in het publieke domein terecht zijn gekomen, danwel dat kandidaten tijdens de toetsafn ame de beschikking hebben over ongeoorloofde bronnen, danwel dat kandidaten toetsuitslagen kunnen wijzigen. Zie o.a. de SURF uitgave Handboekveilig toetsen. 4.1.38 Toetsvraag Zie: item. 4.1.39 Vakgebied Een vakgebied is een terrein van beroepsmatig toegepaste kennis en vaardigheden waar mensen zich in kunnen specialiseren. Voor het uitwisselen van ervaring en vakkennis is meestal een jargon nodig. 4.1.40 Vragenbank Een systeem waarin alle items zijn verzameld. 4.1.41 Volatiel domein Een vakgebied dat aan veel veranderingen onderhevig is door nieuw ontwikkelde kennis, methoden of technieken. Zie ook: stabiel domein. 4.1.42 Workflow Een logische, vastgelegde volgorde van activiteiten die moeten worden uitgevoerd om een van te voren gedefinieerde uitkomst te verkrijgen. Deze stappen kunnen sequentieel zijn, maar kunnen ook parallel worden uitgevoerd afhankelijk van doel van het proces. 4.2 Workflows voor itemontwikkeling In deze bijlage beschrijven we drie praktijkvoorbeelden van workflows voor itemontwikkeling. 4.2.1 Een itemontwikkelproces door twee docenten Een coördinator/examinator van de faculteit Aard- en Levenswetenschappen van de Vrije Universiteit Amsterdam ontwikkelt en onderhoudt een itembank over het onderwerp Microbiologie. Elk jaar worden ongeveer 60 nieuwe items gemaakt. De coördinator is de itembank gestart op basis van de itembank van de uitgeverij McGrawHill: Prescott’s Microbiology. De structuur van de itembank volgt de hoofdstukken uit dat boek. Die itembank wordt door de uitgeverij geleverd in Word formaat. De docent kopieert de inhoud één voor één naar het online itembanksysteem QMLive. Tijdens het invoeren voert de coördinator een kwaliteitscheck uit: ongeveer 80-90 procent van de items zijn relevant en van goede kwaliteit. De rest van de items verwijdert de coördinator. Een nieuw item maakt de docent eerst in MS Word. Een week voor uitvoering van het cursusonderdeel laat de coördinator nieuwe items reviewen door de uitvoerend docent. Reviewen gaat via MS Word. Op basis van de feedback van de uitvoerend docent past de coördinator het item aan. Hij voert het in het itembanksysteem in. Na afloop van de toets en op basis van de psychometrische analyse past de docent een item direct aan in de itembank voor de volgende jaren. Het kost de docent ongeveer een half uur om een goed item exclusief feedback te ontwikkelen. De coördinator maakt vooral andere vraagtypes dan meerkeuzevragen, vaak met afbeeldingen. Dat kost relatief veel tijd per item. Figure 4.2: Itembank workflow 1. 4.2.2 Itemontwikkelproces samenwerking tussen hogescholen Toets &amp; Leer was een samenwerkingsverband van zes hogescholen (2012 – 2018) met itembanken voor de onderwerpen Bedrijfsadministratie/boekhouden, Bedrijfseconomie, Belas- tingrecht, Marketing, Management en Recht. De hogescholen leverden docenten voor de itemontwikkeling. Ze werkten met gesloten beurzen. Op jaarbasis leverden ze 300 per hogeschool. Het werkproces zag er als volgt uit: Een docent ontwikkelt een item en voert dit in het itembanksysteem in. De docent informeert een docent-collega van één van de andere hogescholen, meestal per groepje van items. De collega-docent controleert de items en zorgt voor eventuele wijzigingen en aanvullingen. Aanvullingen en opmerkingen worden in de toetsapplicatie opgeslagen. De collega-docent bericht de oorspronkelijke auteur en deze controleert de wijzigingen. Het item wordt toetstechnisch gecontroleerd door een extern bureau. Eventueel ontvangt de oorspronkelijke auteur een opmerking of een verzoek tot wijziging. Dan volgt hernieuwde controle. Het item in de toetsapplicatie wordt definitief goedgekeurd. Op incidentele basis vindt terugkoppeling plaats op basis van een psychometrische analyse. Idealiter is er een redacteur ingevoegd in het proces. Het kostte in dit geval gemiddeld ongeveer een half uur om een goed item inclusief feedback te ontwikkelen. De meeste items waren meerkeuzevragen (ongeveer 80 procent) en invulvragen (korte tekst of getal, ongeveer 20 procent). Figure 4.3: Itembank workflow 2. 4.2.3 Itemontwikkelproces MBO Kennistoetsenbank De betrokken mbo’s leveren docenten voor de itemontwikkeling. Itemontwikkelaars worden betaald voor vier uur itemontwikkeling per week. Per itembank zijn ongeveer zes tot tien itemontwikkelaars actief. De itemontwikkelaars construeren in een team pre-concept-items met feedback, geordend naar zowel het kwalificatie-dossier als de Body of Knowledge (BoK). Deze pre-concept-items gaan meerdere keren langs meerdere docenten (interne validatie). Labels aan items maken duidelijk welke items nog moeten worden bekeken (metatags). Aanvullingen en commentaar worden bijgehouden in het itembanksysteem. Er vinden maandelijkse face-to-face werksessies met toetsdeskundigen en docenten van de deelnemende mbo’s plaats. Tijdens die sessies bespreken de teams nieuwe items per kwalificatiedossier. Hieruit ontstaan concept-items. De itemontwikkelaar past de concept-items zo nodig aan. Hieruit ontstaan gepubliceerde items. Vóór publicatie vindt taalredactie plaats. Het aanpassen is een continu proces. Gebruikers kunnen opmerkingen plaatsen bij items die tot aanpassingen kunnen leiden. Bij grote aanpassingen is het mogelijk om opnieuw interne en externe validatie toe te passen. In het itembankproject vindt alleen op incidentele basis terugkoppeling plaats op basis van een psychometrische analyse. Het kost gemiddeld ongeveer 1 uur om een goed item inclusief feedback te ontwikkelen. Figure 4.4: Itembank workflow 3. "],["colofon.html", "Part 5 Colofon 5.1 Aan deze herziene uitgave werkten mee: 5.2 De oorspronkelijke publicatie is geschreven door: 5.3 In nauwe samenwerking met verschillende itembank-experts: 5.4 Met dank aan 5.5 Opmaak 5.6 Copyright", " Part 5 Colofon Deze uitgave is een bijgewerkte versie van de uitgave uit 2018. De special interest group (SIG) Digitaal toetsen heeft de uitgave – mede in het licht van de ervaringen tijdens de coronacrisis van 2020/2021 – kritisch bekeken, aangevuld en geactualiseerd. 5.1 Aan deze herziene uitgave werkten mee: Silvester Draaijer, Vrije Universiteit Sharon Klinkenberg, Universiteit van Amsterdam Vincent Hendriks, Open Universiteit Annette Peet, SURF 5.2 De oorspronkelijke publicatie is geschreven door: Silvester Draaijer, Vrije Universiteit Jenny de Werk, SURFnet 5.2.1 Redactie Marjolein van Trigt 5.3 In nauwe samenwerking met verschillende itembank-experts: Sander Schenk, Hogeschool Rotterdam Jane Groenendijk, Prove2Move Jeroen Donkers, Maastricht University Marjolein Wijnker, DAS Peter Roos, 10voordeleraar Angela Verschoor, CITO Wil de Groot-Bolluijt, GrootBolwerk 5.4 Met dank aan Desiree Joosten-Ten Brinke, Fontys Hogescholen Ludo van Meeuwen, Technische Universiteit Eindhoven en SIG digitaal toetsen Rob Reichardt, NHL Stenden Hogeschool en SIG digitaal toetsen Kirsten Veelo, SURFnet 5.5 Opmaak Rochelle Hurenkamp, Werkgroep Toetsen op Afstand 5.6 Copyright Beschikbaar onder de licentie Creative Commons Naamsvermelding 4.0 Nederland "],["literatuur.html", "Part 6 Literatuur", " Part 6 Literatuur "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
=======
[["index.html", "Handbook Item Banking An itembank in 5 steps Part 1 Introduction 1.1 Introduction 1.2 Definitions and concepts 1.3 An item bank in 5 easy steps 1.4 Success factors", " Handbook Item Banking An itembank in 5 steps SURF, Versnellingsplan Updated: 2022-01-08 Part 1 Introduction 1.1 Introduction Digital item banks are a wonderful tool for increasing the quality of assessments and helping reduce costs. Developing exam questions together means coordinating objectives and processes. More expertise and time will be available per item (exam question) for development and quality assurance. Taken together, this will improve the quality of the assessment. Reusing items can help reduce costs. The coronavirus crisis has also shown that item banks play a role in situations where online assessment creates risk exposure. In particular, if test questions leak out or students conspire while sitting an exam. Item banks offer the opportunity to reduce these risks. The development, management and maintenance of item banks require good project management. The aim of this publication is to show you how to set up a digital system to share items with colleagues inside and outside the institution. This handbook combines the knowledge and expertise that institutions, institutional partnerships and SURF have acquired on this topic. It helps institutions wanting to take advantage of digital item banks to get started. The handbook consists of an introduction, a step-by-step plan and an in-depth section. 1.1.1 Who is this publication for? The handbook is intended for anyone who plays a role in the processes surrounding the development, management and maintenance of an item bank and the items in it. For example: Lecturers who want to organise their test items better or who want to develop them or share them with a colleague Educational advisers who support lecturers or who are the pacemakers behind the creation of an item bank A collaborative venture wanting to share assessment resources or deploy assessments together A national consultation aimed at increasing the quality of assessment Functional administrators of an assessment system used for an item bank 1.1.2 Scope The handbook focuses on the development, management and use of item banks within a study programme, within a single institution, across institutions, within a certain domain or even nationally. The essential premise is that the processes will be the same. The handbook does not provide a comprehensive description of existing item banks or testing systems. This is available in the SURF thematic publication Theme edition: Testing and question banks in education. The development of item banks can form part of a larger digital assessment implementation process within an institution. For the implementation of digital assessment, we refer to a number of SURF publications, such as the Guidelines for digital assessment policy, the Getting started with digital testing policy in 5 steps and the Secure assessment workbook: Tools and tips for setting up a secure assessment process. This handbook is based on the assessment cycle described in the Begrippenkader voor digitaal toetsen van SURF uit 2013. The focus is on developing items (the item development process) and managing items in an item bank. The foundation for sound assessment is a good item bank. Other sources are available on how to develop sound exams within an assessment system based on an item bank, such as Toetsen in het hoger onderwijs by Van Berkel, Bax, and Joosten-ten Brinke (2017). 1.1.3 Justification This edition is an updated version of the 2018 edition. The Digital Assessment Special Interest Group (SIG) has critically reviewed, supplemented and updated the publication, partly in the light of experiences gained during the coronavirus crisis of 2020-2021. The original publication was produced in close cooperation with item bank experts from various institutions, the Digital Assessment SIG, Cito, 10voordeLeraar and Prove2Move. This handbook is based on the experiences of the experts involved in developing item banks. We also used collected resources from SURF and available literature on this topic. See the publication details for a list of all those involved who contributed to this handbook. Please feel free to contact the authors and/or the experts who contributed to the handbook for more information. 1.2 Definitions and concepts A good understanding of a number of concepts is essential for the use of this handbook. We briefly describe the components of a digital assessment system and show how they relate to the item bank. 1.2.1 What is an item bank? An item bank is a collection of items or questions (throughout this handbook, we will call them items). An item bank system is a digital system used to store and manage collections of items. An item bank system may contain multiple item banks. When we refer to item banks in this handbook, we actually mean the item bank system (the digital item bank). Definition An item bank is a collection of items for a particular assessment objective. An item bank has a certain structure, usually hierarchical and based on metadata. An item bank system is a digital system used to store and manage collections of items. An item bank system may contain multiple item banks. The questions are structured in the item bank according to a certain logic. Version management is used, and all kinds of metadata can be stored about the content, the question type, the level and other characteristics. You can also store psychometric data, such as the degree of difficulty and the discriminatory capacity of specific items. Sometimes the programmes of an item bank system will run locally on a single computer. Other item banks are deployed in web-based environments. The database and the application are accessed using cloud technology. Multiple users can work in a cloud-based system at the same time. Useful information Why are ‘test questions’ often referred to as items by assessment experts? In the 1930s, standardised tests and rudimentary databases of questions were starting to be used in the United States. Soon, a professional group emerged which developed its own associated professional terminology. ‘Test questions’ became ‘test items.’ In the Netherlands, we have now adopted the same phraseology. 1.2.2 The position of an item bank within the digital assessment system When we refer to a digital assessment system, we mean the software that facilitates the digital assessment process. We can distinguish between a number of core components: authoring environment, item bank, playback environment and analysis tool. We will discuss these components based on the assessment cycle. Not all steps in the assessment cycle are supported by digital assessment systems. Figure 1.1 shows how the components of a digital test system fit in with the assessment cycle. Figure 1.1: Components of digital assessment systems depicted in the assessment cycle. The item bank is the central component in which the individual items are stored. The editing and playback of the items take place in other systems, which then return the results to the item bank. The item bank plays a role in the following components of the assessment cycle: The design of the test (also known as the test specification) relates to aspects such as the purpose of the test, the components to be assessed, the choice of formative or summative assessment and the ‘assessment method.’ This means that no items are yet created in the design. A test is compiled using a collection of items. A specification table is usually used, commonly also known as the test matrix or test blueprint. This specifies how many items, what types of questions and what subjects of which method, knowledge or application will be included in the test. The development (construction) of the items is performed in the assessment system’s authoring environment. In the case of closed questions, the answers will be included; sample answers may sometimes also be included for open questions. Assessment software offers a playback environment for online ** test administration**. The playback environment checks the student’s identity by means of a login procedure, presents the student with a test and stores the student’s answers. The marking of the test will depend on the selected question types. Answers to closed questions are usually assessed and scored without human intervention. The item developer (the lecturer) will have included the correct answer in advance. Open questions must be marked, with systems allowing, for example, ‘blind’ marking and ‘question-by-question’ marking, possibly aided by supporting software. Some software packages included an analysis tool to perform a test analysis, but analysis can also be performed after downloading the data file and preparing it in Excel or SPSS, for example. Test analysis is often used to help determine the reliability of tests and to pinpoint items that are of dubious quality. 1.2.3 Key concepts Formative assessment: Testing in which the learning that comes from taking the test is of primary importance, usually ungraded, but often with feedback on each test question or test. The learning process benefits from taking test assignments and learning from your mistakes. There is no set minimum level that needs to be achieved. Psychometric data: This data tells us about the degree of difficulty of questions and the extent to which a student has internalised the subject matter. Psychometric data can be stored in the item bank along with the item the data relates to. Summative assessment: Testing that focuses on measuring a certain level of competence as accurately as possible. The score obtained is used for a formal award based on study performance, such as study credits or a diploma. Assessment quality: The entirety of aspects relating to assessment including the degree of transparency, reliability and validity of the assessment. For instance, transparency increases when insight is available into how items are created. Reliability increases, for example, when items discriminate more clearly between students with differing degrees of mastery of the subject matter or competence. Validity increases when the items are better at measuring the intended knowledge or competence more completely. Collaborative partnerships In this handbook, we refer to a number of cross-institutional item banks in use in the Netherlands. We are grateful that we have been able to make use of their knowledge and experience. The collaborative partnerships consulted are as follows: deKennistoetsenbank MBO For the vocational education and training (VET) programme profiles in Care and Welfare, Social Work and Nursing/Individual Care, the kennistoetsenbank MBO of Prove2Move provided item banks containing thousands of items each. Kennistoetsenbank MBO is a cross-curriculum digital system that supports learning by providing robust knowledge-testing questions. Prove2move is a cooperative venture of the three regional learning providers (Landstede, Deltion College and ROC Twente). IVTG The Interuniversity Progress Test for Medicine (iVTG) is a benchmarking instrument used to provide metrics on the knowledge progression of medical students during their studies. This is an inter-university partnership consisting of six academic teaching hospitals. 10voordeleraar The 10voordeLeraar programme was developed under the auspices of the Association of Universities of Applied Sciences (Vereniging Hogescholen) and offers national knowledge bases, mandatory knowledge tests and a peer review system for all teacher training programmes. Sharestats The aim of this project is to provide a freely accessible, comprehensive collection of learning resources on statistics. By adding metadata to the learning resources, lecturers from the professional community can determine and use a selection of statistics for their own teaching practice, as and when required. The five partners of the project team (UvA Psychology and UvA Pedagogical and Educational Sciences, VU Social Sciences (FSW), VU Behavioural and Movement Sciences (FGB), UU Social Sciences and EUR Psychology/Pedagogy) form the core of the professional community. 1.3 An item bank in 5 easy steps In practice, item banks can sometimes emerge without any deliberate plan. A small-scale trial grows into something big. An approach like this entails the risk of item bank development stagnating at a later stage, for example, due to a lack of time or as a result of personnel changes. The advice here is think things though before starting out. Plan your work and pace yourself. Racing ahead may come back to bite you later, with consequences for support or even resulting in project failure. We have identified five steps in the development and use of an item bank. Within each step, there are a number of things you need to arrange. These things will sometimes require attention at the same time, or in a different order than that described here. The result you are aiming for – a working item bank – will be further fleshed out during each step, working from the bare bones to the finished product. Part 2 of the handbook, the Step-by-Step Plan, guides you through the various steps. Sometimes, you will need to have certain theoretical knowledge, and for this reason we regularly refer to part 3 of the Handbook, the Going Deeper section. This section contains more background information about setting up an item bank. Tip If you’re short on time to study the entire Handbook, be sure to keep the success factors for developing, managing and using an item bank from Chapter 4 on hand. 1.3.1 Step-by-Step Plan 1 Preparation Formulate the objective and investigate the feasibility of the item bank. Familiarise yourself with assessment systems and the compatibility of the various system. Deepen your knowledge in terms of scale and question reproduction. Make an initial cost and benefit analysis. 2 Plan Create an action plan that describes: the purpose; the outcome (what will you have achieved by the end of the process?); how you intend to achieve this. 3 Design Organise the questions. Organise the steps in the procedure and assign roles and privileges. Decide on quality requirements and check whether the system meets them. Choose an assessment system. Record any arrangements you agree regarding scale and question production. Arrange the financial aspects. Make arrangements to give the project a legal foundation. Organise administration of the item bank. 4 Pilot Develop the items. Set up the item bank and try it out. 5 Real-life deployment Start using the item bank. Make the item bank future-proof. 1.4 Success factors Developing and using an item bank can be complex. The success factors below can help make an important contribution to the viability of an item bank. Be sure to give them due consideration, even when the collaboration is relatively straightforward. These factors run like a thread through the Handbook. Think things though before starting out: take the time at the start, estimate the feasibility as well as you can, and try to find the balance between a project-based approach and allowing things to emerge organically. Choose a method that suits your situation. Designate a project initiator and ensure that ownership is assigned correctly from the outset. Without a project initiator or arrangements relating to ownership, the risk of failure will be high. To develop the item bank, decide on what steps you want to go through and what results you want to achieve at each step. Decide on the purpose of your item bank and assign a clear role to it in the teaching practice. Stick to this plan. Communicate this vision on a regular basis. Be realistic and manage expectations: the perfect item bank will not be created overnight; in fact, it may never be perfect. Perfecting the item bank will not be a desktop exercise. Dare to try out different things. Look around you, and you’ll see that a lot has already been developed. There is no need to reinvent the wheel. Get experts involved. Support is essential. Lecturers who know and trust each other and who support the purpose of the item bank are a prerequisite for success; devote energy in getting to know each other. Put proper guidance and support in place for lecturers. The existence of a network of people already working together and sharing knowledge will significantly increase the chances of success. Start out small and try out different set-ups and metadata options. Continue to make adjustments until you find the correct set-up and method. Keep it simple and ensure it is aligned closely with daily practice. Do not underestimate the nature and extent of the work and/or the project. It is difficult material to get your head around. Changing people’s behaviour as well as their working methods will be no mean feat. Be aware of technical limitations when developing an item bank across institutional boundaries. Literature "],["step-by-step-plan-1.html", "Part 2 Step-by-step plan 2.1 Step 1: Preparation 2.2 Step 2: Action plan 2.3 Step 3: Design 2.4 Step 4: Pilot 2.5 Step 5: Real-life deployment", " Part 2 Step-by-step plan 2.1 Step 1: Preparation Start by formulating the purpose of the item bank. Consider the role of the item bank within the teaching process, phasing (where applicable) and ownership of the item bank. Who will you need? How can you gradually build an item bank that adds value to the teaching process and will stay ‘fresh’ for longer? See H3.1. Consider whether the development of an item bank is in line with the policies of the institution and whether it supports the educational vision. Does it fit in with the digital assessment policy? What are the qualitative and quantitative goals that your institution wants to achieve through digital assessment? Make sure that the overall purpose of your item bank is in line with this policy. This will ensure that the item bank stands a greater chance of success. Commitment in the form of time and money will be required before you can elaborate an idea. Each institution has different procedures for obtaining this commitment. This often involves administrative procedures in which various people will discuss things or lobby at operational, tactical and strategic levels. There are usually guidelines and procedures available for launching a project within an institution. Examples include innovation funds that can be made available through teaching awards or incentive measures. For example, you may need to formulate a project launch memo or project description to help arrange the resources. An initial cost-benefit analysis may already be needed at this stage. It is advisable to define and describe at least the following aspects while you are laying the groundwork: The current situation and the desired situation. The purpose of the item bank. What makes it a good idea? What is the benefit for students? Make clear how the item bank relates to other policy documents, such as the digital assessment policy and the teaching policy. What outcome will be visible when the project is complete? What maintenance and management will be needed in broad terms? What is the theory behind the development of the items? In the groundwork stage, decide on the theory you will be working on: classical test theory (CTT) or item response theory (IRT). What types of question do you want to use and how will feedback be processed? Decisions on feedback and question types will affect the final outcome. See also § 3.6.3.1, § 3.6.3.2, and § 3.7.3. How much time, money and lead time will be needed for the project? Work out how feasible the project is: if necessary, carry out a separate feasibility study or preliminary study . Decide who the project initiator and/or the ultimate owner of the item bank is. Decide on who you will need to help develop the item bank. See also H3.5. 2.1.1 Feasibility study A possible intermediate step in laying the groundwork for your item bank project is a feasibility study. This will explore and elaborate one or more options for organising an item bank. Choose the option that suits your situation best. Consider the following aspects: * Content: What do the items you want to include in the item bank have in common? Is there a common arrangement that can be identified? To what extent do lecturers believe they can reach a consensus on the course content and the items? Knowing this will enable you to decide on your approach. More time will be needed for this part if there are differences of opinion. See also H2.3. Quality: how would you rate the quality of the tests at the moment? What are the p-values and trip-values? How acceptable is this? What improvements do we envisage making with the item bank? The choice to buy in or develop the items yourself. What item banks are already available in the market? Check solutions available outside the institution, such as buying items from an external commercial party. * Is there an option to join an existing partnership that is already developing an item bank? The intended size of the item bank. How many items are already available for use within the institution? How many items will you need? See also H3.3. Technology (assessment systems and item bank systems): Check what item bank systems are already available within the institution and item banks that are available for use. Find out which department is responsible for the management and use of ICT applications and, specifically, assessment applications. Is there anything that the institution needs to buy? See also H3.2. Organisation: is there sufficient support for these option(s)? Talk to organisations that have experience with organising item banks. Funding: detail the costs and benefits for each option and consider the following aspects. Can sufficient time and money be made available for the development of the item bank? To what extent will the item bank be able to improve the price-quality ratio (‘value for money’) of items? Can you raise project funding or subsidies? See also H3.4. 2.1.2 Completion of groundwork stage Process all the data you have in a written project description and obtain approval from the project initiator to start writing an action plan. What will you have achieved by the end of this step? Outcome: insight into the purpose, feasibility and overall approach of the idea. A go/no go from your project initiator for the next step. 2.2 Step 2: Action plan In this step, you will decide how the item bank should look once the project is complete. You will make an action plan to achieve this result. Ask yourself: what will we have achieved by the end? The answer will depend on the purpose that the item bank will fulfil in the teaching process. Formulate the relevant requirements (hard and soft) that the item bank must meet. Hard requirements are those that absolutely must be met; soft requirements are those that people want to achieve, but are not strictly necessary to achieve a satisfactory outcome. Include both in your action plan. In some cases, an organisation may facilitate project-based working and even a specific project management method, such as Prince II. Project leaders may even be available to oversee larger projects. In that case, there will be established procedures in place within the institution regarding working methods, phasing and decision-making. Check what the institution offers in terms of project-based working. Use standard documents wherever possible, such as a template for an action plan. Set out steps from the step-by-step plan in the action plan. Make sure that your action plan describes at least the following aspects in as much detail as possible: The purpose and outcome. How you want to achieve the outcome. The role of the project initiator and who this will be. Ideally, the project initiator will be the future owner of the item bank. Plan sessions to discuss the approach and the plans with this person. Decide what you will need his or her help with. Think about who you will need during the project and once it’s complete. Do they already have the right knowledge and skills? Consider the following areas of expertise: client, project initiator, assessment expert, psychometrist, functional administrator, legal counsel and lecturers. Decide how you will create support. How will you ensure that the item bank is actually used? How will you encourage lecturers and students to put the new working methods into practice? Talk to them about this. Consider how you can ensure the importance of the item bank is placed on the tactical and strategic agendas. Who are the stakeholders? Costs and benefits: provide an estimate of what the development of the item bank will cost and what its benefits will be in the years ahead. Quality: how will you ensure you produce an item bank of sufficient quality? Timeline: what phases do you have in mind? What lead time will you need? Consider how you can achieve the objective you have set, and how you can control the process of getting there and making adjustments as needed. What will you have achieved by the end of this step? The outcome of this step is an action plan in which the purpose, the outcome and how you intend to achieve this are defined as specifically as possible. Invite the project initiator to approve the action plan. The action plan ensures that the time that people will need has been agreed with their superiors. This will ensure commitment and support. 2.3 Step 3: Design By the end of this step, you will have a design that makes it clear to the project initiator what deliverable they can expect to receive. What’s more, the employees involved in the next step, testing in a pilot, will know what needs to be done. The following aspects are defined and described in the design: Arrangement of items Item development process Item quality System Production and planning Funding Legal aspects that need addressing Management Clearly document the arrangements that are agreed for each focus item. The outcomes of each component add up to the overall design of the item bank and are therefore also interrelated. 2.3.1 Arrangement of items Map out the extent of commonality between the disciplines. Invite the lecturers concerned to decide whether and, if so, to what extent they view a subject or discipline in the same way. They will work together in-depth to decide what is included in the item bank at the subject level and the question construction level. This phase is a difficult one because it is here where differences will come to light. It is important to remain pragmatic. Accept that some adjustments may be necessary in the teaching process, for example, you may have to choose a new teaching method or modify the use of specific names for variables or concepts. Questions that come up: How do you describe the items? What do you consider relevant or programme-specific? What do the test matrices look like? There may be differences in notation or illustration. How do you deal with this? Decide what hierarchical structure the item bank will have and what metadata is desirable. See also H3.6. 2.3.2 Item development process Decide who will run through which steps in the item development process to ensure the assessment quality. Prepare an estimate of the expected development time per item. Consider the current process steps. Fine-tune these or expand them if necessary. Do existing items need to be reviewed and approved? How much? See also H3.5. 2.3.3 Item quality Decide on and document guidelines that items must comply with. Consider also rules on language use and citation of sources. Wherever possible, involve item experts (not everyone makes good items) or a psychometrician. Plan review sessions led by a question expert/coach. Invite lecturers to submit a large number of existing sample items. Together, discuss the variation between the disciplines, the manner in which the questions are set and the construction errors in these items. This will give the stakeholders ideas on standardising the formulation of questions and question formats. Consider how you can measure the desired quality improvement. Start by thinking about the outcome: identify the data you want to be able to retrieve from the item bank for your educational process. See also H3.7. 2.3.4 System For a simple item bank project: see which assessment system the institution uses. Does the system support the envisaged item development process, roles and privileges, hierarchical structure and metadata? For a complex item bank project: if there are multiple systems in use, decide which system is most suitable for the purpose. Does the envisaged item development process fit the system? You will no be able to calculate how much work it is to organise. If it does not fit, decide whether you can modify the process, or whether you would prefer to investigate whether the structure of the system can be modified. State what is needed to ensure the safety of the items in the bank. How scalable should the system be? Not just now, but in the future too – as far as you can tell? See also H3.2. 2.3.5 Production and planning In the groundwork phase, you will have roughly determined how many items you will need in the item bank. Convert this number into time (short and long term). How much pressure will this put on employees? How many items are already available from the various stakeholders, and can these be contributed with relative ease? Think about how you will approach filling the item bank. Determine the period, any deadlines (when will the first tests take place?) and target production by scaling up (batches). Decide when to conduct quality reviews based on assessment statistics and psychometric data. See also H3.3. 2.3.6 Funding Further elaborate the cost-benefit analysis from the previous steps. Consider how the initial development will be financed. Determine how you will fund the operation of the item bank once the project has run its course. Prepare an operating plan. Opportunities to generate revenues include: Institutions take out a subscription to the item bank, which they then offer to students free of charge. Students pay for access to the item bank. Involve the project initiator of the item bank when preparing the operating budget. Ensure that he or she feels responsible for this. See also H3.4. 2.3.7 Legal aspects that need addressing Decide who owns the items in the item bank. Think about how you want to handle third-party material that is copyrighted, such as images included with items. Decide what you need in terms of personal data protection. Discuss what arrangements you want to make with the other stakeholders about collaboration. At the very least, establish how many hours will be provided for item development. Engage a legal expert in inter-institutional collaborative partnerships in order to assess what form of collaboration will work best and what will be needed to achieve this. See also [H3.7.4](#legal-aspects-that-need-addressing]. 2.3.8 Item bank management Think about how you can best support the users, such as lecturers and students, with getting the most from the final product. Talking to them and listening to their arguments will help here. Invite them to contribute their ideas on the design. Decide on who or what you will need to manage the item bank. Who will you need to provide support and administration once the project is concluded? Create instructional materials and plan instructional sessions. Talk to the testing and management organisation, where relevant, to coordinate the working method. See also H3.5. What will you have achieved by the end of this step? A design for the item bank. All interim outcomes for each component will add up to the overall design of the item bank: Structure: a map/table with the agreed subject structure and arrangements regarding notation methods, for instance. Organisation: approach and documented organisational arrangements (description of the testing and administration organisation) on how the item bank will be managed, including governance of the item bank, including the necessary knowledge and skills of the parties concerned (a training plan). System: organisational plan system; this concerns the workflow (of the item development process) and the meta structure (hierarchical structure and metadata structure). Quality: agreed arrangements on the envisaged quality of the items, including a description of guidelines and criteria that the items must comply with. description of the working procedure for item development, including management, analysis and roles and privileges. an indication of the time it will take to develop an item. an indication of the increase in and consistency of the item quality. Production and planning: approach and documented organisational arrangements on how to ‘fill‘ the item bank and keep it up to date. Financial: a business case and operating plan. Legal: agreed arrangements on the legal aspects of the items and the system 2.4 Step 4: Pilot The design is complete. Now set up the item bank and prepare it for use. In this step, you will set up an item bank structure, enter metadata categories, create accounts, convert existing items and assign roles and privileges. Run a pilot in which you are fully operational for an initial assessment period. In the pilot, you will be testing whether the chain of activities works. Create enough items so that you can cover the initial assessment period. Check whether the agreed arrangements are feasible in practice. Are the assumptions correct? Does everyone know what roles and privileges they have? Test whether the systems and the method work from a technical perspective. Are the items used by students as intended? Is formative assessment used? Evaluate and adjust as necessary. The following activities will be carried out during the pilot: Set up the item bank from a technical perspective and test it. If old items are to be converted to the new situation: perform the conversion with a small number of items. Train employees. Enter a first batch of items based on the agreed working method Prepare a test using the available items. Offer the test in a course or module. Evaluate the use and results of the test. Establish an administration system for people and funding (for example, if a fee per constructed item or for coordinating role has been chosen). Evaluate the use and procedures for creating items. Is there a better way to control quality? Are the assumptions correct? For example, the estimated time for question development? Where necessary: make adjustments, fine-tune the arrangements made, modify the assumptions. What will you have achieved by the end of this step? A documented evaluation of the pilot in terms of the various design aspects and from the perspective of the users, and an improvement plan. An established and tested item bank. 2.5 Step 5: Real-life deployment In this step, you will use the item bank in real-life situations. You will also take action to increase the use of the item bank. The pilot will have generated a number of lessons. Based on these, you will be able to scale up. During the real-life deployment, larger numbers of items will be created, and more and more tests will be deployed. Points to consider: Ensure that the item bank owner continues to follow and monitor the objectives and outcomes. Regularly discuss the topic of embedding with the item bank owner. Are the item developers creating sufficient items? Are the items of good quality? How can you ensure the least possible variation in quality? Is everyone satisfied with the process? Is it easy for lecturers to create tests? Do students consider the formative items developed in this way (including feedback) appealing? Check the ongoing viability of the business case. See also H3.7. 2.5.1 Aftercare The item bank has been implemented. The process is operational, everyone is performing their tasks and using the item bank in the agreed way. There is a formal owner who monitors the purpose of the item bank. The item bank has found its place within the organisation: there are people who ensure the item bank’s continued existence and proper use. The added value for the teaching process is clear for now and in the future. Points to consider: Ensure that the maintenance of the item bank is carried out, for example, by a functional administrator and the editor(s). Ensure that the financial administration functions as intended throughout the operating phase. Keep an eye on whether operation will provide an appropriate level of funding in the future. Ensure proper documentation is available, such as: a description of how the item bank will be used in a course. a description for testing and management of how the item bank will be maintained. a description for the coordination team, where applicable, and for each team of authors for each item bank of how the item bank will be maintained. See also § 3.7.2.3. a standard instruction programme for use of the item bank system for new employees. changes to the instruction programme when the item bank system is updated or amended. "],["going-deeper.html", "Part 3 Going deeper 3.1 Why create an item bank? And why collaborate? 3.2 Technology: Systems 3.3 Item bank size 3.4 Costs and benefits 3.5 Organisation: process steps, roles and privileges 3.6 Organising an item bank 3.7 Didactics: quality of items", " Part 3 Going deeper 3.1 Why create an item bank? And why collaborate? You describe the purpose of the item bank in the preparatory phase. In this chapter, we describe a number of possible ways in which an item bank can have an impact on teaching. We will also elaborate on the benefits of collaborating on an item bank. Finally, we will examine which factors within the professional field contribute to a successful joint item bank project. 3.1.1 Why create an item bank? Decide what role the item bank will play in the learning and assessment process. An item bank is often created with the aim of having an impact on education, such as improving quality or increasing efficiency. Examples of the intended impact are: Psychometric: increasing the reliability and validity of tests. Accountability and transparency: the launch of the item bank will provide more opportunities to account for the quality of assessment in a transparent manner. For instance, it will provide a good view of the entire item bank and the match with the test matrix. Assurance: a more verifiable and less error-prone production process of items, for example, by verifiable version management of items. Sustainability: items are managed within the item bank independently of a specific lecturer. So they will not disappear when a teacher leaves. ** Test security** it is easy to produce multiple test versions and therefore prevent cheating. ** Test security** it becomes easier to develop tests in a secure way, for example, because items are no longer forwarded through email systems. Efficiency: reusing items reduces costs. Items only need to be typed once, editing and improvement can be performed much faster, and items can be more easily selected for inclusion in a test. Educational quality: reusing items allows lecturers to spend more time teaching. Educational quality: there will be more practice opportunities and opportunities for level differentiation. Educational quality: resits and better feedback opportunities can be organised more quickly. Educational quality: the study success rate will be higher, as students will be able to practice and internalise the subject matter better. Utility: investments in items that have already developed are more effectively recouped. 3.1.2 Why create an item bank? And why collaborate? There is a lot to be gained from collaboration, both within and between institutions. The joint creation and use of item banks can even lead to a significant qualitative and quantitative benefit in the short term. Why? Firstly, lecturers who jointly set up an item bank will have to enter into a dialogue based on clear terms. They decide together how they will talk about item development and what minimum quality requirements they set. This already helps to raise the standard of quality. Secondly, the use of assessment experts in the collaboration is self-evident. They can contribute to the assessment content but also, for example, in the field of test competence, editing and layout. This will also help improve the quality of the items. See § 3.7.2 about the various roles in item development. Thirdly, when you collaborate, you will agree more rules about the item development process. This means that the item developers will become more aware of the various development phases, the responsibility and assurances in the process. If item developers adhere to these rules, this will lead to an increase in quality. This will allow for better control of quality because the process is assured. Finally, collaboration can lead to lower costs. Initially, collaborating will entail more costs, but once developed items are used more often and by more lecturers – and therefore used by more students – the cost of use of the items will reduce on balance. For a further explanation, see Hoofdstuk 3.4. 3.1.3 Potential fields for successful collaboration A shared vision within the field of study could help you achieve a successful item bank more quickly. The more people within a given field agree on the classification of the subject matter, the more self-evident it will become to develop item banks together. Fields of study with a common body of knowledge (BoK), such as teacher training programmes or the medical field, are good examples of this. Subject areas in which most study programmes provide introductory courses are also suitable for organising joint item banks. Examples include Introduction to Psychology, Introduction to Sociology, Introduction to Neurobiology, and Introduction to Programming. Stable domains lend themselves well to joint item banks that can be set up with little maintenance. Classical subjects such as logic, mathematics, statistics, mechanics, accounting and the fundamentals of microbiology lend themselves well to such collaboration. Once developed, questions will stay fresh for a long time. For disciplines that are constantly developing thanks to new understanding, it will be important to maintain items. Outdated or obsolete items must be identified and deleted or updated. This process must be structured systematically in order to ensure the quality of the questionnaire. For instance, a yearly review of all items to check for freshness. When you collaborate with others, it is essential to agree who will do this and when. Examples of subject areas to which this applies are the medical field, nursing, tourism and history. Collaboration to share costs and increase quality is an interesting driver here. Subject areas that are constantly developing in terms of content (volatile domains) require an even stronger focus on the design of the item bank system and the assurance process. Think of subjects where recent legislation is always important, such as Law, where adaptation to new protocols is always happening, such as Nursing, or where work is largely thematic. Ongoing attention is required for screening, editing, deletion or supplementation. A process has to be established to deal with this. In subject areas in which large numbers of students are taught, item banks are also more suitable for collaboration, because the investment costs and maintenance costs per student will then be much lower. 3.2 Technology: Systems You will conduct research into the item bank system to be used when preparing the project. In the case of a small project, this will probably be the institution’s central assessment system. Find out within the institution which department is responsible for managing educative ICT applications and, in particular, assessment systems, and discuss your wants and needs for the item bank. First and foremost, item bank systems must facilitate: proper entry and organisation of items, as well as search and selection activities relating to quality assurance of the items. Assessment systems come in many types and sizes and offer many features that meet these requirements. The core components of a digital assessment system are the authoring environment, the item bank, the playback environment and the analysis tool. In Hoofdstuk 3.5, we will take a deeper dive into the underlying processes of these components. 3.2.1 Commonly used assessment systems in education This section provides a run-down of the most commonly used assessment systems in higher education. The list has been compiled by experts involved in the authoring of this manual. The market for assessment system suppliers is dynamic, and for this reason the list is incomplete.Commonly used assessment systems in the Netherlands: TestVision RemindoToets MapleTA Cirrus Assessment QM iQualify Ans Delft Assessment systems within Learning Management Systems with limited item bank functionality: Canvas Blackboard Desire2Learn BrightSpace Moodle Cumlaude Item bank systems (stand-alone programs) with direct item export functionality: Respondus ExamView Assessment systems with questions: SOWISO (Mathematics) Grasple (Statistics) Zeer Actieve Psychologie R/Exams (Statistics) Drillster Quizlet ProProfs Item banks by publishing houses, often corresponding to a specific textbook or online learning environment (e.g. Pearson MyLab series, (McGraw Hill methods, WebAssign etc.) :::: 3.2.2 Sharing test items between systems Not every educational institution in the Netherlands uses the same assessment software. So how can you still collaborate on item banks? It is often possible to convert simple multiple-choice questions from one system to another. For example, institution A creates the questions and institutions B and C convert them for use in their own assessment system. Ideally, however, you will work in the same authoring environment, follow a joint item development process, and all the questions and data about the created items will flow back to a single item bank. 3.2.2.1 QTI Standard In recent years, research has been carried out in SURF’s own network into interoperability and the ability to share questions between the various systems. At the international level, development of the IMS QTI standard (Question and Test Interoperability)(https://www.imsglobal.org/question/index.html) is ongoing. This standard will not solve all interchange issues in the short term, because some systems have question types that others do not, such as pointing to multiple points in an image or unequal matching questions. At the same time, QTI already works in many cases for multiple-choice tests. In practice, therefore, a more complex collaborative venture between multiple institutions means that most of the participating partners would have to work in a system other than their home system. If you cannot work in a single item bank system during the collaboration, limit the question type to be used to the multiple-choice question. This form is so simple that exchange via QTI or plain text usually runs without a hitch. Assume that metadata and media (images, sound) will not transfer well with you when you export or import items. During the preparation, perform an item interchange test. This will allow you to establish whether the transfer is working or whether conversion tools need to be created. Try to use the same system wherever possible, even when working across multiple institutions. 3.2.2.2 LTI and SCORM standard Assessment professionals often use the IMS LTI (Learning Tools Interoperability) standard and SCORM (Sharable Content Object Reference Model) standards. However, these are not item interchange standards. These standards are used to link assessment systems to other systems, such as learning management systems (such as Blackboard or Canvas). Lecturer and student data, in particular, is automatically exchanged between the systems, assessments can be initiated from the LMS, and the test scores are fed back to the grading functions of the LMS. This creates a more distinctive experience of the LMS for the end-users and they do not need any extra logins. 3.2.3 Security and scalability of item banks Summative items, in particular, must be developed and stored in item banks in secure conditions. The greater the importance of the tests, the more thought needs to be given to security. You also want to prevent the distribution of items during the administration of exams, especially where questions have been calibrated and item banks are small. When administrating exams by computer, it is important to use a protected environment (secure browser), but also to check carefully for mobile phones, glasses, watches and other devices that may be able to take pictures of the screen or paper. Professional invigilators remain necessary. At the same time, during the coronavirus period of 2020-2021, a huge leap was made in administrating exams and tests using online invigilation (also widely known as online proctoring). Online proctoring allows students to be monitored remotely by collecting images of the student – using his or her webcam – and of the screen. For more information, see SURF’s publication Secure Assessment Workbook. Scalability is closely related to the desired size of the item bank. Be sure to consider the addition of any external authors, the performance of the authoring environment, the possible growth of the number of questions and subjects to be added, and metadata. 3.3 Item bank size How do you decide on the number of items in an item bank? During the preparation, you will already be giving some thought to the required size, because this will give you some idea of the amount of time and the resources you will need to create the item bank. Sometimes impressive numbers of items are quoted, but the required size actually depends on many factors. First of all, you will need to know how many items you want to include in a test. Can you and are you allowed to reuse items? Do you want to use the items in a summative or formative test? This can make a difference for the required number. In the sections below, we outline scenarios for various tests in which you might want to use the items. 3.3.1 Summative tests conducted simultaneously for all students Most tests in higher education are conducted as classical examinations. One lecturer or team puts together a test for a group of candidates for one course from a single institution. All students sit the exam at the same time. What will the situation be then? Test questions released Test questions not released Let’s say that the test questions will be released following the test, that you compile three different versions of the test and that the test consists of fifty items. In that case, you will need an item bank whose size will have to increase by 150 items for each test. After five years, there will be 750 items in the item bank. Institutions often stipulate in their assessment policy the rules relating to what is and what is not permitted with regard to the reuse of items. Let’s say that the test questions will not be released following the test, that you compile three different versions of the test and that the test consists of fifty items. In this case, an item bank of 150 items will be sufficient. Let’s say that the test questions will be released following the test, that the position of a question in the test is randomly selected from one of two positions, and that the test consists of fifty items. In that case, you will need an item bank whose size will have to increase by 100 items for each test. After five years, you will have an item bank of 500 items (Draaijer and Klinkenberg 2015). Let’s say that the test questions will not be released following the test, that the position of each question in the test is randomly selected from one of two positions, and that the test consists of fifty items. In this case, an item bank of 100 items will be sufficient. 3.3.2 Summative tests that can be taken at any time There are also tests that are administered whenever requested by the student. Here, there is a risk that students will collect items and share them among their peers. Or that students will enrol for the test multiple times in the hope of answering the same items to which they then know the correct answer. Some people argue that it is not necessarily a bad thing if test questions are known, as long as the item bank is large enough. The reasoning is that if students can practice using all the items, they will learn the subject matter by themselves. However, there will always be a type of student who will try to memorise the correct answers so that no real knowledge or understanding is acquired. If they pass this test, it will impair the validity of the test. How many items are needed to prevent this is impossible to ascertain. Nevertheless, item banks holding at least 1,000 to 2,000 items would seem necessary. In all cases, it is advisable to make sure that students answer new questions. A minimal variant is that alternatives from multiple-choice questions do vary in terms of content or their position in the test. In addition, parameterised test items could be used. These are questions in which numbers, objects or concepts are drawn from a collection (Fattoh 2014). At the start of the Dutch Open University’s item bank project, the guideline in use was that there should be fifteen times as many items in the bank as the number of items in a test. This requirement comes from the three examination opportunities per year and the assumption that students would complete the study programme within five years. For lecturers, it was almost impossible to create so many items, although about fifty item banks managed to succeed in doing so. The OU had the software modified so that students could only be presented with the same item again after five times. This made it possible to reduce the requirement of fifteen times to six times, which was a relief for the lecturers. Another requirement was that the item bank would be able to produce valuable psychometric analyses after one year. This would mean that each item would have to be answered at least thirty times. For some item banks, however, the requirement of fifteen times was already far too high; for example, when only ten students sat the exam for a particular specialisation on an annual basis. 3.3.3 Formative diagnostic tests Formative diagnostic tests that represent the classical examination correspond to the sitting one to three representative one-off tests. In this case, it does not matter if the test questions are leaked. The reason being that it is the student’s own responsibility to take a diagnostic test seriously or not. If the idea is that students can sit a diagnostic test very frequently, the most sensible choice seems to be to determine how many tests like this you want to offer. If two are available, students will usually check the first one at the beginning of the semester to assess the desired learning outcomes, and the second one just before the summative examination to perform a self-check of whether they have mastered enough of the subject matter. The number of items required for summative tests that can be administered at any time is not relevant here. 3.3.4 Formative practice tests Formative practice tests require fewer items for each topic. First and foremost, the student will need good instructional items on key, difficult topics. For each topic, twenty or so items will probably suffice. If a subject area consists of twenty topics, approximately 400 items will then be required. 3.4 Costs and benefits During the preparation, you will assess the financial feasibility of your project. You will make an overall estimate of the expected investment in the item bank in terms of investment costs and running costs. In addition, you will assess the quantitative and qualitative advantages or benefits that the item bank may provide. You will try to find answers to the questions ‘why invest in an item bank? ’and ‘do the benefits outweigh the costs?’ Performing a cost-benefit analysis will help you find the answers to these questions. In the case of large investment plans, a business case is something that is increasingly being requested. A business case answers the question ‘why do we actually want this?’ It underpins future decisions and helps everyone to understand why the project is important. The development of item banks can form part of a larger digital assessment implementation process within an institution. This will often be based on a business case. The costs and benefits of a proposed item bank will form one of the components of the business case. 9 For more information on the business case, see the publication Theme edition: Testing and question banks in education, Jan 2017 (chapter on Costs and Benefits of an Item Bank, page 12; only available in Dutch). 3.4.1 Costs Developing an item bank will require an investment. These are the project costs. Subsequently, you will need money for running costs and for ongoing development and administration. 3.4.1.1 Project costs The initial investment consists of creating a collaborative venture group, developing a plan, creating the organisation, consulting and negotiating on the structure of the item bank and its working methods, licencing and organising the administration of an item bank system, developing an initial number of items for the item bank and conducting the first tests using the items contained in the item bank. Different time budgets are used for the development of items. For simple knowledge questions in a multiple-choice format, especially if it is also easy to formulate distractors, it may sometimes be possible to develop four items per hour. Questions concerning case studies or application usually take more time to develop, up to an average of 60 minutes per question. This time can increase if an item also needs accompanying audiovisual material and feedback. If questions are to be used more than once, it is wise to review the quality after their first deployment in a test. Experience shows that after the first deployment, as many as half of the items will need to be modified. This can be a textual modification or may concern the subject matter itself. In the progress test for medical students, the development of a single item involves a large number of people. Despite this thoroughness, about one percent of the items are later found to be sub-standard in terms of content. To keep track of costs, log the hours of the project team members and multiply the total hours by their hourly rate. Include costs of external hires too, as well as the cost of resources used (such as systems, rooms, travel expenses, training, materials). Also include an amount for contingencies, which you can substantiate by referencing potential risks. A number of opportunities to finance the development of more complex item banks: ‘Closed exchanges’ are used for the development of the item banks. Decide who will create and review how many items and the time periods for doing so. No invoices will be sent between the team members. Item developers receive a fee per constructed and approved item. As a result, it will not matter which lecturer or institution creates the most items. In this case, however, out-of-pocket expenses will have to be available to fund item developers. The coordinating institution or lecturer will receive a separate fee because the coordinator will spend relatively more time and will have greater responsibility. Day-to-day running Following completion of the project, the item bank will have to be maintained and further developed. During the project, you will make a plan of what this will entail. Among other things, the day-to-day running plan will stipulate that workers will be given time to continue developing items, keep the item bank live and administrate it. Assume that all items will be reviewed every couple of years, for example, to recode them due to curriculum changes, new books, etc. Remember that without a joint item bank, the cost of developing items will in many cases be untransparent. It takes time and money each time you create items on an individual basis, and the results may be of varying quality. 3.4.2 Benefits WatWhat are the benefits of the item bank? § 3.1.1 sets out the possible types of impact the item bank may have. Try to formulate these quality goals in such a way that they can be expressed in financial terms. This is often difficult and will depend on the strength of arguments. Benefits may include long-term operation, where funding is secured, for example, through a lump sum, membership contributions or funding based on test administration by students or lecturers. Map out in advance the type of funding you want to secure. Make sure that the parties commit to this as early as in the preparation stage. Benefits may also include lower costs and quality improvements. Once developed, items are used by several lecturers and, more importantly, by several students. This means that although there are costs involved in developing an item bank, on balance, the ratio between cost and quality per test taken per student is better. 3.5 Organisation: process steps, roles and privileges You will arrange the development and administration in the design of the item bank. You decide who performs which tasks and how. Insight into the global process steps associated with an item bank can help you to gain a clear overview of these tasks. 3.5.1 Components and process steps Item bank systems often consist of disparate components with related functions. Core components are usually an authoring environment, an item bank, a playback environment and an analysis tool. Process steps within each of these core components can be identified. Take, for instance, the features of the item bank. We can distinguish between processes for entering, checking and modifying items, adding metadata, organising and selection. Figure 2 depicts a number of process steps that can be taken within the various components of an assessment system. Auteursomgeving Auteur schrijft item Review door collega reviewer Redactie door toetsexpert Specialist metadateert item Gatekeeper geeft items vrij Afname omgeving Toetsbureau zet toets klaar Analysetool Docent analyseert toets met toetsexpert 3.5.2 Roles and privileges Who can access which information in the item bank? Who can modify or add something? You decide who can do what by assigning roles and privileges. In most item bank systems, the possible roles and privileges have already been defined. It is then up to you to link people to the system. Some systems make it possible to define roles yourself. Preferably, start by deciding which roles and privileges you want to be able to distinguish before you choose a system. Examples of possible roles: Administration Super user: can access all items and can set privileges for all users. Coordinator: has access to and can make changes to a sub-set of the item bank and a sub-set of the users. Item development Author: may add and modify items. Controller: may modify and comment on items. Psychometric specialist: may add and edit metadata and comment on items. Editor: checks linguistic and spelling mistakes, uniform use of language and uniform formatting of items. Illustrator/multimedia specialist: creates uniform and useful images and adds them to the items. Copyright controller: ensures that the use of images and texts is legal. Gatekeeper: performs the final check of an item. Examinations Board member or Inspection Committee member *: can view the item bank to verify that any item bank policy incorporated in a study programme plan has been implemented. External validator: an individual from outside the development team who inspects the item bank. Examples of different privileges in an item bank system: Access to all items. Access to sub-sets of the item bank. Read privileges, modify privileges, delete privileges, move privileges, add privileges, comment privileges, metadata privileges. Privileges to create tests. Privileges to stage tests ready for administration. Privileges to perform test analyses. Tip Be frugal in assigning different roles and privileges. The more complex the number of roles and privileges, the more complex the workflow, the more time and energy the editorial team will have to spend on arranging and administrating them. A lack of good administration will reduce user-friendliness and stagnate the item development process. 3.5.3 Automated workflow The current assessment systems provide working procedure support — to varying degrees — for collaboration on items. This concerns the process that an item undergoes from conception to finished product and ultimately to deletion. The way the working procedure for item development is organised is therefore already largely static. When choosing a system, make sure that several people can work in the system at the same time and can even work on the same items at the same time. In web-based assessment systems, this is usually possible by default. A system of roles and privileges makes it possible for items to be made available sequentially to different persons for revision, checking or approval. This is the automated workflow. In a workflow, you can specify that an item can only progress to the next phase once the previous phase — for example, grammar checks — has been completed. Assessment systems keep a history of each item. This provides insight into who has worked on which item and when. If any errors have taken place during the process, it is possible to roll back to an earlier situation if needed. Tip If the quality is already high on deployment: organise a more comprehensive workflow process with a variety of check events. This will optimise the opportunities for raising the quality of the items. In § 3.7.2.2 you can read more about controlled item development. 3.6 Organising an item bank If an item bank is well organised, this will help provide insight into its structure and its suitability for composing tests. Good organisation also helps with the efficient management of items. At the same time, practical experience shows that organisation alone can never produce a fully watertight system. However, it is possible to create a watertight system for a single test. If you use a single item bank to select items for tests across multiple disciplines, it will be an almost impossible task. How should an item bank be organised? Establishing what everyone understand by the term is essential to avoid confusion. Make a solid distinction between the structure of an item bank and metadata. This chapter discusses these concepts and classification principles. 3.6.1 Metadata Metadata are pieces of information that are added to items. In the case of items within an item bank, metadata will be the main topic of the question, for instance, a sub-topic of the question, the status of the item (draft, pending revision, pending editorial control, approved, rejected), the degree of difficulty or discriminating capacity, the type of question, the taxonomic designation (e.g. knowledge, application, insight), the function of the question, who created or edited the item and the tests the item is used in. Figure 3.1: An item with some metadata fields added. 3.6.2 Structure The structure of an item bank is the division of the item bank into independent, often hierarchically ordered units. This is similar to the folder structure found in the file explorer of a computer. Generally speaking, users need this clear hierarchical structure to help them understand the structure of the item bank. This structure is more useful than a structure based solely on metadata. In practice, what works best is to first choose the hierarchical structure and then add metadata to the questions. The structure and metadata are often interwoven technically in different ways depending on the item bank system. Users can use the structure and metadata separately to classify and select items, but sometimes also in combination. This makes it even more difficult to comprehend structure and metadata. Some examples of existing item bank structures: The VUmc has a separate item bank within the item bank system of Questionmark Perception. Figure 3.2: The main structure of the VUmc follows the separate course-dependent tests (CAT) that are administered within the curriculum. Figure 3.3: The main structure for an item bank on statistics at the Faculty of Social and Behavioural Sciences at VU Amsterdam. This item bank is part of a course in Blackboard. The item bank follows the subject classification within statistics with a distinction between Advanced/Basic and level of difficulty. The Kennistoetsenbank contains item banks for users of Pedagogical Work (PW), Social Care (MZ) and Nursing and Care programmes within MBO Care and Welfare. Figure 3.4: These item banks are classified according to: question types, the structure of the body of knowledge (BoK), the qualification dossier (QD) and special attributes. The image shows how the item bank of Pedagogical Work (PW) is configured. Figure 3.5: A lecturer from the Microbiology Introduction course has created an item bank in QuestionmarkLive. The structure follows exactly the structure of the textbook that he uses. Structure of the LUMC questionnaire. Leiden University Medical Centre (LUMC) has an item bank based on the RemindoToets item bank system. Figure 3.6: Initially, LUMC opted for a thematic classification based on disciplines from the national Progress Test. This structure led to problems due to the poor findability of items for lecturers and also problems with the maintenance of the questions by lecturers. Figure 3.7: A more pragmatic organisation based on programmes and curricula has now been adopted. Structure of the Toets &amp; Leer item bank. Figure 3.8: Each item bank within the RemindoToets item bank system of Toets &amp; Leer offers three categories: Subject – Learning objective – Test objective. This means that you can only expand three categories to search for a test question, i.e.: subject, learning objective and test objective. The art has been to make the list of topics not too general, but at the same time not overly detailed. 3.6.3 Use structure and metadata How should you configure the structure and metadata of a specific item bank? First of all, the structure should help to achieve the purpose of the tests – which are composed of the items – as efficiently as possible. The test matrix is authoritative here. If there is no test matrix, the starting point should be a sequence of items that students can use to practice. The structure and metadata will help you to select the items, make them findable and add them to a test. They make it possible to organise the large number of items in such a way that the user can work effectively with them and, for example, compile a test with the desired attributes. Tip For metadata, the adage ‘garbage in, garbage out’ holds true. If there is too little time or money to maintain the metadata consistently and accurately, the metadata will lose its value. In that case, it’s better to have less metadata and make sure that the quality is high. A basic principle for organising the structure and the quantity of metadata is ‘less is more.’ Maintain a level of complexity that is just enough to achieve the purpose that the tests serve and to safeguard the basic steps needed for quality assurance. Secondly, organising the metadata facilitates the process of item development and quality assurance. It is therefore important that metadata follows the form of the development process steps. See also § 3.5.1. The same applies to metadata that allows you to identify outdated items or select items from a particular author. Thirdly, you must configure the metadata in such a way that it can form the basis for meaningful reports and feedback for students. If the purpose of a specific test is for students to receive feedback on how they score on each cell of a test matrix, structure the metadata in such a way to make this possible. If this purpose has not been established in advance, do not structure it in this way. Tip If there is a broad body of knowledge (BoK) or national or international standards that apply in your field, such as CanMeds or the Interuniversity Progress Test for Medicine (iVGT), it makes perfect sense to choose this classification as your main structure. Especially if this structure is already hierarchically organised. Its use can be even more effective if it is also closely aligned with the structure of the curriculum of a study programme. If it is not closely aligned to the curriculum, which is unfortunately often the case, choosing the BoK as your main structure will create problems in practice. It is often more efficient to follow the curriculum (course) structure as your main structure and to link BoK data to this in the form of metadata. Figure 3.9: An item bank with a set of chapters as its main structure. Questions can be filtered based on different categories of metadata. Finally, accept that there are different item banks that are differently organised and have differing metadata structures within a single item bank system. Higher education institutions often seek to achieve a uniform organisation and metadata structure for all items. Practical experience shows that this is not possible. But this is nothing to worry about. Item banks are used in higher education for various purposes and across diverse subject areas and should therefore be organised in different ways with varying metadata structures. However, do try to limit the number of variants. Tip It is very difficult to devise the structure needed to achieve a particular test objective ’on paper’. The best way to devise a structure is to carry out a pilot. Develop a matrix and a test using the item bank system. Then iterate until you find the optimal solution for the given context and the purpose. Make sure that the users of the system are aware of the advantages and disadvantages of the chosen structure so that you can build an appropriate support base. There is a ‘political’ aspect to this, because subdivisions within a field of study always like to see their subdivision represented in a test with the right ‘weighting.’ Especially if you want to compare test results over an extended period of time (such as the progress tests in medical studies), it is vital that the test matrix is not changed too often. Tip Often, lecturers want to be able to stipulate that a certain question within an item bank should not appear simultaneously in a test with certain other test items. The number of possibilities that have to be reviewed to achieve this purpose will increase exponentially as the item bank grows. The ongoing procedure needed to achieve this is a manual one. It is easier to examine whether the tests can be compiled and fine-tuned in such a way that potentially interdependent items can be removed manually after the initial selection of items. Tip It makes sense to assign each item in an item bank a unique code, which itself contains the hierarchical or metadata structure. Items can sometimes be moved accidentally. After a test is administered and only raw data remains, the codes will make it possible to retrieve individual items from the bank. The unique technical ID of an item at the system level is often meaningless. An example of this code: &lt;unique no.&gt; B2_AF12-3_RegcoefT is an item from the Bachelor, second year about Chapter 12 of the textbook by Agresti and Finlay on the subject of the regression coefficient. However, decoding this requires painstaking work by those who enter and check items. It underlines the importance of findability and the use of too many or too few tags.### Feedback and question types Depending on the purpose of your item bank, you will be making a choice about the type of items and the nature of the feedback on the items. 3.6.3.1 Feedback For formative tests, developing feedback for each item is very important. Feedback can provide the student with important pointers for their study. It makes sense to check early in the project what type of feedback you want to develop. This will allow you to make an accurate assessment of the development time required for each question. Especially in the case of practice tests, it is important that substantive feedback is given for each question. Feedback must be sufficiently specific and detailed and, if possible, contain pointers for the student or references to study resources to enable the student to revisit the subject matter. There is no way that you can second-guess why students will choose an incorrect answer over a correct one. So do not even try. Formulate feedback carefully and – preferably – using neutral language. You should not demotivate students, but invite them to continue their studies. 3.6.3.2 Question types You can create items in an item bank based on a variety of question types. Most test and item bank systems support more than ten types. See, for example, this overview of all QTI-defined question types. See also § 3.2.2.1. For practice tests, it is advisable to use a variety of question types. This will increase the appeal of the tests. For instance, these question types allow you to interrogate multiple sub-topics at the same time or to avail of the benefits of computer-based test administration, such as dragging or clicking objects. It is of little consequence in the case of practice tests that each question can result in a different score. For summative and diagnostic tests, it is advisable from a statistical point of view not to use a variety of question types. Choose multiple-choice questions and give each item an equal weighting (e.g. 1 point). This choice will ensure that no problems arise in respect to different scoring options or the manner in which they are answered. This will ensure greater flexibility in being able to pull items from an item bank at random. Varying the number of alternatives (1 out of 3, 1 out of 4 or 1 out of 5 questions) should not ordinarily pose any problems, because the scores are based on 1 point for the correct answer and 0 for a distractor. In multiple choice questions, five possible answers are popular in the United States. In the Netherlands, three or four possible responses are more common. The success of a stab in the dark varies depending on the number of possible responses, but sometimes it is difficult to devise sufficient distractors. Four responses are suitable for subject areas where certain answers are clearly wrong. Examples might include exact subjects such as mathematics or biology. In fields where connections are important or where things are not always necessarily wrong, three responses to a question may be preferable. Examples include communication and most courses in psychology. Be cautious with items describing a case study, including sub-questions. A question type like this makes it more difficult to draw on flexible items because the number of sub-questions can differ per case study. This in turn affects the maximum score that can be achieved. 3.7 Didactics: quality of items Improving the quality of assessment is usually one of the main reasons for collaborating on item banks. See § 3.1.2. The quality of assessment increases in line with an increase in the quality of the items. But how does a tool like a joint item bank help increase item quality? This is what this chapter is all about. There are many definitions to describe the quality of assessment (Joosten-ten Brinke and Draaijer 2014). The quality of an item bank system refers to the extent to which the system makes it possible to develop, manage and select items for inclusion in a test in an effective, efficient and user-friendly manner. The quality of the item bank increases as the items better represent the learning objectives from the test matrix. Quality also increases as the number of high-quality items in the bank increases. 3.7.1 Quality of items If you want to improve the quality of items, focus on two types of fundamental quality in the assessment process: validity and reliability. Item banks can help bring about improvements in both. This applies both to formative assessment and to summative assessment. The validity of a test concerns the extent to which it measures what you intended it to measure. To ensure validity, it is essential that the items are relevant for a particular topic or learning objective. Quality improvement occurs when lecturers become more proficient in creating these types of items. Training and collaboration benefits this process. Item bank systems support this process by providing opportunities, for instance, to provide direct comments, by facilitating collaboration and by structuring the improvement cycle in the form of a workflow. See § 3.5.1 and 3.5.2. Reliability concerns the extent to which the assessment outcome (the score obtained on a test) is reproducible and depends as little as possible on chance. To achieve high reliability, the ‘noise‘ or ’measurement error‘ must be as small as possible. Firstly, the reliability of tests can be increased by including more items in tests. You can increase the stock of items by collaborating on item banks. The number of items that a lecturer can use to compile tests will increase. Reliability is also increased by constructing items that better discriminate between students who have either a sufficient and or a deficient grasp of the source material. Having items that discriminate better will generally also increase validity: you will measure what you set out to measure better. Formulate items from the outset as clearly, efficiently and objectively as possible to achieve a better level of discrimination. There are many manuals and lists of rules of thumb to prevent the most common causes of ambiguous items (‘noise’). See, for example, Van Berkel, Bax, and Joosten-ten Brinke (2017). 3.7.2 How do you achieve quality improvements? You improve the quality of items by reducing fluctuations in the quality of items and improving the quality of questions. This insight comes about from the subject area of operations management, as illustrated in Figure 11. In the picture on the left, you can see that the quality of a product or service is quite low on average, sometimes increasing and sometimes decreasing, but no overall rising trend. The picture on the right shows that the quality is steadily increasing, and the amount of fluctuation is decreasing. A targeted approach is needed to achieve this kind of improvement in quality. Figure 3.10: Graphical representation of quality development. Left: the quality varies a lot; the average quality is relatively low. Right: the quality becomes more constant, and the quality systematically increases. There are two ways to improve the quality of new items: either by developing a greater capacity for constructing items, or by reviewing during the construction process. Item bank systems offer workflow support to help achieve this. In this way, you can systematically improve quality. In order to improve the quality of existing items in the item bank, you can continue to carry out analyses after administrating tests and make adjustments based on interpretations of those analyses. This is in fact a cyclical process, which forms part of the maintenance and administration of an item bank. We will elaborate on this in the sections below. 3.7.2.1 Information and training In order to be able to construct high-quality items, information, training and a good feedback loop in the item development process are indispensable. When you launch an item bank, offer training to the intended item developers. This will help to develop and grow a common sense of quality. Involve assessment experts in the design of the training. Once item development is underway, as many training courses as you like can take place, for example, to coincide with editorial meetings of the author team. See also § 3.7.2.3. In fact, the in-depth discussion of items based on assessment and item analyses constitutes training, albeit non-explicit. Training is therefore an ongoing and recurring part of the maintenance and administration of the item bank. 3.7.2.2 Controlled item development by review You can identify and resolve common pitfalls or ambiguities early by following a step-by-step item development process with built-in review points. It is widely known that it always helps to have a second pair of eyes look at your work. Reviews are performed by peers or experts. You do not have to define the review process or the division of roles in great detail for two collaborating lecturers, certainly not in a technical sense. Today, however, it is required that each item has been reviewed at least once by a fellow lecturer. The approver of an item must also be clear. Do not underestimate this: the consequences of a test developed by one lecturer are the same as those of a national test. A student who fails the exam will not graduate. Item banks for high-stakes tests will require more roles and privileges. Let’s say that a selection of tests are prepared using an item bank and that no resit option will be available. Stakeholders, such as politicians, the public, administrators and student unions, set high standards for transparency and the quality of assessment. Examples include national tests, such as the teacher training programme (‘PABO’) arithmetic test, the test in the context of the ‘10voordeleraar’ teacher training programme or the central test in senior secondary vocational education (MBO). A comprehensive, transparent review process is required. For a description of possible roles in this process, see § 3.5.1. The larger the team of stakeholders, the more important it will be to have a coordinator in place to manage teams and provide project support. 3.7.2.3 Item bank maintenance and administration The maintenance and administration of the item bank includes the discussion of items that perform less well than anticipated. Psychometric analysis of these items shows that they have low Rit values or high or low proportion values. For a detailed explanation of these terms, see § 3.7.3.2. Try to work together to find out what the cause is and what you can do to improve the items. You will regularly find yourself deciding to delete items and to start afresh. These discussions may offer an opportunity to clarify the rules and guidelines for creating items within the team. Sometimes it turns out that the problem is not with the items, but rather with the teaching. In that case, you will need to examine how the teaching and the items can be better aligned with each other. Sometimes, it may be necessary to reappraise items or modify the way items are organised. For example, there may be new understanding within the subject area, changes in topics or learning objectives, or in the metadata and the assessment matrix. You may then have to ask yourself if new sets of items are needed for new topics. If items need to be recoded, issue the item developers with specific development assignments. Editorial team The team of item developers should meet regularly during the development process to monitor the progress of item development. Assign one person responsibility for the item development process. Call this person the coordinator or editor-in-chief. Agree that this person has the mandate to hold people to account in respect of the rules agreed on for item development. The editor-in-chief should schedule regular meetings with all stakeholders. During these meetings, try to foster mutual trust and discuss any problems relating to the item development process. 3.7.3 Test data The data you collected from the administered tests provides you with a powerful tool to improve item quality. Combined with the quality increase that item banks generate, you will have a double improvement. This data is an indicator of the quality of the items and the performance of students perform, or of the quality of the teaching. It is important to distinguish between descriptive and psychometric data. They have very different characters, which must be aligned with the purpose for which you are using the item bank. 3.7.3.1 Descriptive data Simple descriptive data of items concerns attributes such as the number of times an item has been used in a test or answered. Looking at a test as a whole, descriptive data can include how often a student has attempted a test, when the attempts were, how the student has scored, what the pass rates are, etc. This data comes from assessment service systems. You can translate the data into a graphical dashboard for lecturers or students. Lecturers are presented with a summary of the progress of a group of students and the topics that they struggle with. Students will gain insight into their progress and their position relative to their peers. In fact, this is pure management information. The use of data in this way is also known as learning analytics. 3.7.3.2 Psychometric data Psychometric data provides you with insight on the level of difficulty of items and the extent to which they discriminate sufficiently between students who have fully internalised the subject matter and those who have not. However, there is a difference depending on whether you are using classical test theory (CTT) or item-response theory (IRT). In this section, we briefly explain both theories. Classical Test Theory (CTT) The classical test theory assumes that the score in a test will consist of the actual score and an error in the measurement (noise). Statistical operations can be used to calculate the reliability of one test, i.e. in which range the student’s actual score lies with a certain degree of certainty. The most important psychometric data for items within the concept of classical test theory are: Reliability: the extent to which the test as a whole, i.e. all items taken together, are good at discriminating in the extent to which the subject matter has been sufficiently internalised. This is a measure of measurement accuracy and the extent to which the score obtained on a test is not simply due to chance. According to the literature, examinations must have a value of 0.7 or higher and, for example, selection tests must have a value of 0.9 or higher. p-value: the proportion of correct answers of the student population. The p-value is often referred to as the level of difficulty of an item, but is in fact a level of ease. For open items, an optimum of 0.5 is considered reasonable and for multiple-choice questions with four alternatives, 0.67. The a-value is also worth mentioning here. This is the proportion of students in the group that chose a specific distractor. Rit value: the correlation between the score obtained by the students on the item and the score on the test as a whole. The Rit value is a measure of the discriminating capacity of a question. This is the extent to which the item discriminates between students who have a better or worse command of the subject matter. The Rir value is a somewhat stricter measure of discriminating capacity because the influence of the item itself on discriminating capacity is omitted. According to the literature, the aim is to have a Rit value of at least 0.3. Values below 0.1 are considered poor. Negative values deserve immediate attention. This is where the Rat value comes into its own. This is the measure of the extent to which the choice of an incorrect alternative correlates to the test score. It can be valuable to store psychometric data with the items. Some assessment systems even support an automatic update of this data after each student response. This sounds attractive, were it not for the fact that they can only be interpreted for each test. You can only compare them with the data from other tests if the tests are administered under identical conditions. Psychometric data per administered test is useful. 3.7.3.2.1 Item response theory (IRT) If you want to know the more absolute level of difficulty and the discriminating capacity of items, the item-response theory (IRT) offers a good solution. IRT generates “calibrated items.” In order to generate these, the items are administered among a large sample of a group of students with widely differing knowledge and abilities. The calibration process measures very precisely how items discriminate between students at specific levels of difficulty. Item banks with items developed in this way can be used in so-called adaptive assessment systems. The underlying techniques are so complex that they can only be created when development budgets are generous enough to allow. Examples of this are the teacher training programme arithmetic test (Wiscat), the Rekentuin test (primary education) and the computer-adaptive version of the iVTG (interuniversity progress test in medicine), which is currently being developed. What is also true is that the items must meet stricter requirements than in the case of classical tests, especially in terms of discriminating capacity. (Linden, Linden, and Glas 2000) In the preparation phase, lecturers sometimes opt for adaptive testing without fully realising that this requires complex and costly IRT techniques. Only opt for this if sufficient project funding will be available. The choice of IRT does not have so many consequences for how the item bank is organised. Building an adaptive item bank requires more knowledge and resources to perform the calibration, but the result can be included in an ‘ordinary‘ item bank. Sufficient quantities of items are needed, distributed across the various levels of difficulty, but with more difficult items than items with an average level of difficulty. For a more detailed and easier to understand explanation of classical test theory and item-response theory, see De Gruijter, D. N. M. (2008). Toetsing en toetsanalyse. 3.7.4 Legal aspects that need addressing If you want to develop an item bank on behalf of your institution, either alone or in collaboration with other institutions, you will have to think about a number of legal aspects. How will you assure ownership of the items and the item bank? What arrangements do you need to make in relation to personal data protection? And in relation to forms of collaboration? How will you prevent the distribution of items? This chapter discusses these matters. If you are unsure how things work within your institution, ask for more information from your Legal Affairs department or equivalent. If you are collaborating on a cross-institutional item bank, making the necessary legal arrangements can be daunting. Be sure to engage an experienced lawyer or legal adviser. Take your time to find someone with the right expertise. 3.7.5 Ownership of items and item bank Set out clear arrangements about the rights to the items. This will create assurances that the item bank will remain available to the institution and/or the collaborative venture. Items are protected by copyright. Copyright begins when a work is created. According to the Dutch Copyright Act, it is the author’s exclusive right to publish and duplicate the work. Be aware of the following aspects: For every lecturer with an employment contract, any copyright will belong to the employer if the work is created during the performance of the teaching tasks, unless the parties have agreed otherwise. In the case of a collaborative venture: ensure that the items are the property of the collaborative venture. Spell this out explicitly in the collaboration agreement or make the items available under a Creative Commons licence. Please note: if the items are used summatively, making the items available under a CC licence may not be desirable, because you will not want everyone to be able to use the items. If one of the partners leaves the collaborative venture, arrange for the ownership of the items to remain with the collaborative venture. Set this out in the collaboration agreement. If you use images within your items, you will have to follow the law governing the use of image libraries. The copying and use of any visual material is prohibited. The institution or the collaborative venture will have pay for its images either on a per-use basis or in the form of a subscription. Tip: define an additional step within your item development process in which the item reviewer also checks whether any included image or sound recording is permissible. Establish whether any database right will be created. The Dutch Databases Act protects against copying or the repeated retrieval of data from a database without the consent of its creator. The data contained in the database is protected as a ‘collection’ by database law. If you make an investment in the database when it is created, you will automatically have the right to use the database. The original investor may also exploit the database commercially. Consult with the relevant commercial market operator about how you can regain your content in an exit scenario. What kind of tool does the market operator offer? Secure good advice if you involve a commercial operator to ensure that your interests are covered. Do not allow some other party to take the valuable content of the item bank with them if they decide to walk away. 3.7.6 Abuse of item bank resources Institutions usually want to prevent the items from ending up ‘on the street,’ especially when it comes to summative items. Tests and test items are often posted on www.studeersnel.nl, www.stuvia.nl and www.knoowy.nl. Hold these websites to account by enforcing your copyright claims. They do not own the items and are therefore not allowed to publish them. If you ask them to take the items down, they must comply. 3.7.7 The use of third-party resources in item banks You cannot simply grab videos and images from elsewhere and include them in your test items. What you can do is covered by laws and regulations. However, there are many ways to legally use existing resources, but otherwise there is the option to recreate images or videos especially for your own item bank. Make sure you allow a budget for this in your project. Check whether your own institution or the institution you wish to collaborate with has an agreement with Stichting Pro (Publication and Reproduction Organisation). This body monitors copyright within the education sector and protects the publishing houses that it represents. Stichting Pro monitors the use and reuse of publications of commercial parties by educational institutions. Institutions may choose to pay Stichting Pro an annual buy-out fee. If they choose this option, these institutions will not have to keep records of short pieces of text that are copied. If your institution does not pay a buy-out fee, you must explicitly take copyright into account. Sometimes you will have to pay for a resource that you have acquired, but not always. Below, we list the most important rules of thumb for the use of images and sound recordings within your items. The information provided on the website comes from www.auteursrechten.nl. 3.7.7.1 Linking is always okay You can link to images, videos and sound clips on the Internet. Publication on the Internet must, however, be lawful. You stream a video or sound clip directly. Downloading or presenting is prohibited unless the licence allows it. For summative tests, streaming can be problematic. Tests often uses resources that must not be publicly accessible. Moreover, you do not want to discover that the resource has disappeared from the Internet at the time of the test. 3.7.7.2 Licence for re-use Images and audiovisual works with a licence that allows reuse, such as a Creative Commons licence, can be downloaded and presented without problems. Depending on the CC licence used, the options will be broader or more limited. Therefore, always check the meaning of the licence at https://creativecommons.nl/uitleg/. 3.7.7.3 The right to cite offers many possibilities The right to cite another work makes it possible to use images or audiovisual works. You must ensure the following conditions are met: The citation serves to support the content being taught. You may not use them for decorative purposes. You must also not make any changes to the excerpt being cited. The size of the quote is in proportion to the purpose for which you are using it. In practice, this means: always use short fragments. You can ‘cite‘ images in their entirety. You must cite sources. 3.7.7.4 Images or audiovisual works from your own library] Your institution may have concluded user licenses, for example for Academia, which allow the use of images and audiovisual works within the education process of your institution without you having to request separate consent and paying a fee for each use. Check within your institution if any such licence exists. 3.7.7.5 Entire audiovisual works You may play or show an entire audiovisual work for free without permission, provided that: playback serves an educational purpose during a lesson or class and is part of the teaching programme. playback takes place physically within the educational institution. playback takes place in the context of not-for-profit education. A copy of the work may not be included in an assessment application so that students can play it at home. You must ask permission from the creator for this. 3.7.7.6 Sharing audiovisual works You may include parts of audiovisual works in a test, provided that: presentation or playback is only for explanatory purposes in the case of non-commercial education. It is therefore supplementary and not a substitute for teaching. the presentation takes place in a confined environment, to which only students have access. If students have to log in for the test, you will already meet this condition. equitable remuneration is paid to the rights holders. You must contact them about this. The presentation does not have to take place physically in the classroom. Students may view or play the works at home. 3.7.7.7 Attribution If you use third-party resources for which you have to pay, you must contact the rights holders. Even if you will be recreating a resource. Contact can be made through the following organisations: VIDEMA BUMA/STEMRA PICTORIGHT More information: https://www.auteursrechten.nl/en http://www.onderwijsenauteursrecht.nl/ https://IUSmentis.com https://creativecommons.nl/ 3.7.8 Personal data protection Under the General Data Protection Regulation (GDPR), you need data processing agreements to safeguard the privacy of lecturers, employees and students. As soon as employees or students create an account for, for example, the assessment application and the institution or supplier will have access to this personal data, a data processing agreement must be signed between the institution and the supplier. Be aware of the following aspects: The use of software-as-a-service (SAAS) requires additional actions. The institution or the collaborative venture must make sure a data processing agreement is signed with the supplier. When logging in to the application, it will often say: ‘Terms and conditions of use apply.’ The users can the decide to accept or decline this, which guarantees the protection of privacy. The institution or the collaborative venture itself must take sufficient appropriate measures. It is imperative that arrangements with the supplier are set out in a contract and employees must know where they stand. If an application is hosted on the institution’s own servers, the institution or the collaborative venture itself will bear responsibility for the system and the data. In that case, do not forget to clearly communicate the arrangements to employees. Read more about the General Data Protection Regulation on the dedicated SURF page General Data Protection Regulation (GDPR). 3.7.9 Legal forms for collaborative ventures If you are planning to develop an item bank with other institutions, decide on the legal form and the relationships between the parties. Also decide who will deliver what and when. For example, you can set this out in the articles of association and/or a collaboration agreement. Literature "],["appendices.html", "Part 4 Appendices 4.1 Conceptual framework 4.2 Item development workflows", " Part 4 Appendices 4.1 Conceptual framework 4.1.1 Adaptive assessment A test in which the candidates have to perform assignments of varying difficulty depending on the candidate’s competence, which is estimated based on previous items. An IRT-based adaptive assessment involves calibrated items that can be assessed most accurately at a given level of competence of the candidates. In the case of a computer-based adaptive assessment, items will no longer be offered to the candidate once the level of competence has been determined at the desired level of accuracy. See also: item response theory. 4.1.2 Playback environment The component of a test system in which assessments and items are offered to the candidates and the responses are recorded. ensures that students can take the test after logging in; is often combined with a secure browser, which ensures that students cannot use unauthorised software to search the Internet, communicate with third parties, etc.; records the responses and sends them back to the item bank. 4.1.3 Authoring environment The component of an assessment system in which items can be entered. is an interface in which items can be developed and metadata added to them; offers a choice of various types of questions (multiple choice, matching questions, open questions, etc.); allows the inclusion of multimedia components; usually supports the workflow and records the review process; supports the revision of items (e.g. after the first deployment of an assessment). 4.1.4 Reliability The extent to which the score obtained on an assessment does not depend on chance. The extent to which an assessment yields the same result under identical conditions. In statistical terms, reliability is described in terms of the extent to which a measurement is free from measurement errors (also known as noise or error). See also: misclassifications. 4.1.5 Body of Knowledge (BoK) A Body of Knowledge (BoK) is a common knowledge base for a field of study or discipline from which a professional practitioner derives their theoretical and practical knowledge, insights and methods. This concerns not just theory, but also the proven insights and methods of the profession in question. 4.1.6 Businesscase A business case or a feasibility study describes the decision to either launch or discontinue a project or task. A business case weighs the costs against the benefits and considers the attendant risks. A business case does not intend to make any financial assessment. A good business case will explicitly also include considerations relating to quality. Aspects such as quality improvement can be very important for an institution, while the purely financial gain can be difficult to pinpoint. See also the whitepaper De businesscase van digitaal toetsen. 4.1.7 Cronbach’s alpha A psychometric measure used for non-repeated assessments (i.e. a one-off assessment such as an examination) that reflects the extent to which the measurement is subject to measurement errors. Cronbach’s alpha is suitable for polytomous scored items. Questions to which, in addition to correct-incorrect (0 or 1), points can also be given, for example 0, 1, 2, 3, etc. See also: KR20. 4.1.8 Diagnostic assessments (a subset of formative assessment) A formative assessment in which a candidate gains insight into their progress through the subject matter. A diagnostic assessment is representative in terms of content and level, possibly followed by a final summative assessment. See also: practice assessments. 4.1.9 Digital assessment system This is the cyclical process of implementing and continuously improving an assessment process, starting from the learning objectives and including test items, assessment, exam administration and analysis. The development and management of the item bank is often located at the centre of this cycle. Figure 4.1: Components digital assessment systems portrayed on the assessment cycle. 4.1.10 Formative feedback Feedback in which the benefit for the learning process is paramount. Feedback is intended to encourage students to deepen their knowledge of the subject matter. When an incorrect response is given, instructions are often given on how to ascertain the correct answer or else the correct response is simply provided. If the responses are correct, in-depth resources can be provided or another example is sometimes discussed. 4.1.11 Formative assessment Assessment in which learning for the exam is paramount. Based on the literature, it is recommended that you do not follow the practice of giving grades. The learning process benefits from taking test assignments and learning from your mistakes. No effort is made to achieve a certain minimum level. Sometimes also referred to as Assessment for Learning or Assessment as Learning. 4.1.12 Parameterised question An item consisting of a fixed main structure plus some variable elements (numbers, objects, concepts, principles), so that they constitute a new item each time. Some item bank systems generate a unique new item for the lecturer or a unique set of values for each student each time. 4.1.13 IMS An IMS standard is established by the IMS Global Learning Consortium, a community of higher education institutions, suppliers and government institutions that jointly develop standards to ensure interoperability. When IMS started out back in 1997, its official name was the Instructional Management Systems (IMS) project. 4.1.14 Item response theory (IRT) Item response theory (IRT) is a method based on the assumption that the probability of a candidate correctly answering an item is determined by the candidate’s skill as a function of the difficulty level of the item and the distinctiveness of the item. IRT is a theory based on the attributes of the items, while classical test theory is based on the attributes of an exam. Before you can deploy IRT in a meaningful way, these attributes must be established prior to an assessment based on a large number of administered tests among a population in which all skills levels are represented (known as calibration). See also: adaptive assessment. 4.1.15 Item An assignment/task in which the candidate must give the correct response. Also referred to as a question, test question or test item. 4.1.16 Item bank A collection of items for a specific test objective. An item bank has a certain structure, usually hierarchical and based on metadata. 4.1.17 Item bank system Software used for storing and editing item banks. Item bank systems may be separate systems or may be part of an assessment system. An assessment system will also include features to help you create assessments, administer tests, score them and carry out test analyses. 4.1.18 Classical test theory The basic assumption of classical test theory (CTT) is that the score observed in a test will consist of the actual score plus a measurement error. Values such as Cronbach’s alpha, which is an estimate of the magnitude of the measurement error, can be calculated as a function of these fundamental assumptions. 4.1.19 KR20 A psychometric measure used for non-repeated assessments (i.e. a one-off assessment such as an examination) that reflects the extent to which the measurement is subject to measurement errors. The only difference between KR20 and Cronbach’s alpha is that only dichotomous scored items, such as multiple-choice questions, can be counted in KR20. Items that are correct or incorrect and will return 0 or 1 point. See also: Cronbach’s alpha. 4.1.20 Learning analytics Learning analytics is the collection and analysis of educational data generated by students while learning online. The educational data will be converted into valuable information and can help to improve the quality of teaching. See also: https://www.surf.nl/en/expertises/learning-analytics. 4.1.21 LTI LTI (the Learning Tools Interoperability standard) is an IMS standard that facilitates the linking of assessment systems to other systems, such as learning management systems (such as Blackboard or Canvas). Lecturer and student data is automatically exchanged between the systems, assessments can be initiated from the LMS, and the test scores are fed back to the grading functions of the LMS. See also: SCORM. 4.1.22 Measurement error The measurement error in an assessment is the deviation from the score obtained on a test that cannot be explained. The actual score is the observed score less the measurement error. 4.1.23 Metadata Data added to items to describe additional attributes. This data can be used to structure the items (see: structure), search, filter and select. 4.1.24 Misclassifications The proportion of candidates who have either wrongly failed or passed an assessment. Assessments with a low degree of reliability have a high degree of misclassification. See also: reliability. 4.1.25 Practice assessments (a subset of formative assessments) An assessment in which a candidate can practice dealing with the subject matter. The candidate can choose the topics and level themselves. These tests are often used to practice dealing with concepts or ideas, especially if these are particularly difficult. Some adaptivity is sometimes built in so that the playback environment offers items to candidates at the most appropriate level. See also: diagnostic assessment. 4.1.26 Psychometric data Psychometric data refers to the data that is calculated from assessments that are administered and items that are indicative of the quality of the assessment. On the one hand, they are indicative of the reliability and the level of difficulty of assessments as a whole. Yet they are also indicative of the distinctiveness and the level of difficulty of individual items in those tests. See also: classical test theory, item response theory, Cronbach’s alpha, P-value, Rit value. 4.1.27 P-value The proportion of candidates who answered an item correctly. Also called the level of difficulty of an item. The higher the P-value, the easier the item is. The P-value is only used in classical test theory. 4.1.28 QTI QTI (Question and Test Interoperability) is an IMS standard for the interoperability of items and tests between assessment systems. 4.1.29 Rit-value The correlation between the score of the candidates on the item and the score on the test as a whole less the specific item in question. The Rir-value is a somewhat stricter measure of the distinctive capacity than the Rit-value because the Rir-value excludes the influence of the item itself on distinctive capacity. The Rir-value is only used in classical test theory. See also: Rit-value and classical test theory. 4.1.30 SCORM SCORM (Sharable Content Object Reference Model) is an IMS standard that facilitates the linking of assessment systems to other systems, such as learning management systems (like Blackboard or Canvas). Lecturer and student data, in particular, is automatically exchanged between the systems, assessments can be initiated from the LMS, and the test scores are fed back to the grading functions of the LMS. See also: LTI. 4.1.31 Stable domain A field of study that is subject to little change. See also: volatile domain. 4.1.32 Structure The way in which a collection of items is logically ordered, for example, in a tree structure. 4.1.33 Summative assessment Testing that focuses on measuring a certain level of competence as accurately as possible. The score obtained on an assessment of this type is used in the formal allocation of a study achievement, such as credits or a certificate. For example, following an exam. Sometimes also referred to as assessment or learning. 4.1.34 Test item See: item. 4.1.35 Test transparancy The entirety of aspects relating to assessment, including the degree of transparency, reliability and validity of the assessment. For instance, transparency can increase if insight is available into how items are created; reliability can increase where items are better at differentiating between students with differing degrees of competence in the subject matter or skill; validity can increase as the items better and more fully measure the intended knowledge or skill. 4.1.36 Test matrix A test matrix is a table showing how test assignments in a given test are distributed over the subject matter. The desired type of skill, such as knowledge, understanding or application, will often also be shown for each component of the subject matter. Sometimes also referred to as a test plan, specification table or blueprint. 4.1.37 Test safety The extent to which candidates provide correct responses to items and tests in a way which is compliant with the regulations (no fraud). For instance, because items have been stolen or have ended up in the public domain, or because candidates have access to unauthorised resources during the assessment, or because candidates can tamper with test results. See SURF’s publication Secure Assessment Workbook. 4.1.38 Test question See: [#item]. 4.1.39 Field of study A field of study is a field of professional knowledge and competencies in which people can specialise. The sharing of experiences and professional knowledge usually requires a specialist vocabulary. 4.1.40 Question database A system in which all items have been collected. 4.1.41 Volatile domain A field of study that is subject to many changes due to newly developing knowledge, methods or technologies. See also: stable domain. 4.1.42 Workflow A logical, fixed sequence of activities to be performed to obtain a predefined outcome. These steps may be sequential, but may also be carried out in parallel depending on the purpose of the process. 4.2 Item development workflows This appendix describes three practical examples of item development workflows. 4.2.1 An item development process by two lecturers A coordinator/examiner from the Faculty of Earth and Life Sciences at VU Amsterdam develops and maintains an item bank on the subject of Microbiology. Approximately 60 new items are created each year. The coordinator started the item bank based on the item bank of McGrawHill: Prescott’s Microbiology. The structure of the item bank follows the chapters from that book. The item bank is supplied by the publisher in Word format. The lecturer copies the items one by one to the online item bank system: QMLive. The coordinator performs a quality check during data entry: about 80-90 percent of the items are relevant and of good quality. The coordinator deletes the rest of the items. The lecturer first creates a new item in MS Word. One week before the course component is taught, the coordinator will invite the lecturer who will be teaching to review new items. The review is carried out in MS Word. The coordinator will modify the item based on the feedback from the lecturer. He or she then enters it into the item bank system. Following the test and based on the psychometric analysis, the lecturer will modify an item for future use directly in the item bank. It will take the lecturer about half an hour – excluding feedback – to develop a good-quality item. The coordinator will primarily create questions of any type, often with images, except multiple-choice questions. The time per item is relatively long. Figure 4.2: Itembank workflow 1. 4.2.2 Item development process with collaboration between universities of applied sciences Toets &amp; Leer was a collaborative venture between six universities of applied sciences (2012 – 2018) involving item banks for the subjects of Business Administration/Accounting, Business Economics, Tax Law, Marketing, Management and Law. The participating institutions provided lecturers to develop the items. They provided their time in kind. On an annual basis, they delivered 300 items per institution. The work process was as follows: A lecturer develops an item and then enters it into the item bank system. The lecturer informs a fellow lecturer from one of the other institutions, usually per group of items. The fellow lecturer checks the items and makes any changes or additions that may be needed. Any additions and comments are stored in the assessment application. The reviewing lecturer then notifies the original author, who then reviews the changes. The item is subject to testing by an external agency. If necessary, the original author receives comments or a change request. This is followed by a new check. The item in the assessment application receives final approval. Occasionally, feedback will be based on a psychometric analysis. In an ideal scenario, the process will include an editor. In this case, it took about half an hour on average to develop a good-quality item including feedback. Most items were multiple-choice questions (about 80 percent) and cloze test questions (short text or number, about 20 percent). Figure 4.3: Itembank workflow 2. 4.2.3 Item development process for MBO Knowledge Test Bank The vocational education and training schools involved provide lecturers to develop the items. Item developers are paid for four hours of item development per week. About six to ten item developers are active per item bank. The item developers work in a team to construct rough draft items with feedback, arranged according to both the qualification dossier and the Body of Knowledge (BoK). These rough draft items are circulated among multiple lecturers several times (internal validation). Labels on items clearly show which items still need to be viewed (meta tags). Additions and comments are tracked in the item bank system. Monthly face-to-face working sessions are held with assessment experts and lecturers from the participating schools. In these sessions, the teams will discuss new items for each qualification dossier. The resulting items are drafts. The item developer then modifies the draft items if needed. The resulting items are then ready for publication. Language editing will take place prior to publication. Modification is an ongoing process. Users can comment on items, and this may lead to more modifications. If major modifications are made, it is possible to resubmit the items for renewed internal and external validation. Feedback in the item bank project is only provided occasionally and is based on a psychometric analysis. On average, it will take about 1 hour – including feedback – to develop a good-quality item. Figure 4.4: Itembank workflow 3. "],["colofon.html", "Part 5 Colofon 5.1 Contributors to this revised edition: 5.2 The original publication was written by: 5.3 In close collaboration with various item bank experts: 5.4 With thanks to 5.5 Formatting 5.6 English translation 5.7 Copyright", " Part 5 Colofon This edition is an updated version of the 2018 edition. The special interest group (SIG) Digital assessment critically reviewed, supplemented and updated the edition, partly in the light of the corona crisis of 2020/2021. 5.1 Contributors to this revised edition: Silvester Draaijer, Vrije Universiteit Sharon Klinkenberg, Universiteit van Amsterdam Vincent Hendriks, Open Universiteit Annette Peet, SURF 5.2 The original publication was written by: Silvester Draaijer, Vrije Universiteit Jenny de Werk, SURFnet 5.2.1 Editor Marjolein van Trigt 5.3 In close collaboration with various item bank experts: Sander Schenk, Hogeschool Rotterdam Jane Groenendijk, Prove2Move Jeroen Donkers, Maastricht University Marjolein Wijnker, DAS Peter Roos, 10voordeleraar Angela Verschoor, CITO Wil de Groot-Bolluijt, GrootBolwerk 5.4 With thanks to Desiree Joosten-Ten Brinke, Fontys Hogescholen Ludo van Meeuwen, Technische Universiteit Eindhoven en SIG digitaal toetsen Rob Reichardt, NHL Stenden Hogeschool en SIG digitaal toetsen Kirsten Veelo, SURFnet 5.5 Formatting Rochelle Hurenkamp, Werkgroep Toetsen op Afstand 5.6 English translation Chris Hopley 5.7 Copyright Available under the Creative Commons Attribution 4.0 Netherlands License "],["literature.html", "Part 6 Literature", " Part 6 Literature "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
>>>>>>> 54b73ec513bf3c0b5707e78f8c7bf403036ef8fb
