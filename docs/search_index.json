[["index.html", "Handbook Item Banking An itembank in 5 steps Part 1 Introduction 1.1 Introduction 1.2 Definitions and concepts 1.3 An item bank in 5 easy steps 1.4 Success factors", " Handbook Item Banking An itembank in 5 steps SURF, Versnellingsplan Updated: 2022-01-08 Part 1 Introduction 1.1 Introduction Digital item banks are a wonderful tool for increasing the quality of assessments and helping reduce costs. Developing exam questions together means coordinating objectives and processes. More expertise and time will be available per item (exam question) for development and quality assurance. Taken together, this will improve the quality of the assessment. Reusing items can help reduce costs. The coronavirus crisis has also shown that item banks play a role in situations where online assessment creates risk exposure. In particular, if test questions leak out or students conspire while sitting an exam. Item banks offer the opportunity to reduce these risks. The development, management and maintenance of item banks require good project management. The aim of this publication is to show you how to set up a digital system to share items with colleagues inside and outside the institution. This handbook combines the knowledge and expertise that institutions, institutional partnerships and SURF have acquired on this topic. It helps institutions wanting to take advantage of digital item banks to get started. The handbook consists of an introduction, a step-by-step plan and an in-depth section. 1.1.1 Who is this publication for? The handbook is intended for anyone who plays a role in the processes surrounding the development, management and maintenance of an item bank and the items in it. For example: Lecturers who want to organise their test items better or who want to develop them or share them with a colleague Educational advisers who support lecturers or who are the pacemakers behind the creation of an item bank A collaborative venture wanting to share assessment resources or deploy assessments together A national consultation aimed at increasing the quality of assessment Functional administrators of an assessment system used for an item bank 1.1.2 Scope The handbook focuses on the development, management and use of item banks within a study programme, within a single institution, across institutions, within a certain domain or even nationally. The essential premise is that the processes will be the same. The handbook does not provide a comprehensive description of existing item banks or testing systems. This is available in the SURF thematic publication Theme edition: Testing and question banks in education. The development of item banks can form part of a larger digital assessment implementation process within an institution. For the implementation of digital assessment, we refer to a number of SURF publications, such as the Guidelines for digital assessment policy, the Getting started with digital testing policy in 5 steps and the Secure assessment workbook: Tools and tips for setting up a secure assessment process. This handbook is based on the assessment cycle described in the Begrippenkader voor digitaal toetsen van SURF uit 2013. The focus is on developing items (the item development process) and managing items in an item bank. The foundation for sound assessment is a good item bank. Other sources are available on how to develop sound exams within an assessment system based on an item bank, such as Toetsen in het hoger onderwijs by Van Berkel, Bax, and Joosten-ten Brinke (2017). 1.1.3 Justification This edition is an updated version of the 2018 edition. The Digital Assessment Special Interest Group (SIG) has critically reviewed, supplemented and updated the publication, partly in the light of experiences gained during the coronavirus crisis of 2020-2021. The original publication was produced in close cooperation with item bank experts from various institutions, the Digital Assessment SIG, Cito, 10voordeLeraar and Prove2Move. This handbook is based on the experiences of the experts involved in developing item banks. We also used collected resources from SURF and available literature on this topic. See the publication details for a list of all those involved who contributed to this handbook. Please feel free to contact the authors and/or the experts who contributed to the handbook for more information. 1.2 Definitions and concepts A good understanding of a number of concepts is essential for the use of this handbook. We briefly describe the components of a digital assessment system and show how they relate to the item bank. 1.2.1 What is an item bank? An item bank is a collection of items or questions (throughout this handbook, we will call them items). An item bank system is a digital system used to store and manage collections of items. An item bank system may contain multiple item banks. When we refer to item banks in this handbook, we actually mean the item bank system (the digital item bank). Definition An item bank is a collection of items for a particular assessment objective. An item bank has a certain structure, usually hierarchical and based on metadata. An item bank system is a digital system used to store and manage collections of items. An item bank system may contain multiple item banks. The questions are structured in the item bank according to a certain logic. Version management is used, and all kinds of metadata can be stored about the content, the question type, the level and other characteristics. You can also store psychometric data, such as the degree of difficulty and the discriminatory capacity of specific items. Sometimes the programmes of an item bank system will run locally on a single computer. Other item banks are deployed in web-based environments. The database and the application are accessed using cloud technology. Multiple users can work in a cloud-based system at the same time. Useful information Why are ‘test questions’ often referred to as items by assessment experts? In the 1930s, standardised tests and rudimentary databases of questions were starting to be used in the United States. Soon, a professional group emerged which developed its own associated professional terminology. ‘Test questions’ became ‘test items.’ In the Netherlands, we have now adopted the same phraseology. 1.2.2 The position of an item bank within the digital assessment system When we refer to a digital assessment system, we mean the software that facilitates the digital assessment process. We can distinguish between a number of core components: authoring environment, item bank, playback environment and analysis tool. We will discuss these components based on the assessment cycle. Not all steps in the assessment cycle are supported by digital assessment systems. Figure 1.1 shows how the components of a digital test system fit in with the assessment cycle. Figure 1.1: Components of digital assessment systems depicted in the assessment cycle. The item bank is the central component in which the individual items are stored. The editing and playback of the items take place in other systems, which then return the results to the item bank. The item bank plays a role in the following components of the assessment cycle: The design of the test (also known as the test specification) relates to aspects such as the purpose of the test, the components to be assessed, the choice of formative or summative assessment and the ‘assessment method.’ This means that no items are yet created in the design. A test is compiled using a collection of items. A specification table is usually used, commonly also known as the test matrix or test blueprint. This specifies how many items, what types of questions and what subjects of which method, knowledge or application will be included in the test. The development (construction) of the items is performed in the assessment system’s authoring environment. In the case of closed questions, the answers will be included; sample answers may sometimes also be included for open questions. Assessment software offers a playback environment for online ** test administration**. The playback environment checks the student’s identity by means of a login procedure, presents the student with a test and stores the student’s answers. The marking of the test will depend on the selected question types. Answers to closed questions are usually assessed and scored without human intervention. The item developer (the lecturer) will have included the correct answer in advance. Open questions must be marked, with systems allowing, for example, ‘blind’ marking and ‘question-by-question’ marking, possibly aided by supporting software. Some software packages included an analysis tool to perform a test analysis, but analysis can also be performed after downloading the data file and preparing it in Excel or SPSS, for example. Test analysis is often used to help determine the reliability of tests and to pinpoint items that are of dubious quality. 1.2.3 Key concepts Formative assessment: Testing in which the learning that comes from taking the test is of primary importance, usually ungraded, but often with feedback on each test question or test. The learning process benefits from taking test assignments and learning from your mistakes. There is no set minimum level that needs to be achieved. Psychometric data: This data tells us about the degree of difficulty of questions and the extent to which a student has internalised the subject matter. Psychometric data can be stored in the item bank along with the item the data relates to. Summative assessment: Testing that focuses on measuring a certain level of competence as accurately as possible. The score obtained is used for a formal award based on study performance, such as study credits or a diploma. Assessment quality: The entirety of aspects relating to assessment including the degree of transparency, reliability and validity of the assessment. For instance, transparency increases when insight is available into how items are created. Reliability increases, for example, when items discriminate more clearly between students with differing degrees of mastery of the subject matter or competence. Validity increases when the items are better at measuring the intended knowledge or competence more completely. Collaborative partnerships In this handbook, we refer to a number of cross-institutional item banks in use in the Netherlands. We are grateful that we have been able to make use of their knowledge and experience. The collaborative partnerships consulted are as follows: deKennistoetsenbank MBO For the vocational education and training (VET) programme profiles in Care and Welfare, Social Work and Nursing/Individual Care, the kennistoetsenbank MBO of Prove2Move provided item banks containing thousands of items each. Kennistoetsenbank MBO is a cross-curriculum digital system that supports learning by providing robust knowledge-testing questions. Prove2move is a cooperative venture of the three regional learning providers (Landstede, Deltion College and ROC Twente). IVTG The Interuniversity Progress Test for Medicine (iVTG) is a benchmarking instrument used to provide metrics on the knowledge progression of medical students during their studies. This is an inter-university partnership consisting of six academic teaching hospitals. 10voordeleraar The 10voordeLeraar programme was developed under the auspices of the Association of Universities of Applied Sciences (Vereniging Hogescholen) and offers national knowledge bases, mandatory knowledge tests and a peer review system for all teacher training programmes. Sharestats The aim of this project is to provide a freely accessible, comprehensive collection of learning resources on statistics. By adding metadata to the learning resources, lecturers from the professional community can determine and use a selection of statistics for their own teaching practice, as and when required. The five partners of the project team (UvA Psychology and UvA Pedagogical and Educational Sciences, VU Social Sciences (FSW), VU Behavioural and Movement Sciences (FGB), UU Social Sciences and EUR Psychology/Pedagogy) form the core of the professional community. 1.3 An item bank in 5 easy steps In practice, item banks can sometimes emerge without any deliberate plan. A small-scale trial grows into something big. An approach like this entails the risk of item bank development stagnating at a later stage, for example, due to a lack of time or as a result of personnel changes. The advice here is think things though before starting out. Plan your work and pace yourself. Racing ahead may come back to bite you later, with consequences for support or even resulting in project failure. We have identified five steps in the development and use of an item bank. Within each step, there are a number of things you need to arrange. These things will sometimes require attention at the same time, or in a different order than that described here. The result you are aiming for – a working item bank – will be further fleshed out during each step, working from the bare bones to the finished product. Part 2 of the handbook, the Step-by-Step Plan, guides you through the various steps. Sometimes, you will need to have certain theoretical knowledge, and for this reason we regularly refer to part 3 of the Handbook, the Going Deeper section. This section contains more background information about setting up an item bank. Tip If you’re short on time to study the entire Handbook, be sure to keep the success factors for developing, managing and using an item bank from Chapter 4 on hand. 1.3.1 Step-by-Step Plan 1 Preparation Formulate the objective and investigate the feasibility of the item bank. Familiarise yourself with assessment systems and the compatibility of the various system. Deepen your knowledge in terms of scale and question reproduction. Make an initial cost and benefit analysis. 2 Plan Create an action plan that describes: the purpose; the outcome (what will you have achieved by the end of the process?); how you intend to achieve this. 3 Design Organise the questions. Organise the steps in the procedure and assign roles and privileges. Decide on quality requirements and check whether the system meets them. Choose an assessment system. Record any arrangements you agree regarding scale and question production. Arrange the financial aspects. Make arrangements to give the project a legal foundation. Organise administration of the item bank. 4 Pilot Develop the items. Set up the item bank and try it out. 5 Real-life deployment Start using the item bank. Make the item bank future-proof. 1.4 Success factors Developing and using an item bank can be complex. The success factors below can help make an important contribution to the viability of an item bank. Be sure to give them due consideration, even when the collaboration is relatively straightforward. These factors run like a thread through the Handbook. Think things though before starting out: take the time at the start, estimate the feasibility as well as you can, and try to find the balance between a project-based approach and allowing things to emerge organically. Choose a method that suits your situation. Designate a project initiator and ensure that ownership is assigned correctly from the outset. Without a project initiator or arrangements relating to ownership, the risk of failure will be high. To develop the item bank, decide on what steps you want to go through and what results you want to achieve at each step. Decide on the purpose of your item bank and assign a clear role to it in the teaching practice. Stick to this plan. Communicate this vision on a regular basis. Be realistic and manage expectations: the perfect item bank will not be created overnight; in fact, it may never be perfect. Perfecting the item bank will not be a desktop exercise. Dare to try out different things. Look around you, and you’ll see that a lot has already been developed. There is no need to reinvent the wheel. Get experts involved. Support is essential. Lecturers who know and trust each other and who support the purpose of the item bank are a prerequisite for success; devote energy in getting to know each other. Put proper guidance and support in place for lecturers. The existence of a network of people already working together and sharing knowledge will significantly increase the chances of success. Start out small and try out different set-ups and metadata options. Continue to make adjustments until you find the correct set-up and method. Keep it simple and ensure it is aligned closely with daily practice. Do not underestimate the nature and extent of the work and/or the project. It is difficult material to get your head around. Changing people’s behaviour as well as their working methods will be no mean feat. Be aware of technical limitations when developing an item bank across institutional boundaries. Literature "],["step-by-step-plan-1.html", "Part 2 Step-by-step plan 2.1 Step 1: Preparation 2.2 Step 2: Action plan 2.3 Step 3: Design 2.4 Step 4: Pilot 2.5 Step 5: Real-life deployment", " Part 2 Step-by-step plan 2.1 Step 1: Preparation Start by formulating the purpose of the item bank. Consider the role of the item bank within the teaching process, phasing (where applicable) and ownership of the item bank. Who will you need? How can you gradually build an item bank that adds value to the teaching process and will stay ‘fresh’ for longer? See H3.1. Consider whether the development of an item bank is in line with the policies of the institution and whether it supports the educational vision. Does it fit in with the digital assessment policy? What are the qualitative and quantitative goals that your institution wants to achieve through digital assessment? Make sure that the overall purpose of your item bank is in line with this policy. This will ensure that the item bank stands a greater chance of success. Commitment in the form of time and money will be required before you can elaborate an idea. Each institution has different procedures for obtaining this commitment. This often involves administrative procedures in which various people will discuss things or lobby at operational, tactical and strategic levels. There are usually guidelines and procedures available for launching a project within an institution. Examples include innovation funds that can be made available through teaching awards or incentive measures. For example, you may need to formulate a project launch memo or project description to help arrange the resources. An initial cost-benefit analysis may already be needed at this stage. It is advisable to define and describe at least the following aspects while you are laying the groundwork: The current situation and the desired situation. The purpose of the item bank. What makes it a good idea? What is the benefit for students? Make clear how the item bank relates to other policy documents, such as the digital assessment policy and the teaching policy. What outcome will be visible when the project is complete? What maintenance and management will be needed in broad terms? What is the theory behind the development of the items? In the groundwork stage, decide on the theory you will be working on: classical test theory (CTT) or item response theory (IRT). What types of question do you want to use and how will feedback be processed? Decisions on feedback and question types will affect the final outcome. See also § 3.6.3.1, § 3.6.3.2, and § 3.7.3. How much time, money and lead time will be needed for the project? Work out how feasible the project is: if necessary, carry out a separate feasibility study or preliminary study . Decide who the project initiator and/or the ultimate owner of the item bank is. Decide on who you will need to help develop the item bank. See also H3.5. 2.1.1 Feasibility study A possible intermediate step in laying the groundwork for your item bank project is a feasibility study. This will explore and elaborate one or more options for organising an item bank. Choose the option that suits your situation best. Consider the following aspects: * Content: What do the items you want to include in the item bank have in common? Is there a common arrangement that can be identified? To what extent do lecturers believe they can reach a consensus on the course content and the items? Knowing this will enable you to decide on your approach. More time will be needed for this part if there are differences of opinion. See also H2.3. Quality: how would you rate the quality of the tests at the moment? What are the p-values and trip-values? How acceptable is this? What improvements do we envisage making with the item bank? The choice to buy in or develop the items yourself. What item banks are already available in the market? Check solutions available outside the institution, such as buying items from an external commercial party. * Is there an option to join an existing partnership that is already developing an item bank? The intended size of the item bank. How many items are already available for use within the institution? How many items will you need? See also H3.3. Technology (assessment systems and item bank systems): Check what item bank systems are already available within the institution and item banks that are available for use. Find out which department is responsible for the management and use of ICT applications and, specifically, assessment applications. Is there anything that the institution needs to buy? See also H3.2. Organisation: is there sufficient support for these option(s)? Talk to organisations that have experience with organising item banks. Funding: detail the costs and benefits for each option and consider the following aspects. Can sufficient time and money be made available for the development of the item bank? To what extent will the item bank be able to improve the price-quality ratio (‘value for money’) of items? Can you raise project funding or subsidies? See also H3.4. 2.1.2 Completion of groundwork stage Process all the data you have in a written project description and obtain approval from the project initiator to start writing an action plan. What will you have achieved by the end of this step? Outcome: insight into the purpose, feasibility and overall approach of the idea. A go/no go from your project initiator for the next step. 2.2 Step 2: Action plan In this step, you will decide how the item bank should look once the project is complete. You will make an action plan to achieve this result. Ask yourself: what will we have achieved by the end? The answer will depend on the purpose that the item bank will fulfil in the teaching process. Formulate the relevant requirements (hard and soft) that the item bank must meet. Hard requirements are those that absolutely must be met; soft requirements are those that people want to achieve, but are not strictly necessary to achieve a satisfactory outcome. Include both in your action plan. In some cases, an organisation may facilitate project-based working and even a specific project management method, such as Prince II. Project leaders may even be available to oversee larger projects. In that case, there will be established procedures in place within the institution regarding working methods, phasing and decision-making. Check what the institution offers in terms of project-based working. Use standard documents wherever possible, such as a template for an action plan. Set out steps from the step-by-step plan in the action plan. Make sure that your action plan describes at least the following aspects in as much detail as possible: The purpose and outcome. How you want to achieve the outcome. The role of the project initiator and who this will be. Ideally, the project initiator will be the future owner of the item bank. Plan sessions to discuss the approach and the plans with this person. Decide what you will need his or her help with. Think about who you will need during the project and once it’s complete. Do they already have the right knowledge and skills? Consider the following areas of expertise: client, project initiator, assessment expert, psychometrist, functional administrator, legal counsel and lecturers. Decide how you will create support. How will you ensure that the item bank is actually used? How will you encourage lecturers and students to put the new working methods into practice? Talk to them about this. Consider how you can ensure the importance of the item bank is placed on the tactical and strategic agendas. Who are the stakeholders? Costs and benefits: provide an estimate of what the development of the item bank will cost and what its benefits will be in the years ahead. Quality: how will you ensure you produce an item bank of sufficient quality? Timeline: what phases do you have in mind? What lead time will you need? Consider how you can achieve the objective you have set, and how you can control the process of getting there and making adjustments as needed. What will you have achieved by the end of this step? The outcome of this step is an action plan in which the purpose, the outcome and how you intend to achieve this are defined as specifically as possible. Invite the project initiator to approve the action plan. The action plan ensures that the time that people will need has been agreed with their superiors. This will ensure commitment and support. 2.3 Step 3: Design By the end of this step, you will have a design that makes it clear to the project initiator what deliverable they can expect to receive. What’s more, the employees involved in the next step, testing in a pilot, will know what needs to be done. The following aspects are defined and described in the design: Arrangement of items Item development process Item quality System Production and planning Funding Legal aspects that need addressing Management Clearly document the arrangements that are agreed for each focus item. The outcomes of each component add up to the overall design of the item bank and are therefore also interrelated. 2.3.1 Arrangement of items Map out the extent of commonality between the disciplines. Invite the lecturers concerned to decide whether and, if so, to what extent they view a subject or discipline in the same way. They will work together in-depth to decide what is included in the item bank at the subject level and the question construction level. This phase is a difficult one because it is here where differences will come to light. It is important to remain pragmatic. Accept that some adjustments may be necessary in the teaching process, for example, you may have to choose a new teaching method or modify the use of specific names for variables or concepts. Questions that come up: How do you describe the items? What do you consider relevant or programme-specific? What do the test matrices look like? There may be differences in notation or illustration. How do you deal with this? Decide what hierarchical structure the item bank will have and what metadata is desirable. See also H3.6. 2.3.2 Item development process Decide who will run through which steps in the item development process to ensure the assessment quality. Prepare an estimate of the expected development time per item. Consider the current process steps. Fine-tune these or expand them if necessary. Do existing items need to be reviewed and approved? How much? See also H3.5. 2.3.3 Item quality Decide on and document guidelines that items must comply with. Consider also rules on language use and citation of sources. Wherever possible, involve item experts (not everyone makes good items) or a psychometrician. Plan review sessions led by a question expert/coach. Invite lecturers to submit a large number of existing sample items. Together, discuss the variation between the disciplines, the manner in which the questions are set and the construction errors in these items. This will give the stakeholders ideas on standardising the formulation of questions and question formats. Consider how you can measure the desired quality improvement. Start by thinking about the outcome: identify the data you want to be able to retrieve from the item bank for your educational process. See also H3.7. 2.3.4 System For a simple item bank project: see which assessment system the institution uses. Does the system support the envisaged item development process, roles and privileges, hierarchical structure and metadata? For a complex item bank project: if there are multiple systems in use, decide which system is most suitable for the purpose. Does the envisaged item development process fit the system? You will no be able to calculate how much work it is to organise. If it does not fit, decide whether you can modify the process, or whether you would prefer to investigate whether the structure of the system can be modified. State what is needed to ensure the safety of the items in the bank. How scalable should the system be? Not just now, but in the future too – as far as you can tell? See also H3.2. 2.3.5 Production and planning In the groundwork phase, you will have roughly determined how many items you will need in the item bank. Convert this number into time (short and long term). How much pressure will this put on employees? How many items are already available from the various stakeholders, and can these be contributed with relative ease? Think about how you will approach filling the item bank. Determine the period, any deadlines (when will the first tests take place?) and target production by scaling up (batches). Decide when to conduct quality reviews based on assessment statistics and psychometric data. See also H3.3. 2.3.6 Funding Further elaborate the cost-benefit analysis from the previous steps. Consider how the initial development will be financed. Determine how you will fund the operation of the item bank once the project has run its course. Prepare an operating plan. Opportunities to generate revenues include: Institutions take out a subscription to the item bank, which they then offer to students free of charge. Students pay for access to the item bank. Involve the project initiator of the item bank when preparing the operating budget. Ensure that he or she feels responsible for this. See also H3.4. 2.3.7 Legal aspects that need addressing Decide who owns the items in the item bank. Think about how you want to handle third-party material that is copyrighted, such as images included with items. Decide what you need in terms of personal data protection. Discuss what arrangements you want to make with the other stakeholders about collaboration. At the very least, establish how many hours will be provided for item development. Engage a legal expert in inter-institutional collaborative partnerships in order to assess what form of collaboration will work best and what will be needed to achieve this. See also [H3.7.4](#legal-aspects-that-need-addressing]. 2.3.8 Item bank management Think about how you can best support the users, such as lecturers and students, with getting the most from the final product. Talking to them and listening to their arguments will help here. Invite them to contribute their ideas on the design. Decide on who or what you will need to manage the item bank. Who will you need to provide support and administration once the project is concluded? Create instructional materials and plan instructional sessions. Talk to the testing and management organisation, where relevant, to coordinate the working method. See also H3.5. What will you have achieved by the end of this step? A design for the item bank. All interim outcomes for each component will add up to the overall design of the item bank: Structure: a map/table with the agreed subject structure and arrangements regarding notation methods, for instance. Organisation: approach and documented organisational arrangements (description of the testing and administration organisation) on how the item bank will be managed, including governance of the item bank, including the necessary knowledge and skills of the parties concerned (a training plan). System: organisational plan system; this concerns the workflow (of the item development process) and the meta structure (hierarchical structure and metadata structure). Quality: agreed arrangements on the envisaged quality of the items, including a description of guidelines and criteria that the items must comply with. description of the working procedure for item development, including management, analysis and roles and privileges. an indication of the time it will take to develop an item. an indication of the increase in and consistency of the item quality. Production and planning: approach and documented organisational arrangements on how to ‘fill‘ the item bank and keep it up to date. Financial: a business case and operating plan. Legal: agreed arrangements on the legal aspects of the items and the system 2.4 Step 4: Pilot The design is complete. Now set up the item bank and prepare it for use. In this step, you will set up an item bank structure, enter metadata categories, create accounts, convert existing items and assign roles and privileges. Run a pilot in which you are fully operational for an initial assessment period. In the pilot, you will be testing whether the chain of activities works. Create enough items so that you can cover the initial assessment period. Check whether the agreed arrangements are feasible in practice. Are the assumptions correct? Does everyone know what roles and privileges they have? Test whether the systems and the method work from a technical perspective. Are the items used by students as intended? Is formative assessment used? Evaluate and adjust as necessary. The following activities will be carried out during the pilot: Set up the item bank from a technical perspective and test it. If old items are to be converted to the new situation: perform the conversion with a small number of items. Train employees. Enter a first batch of items based on the agreed working method Prepare a test using the available items. Offer the test in a course or module. Evaluate the use and results of the test. Establish an administration system for people and funding (for example, if a fee per constructed item or for coordinating role has been chosen). Evaluate the use and procedures for creating items. Is there a better way to control quality? Are the assumptions correct? For example, the estimated time for question development? Where necessary: make adjustments, fine-tune the arrangements made, modify the assumptions. What will you have achieved by the end of this step? A documented evaluation of the pilot in terms of the various design aspects and from the perspective of the users, and an improvement plan. An established and tested item bank. 2.5 Step 5: Real-life deployment In this step, you will use the item bank in real-life situations. You will also take action to increase the use of the item bank. The pilot will have generated a number of lessons. Based on these, you will be able to scale up. During the real-life deployment, larger numbers of items will be created, and more and more tests will be deployed. Points to consider: Ensure that the item bank owner continues to follow and monitor the objectives and outcomes. Regularly discuss the topic of embedding with the item bank owner. Are the item developers creating sufficient items? Are the items of good quality? How can you ensure the least possible variation in quality? Is everyone satisfied with the process? Is it easy for lecturers to create tests? Do students consider the formative items developed in this way (including feedback) appealing? Check the ongoing viability of the business case. See also H3.7. 2.5.1 Aftercare The item bank has been implemented. The process is operational, everyone is performing their tasks and using the item bank in the agreed way. There is a formal owner who monitors the purpose of the item bank. The item bank has found its place within the organisation: there are people who ensure the item bank’s continued existence and proper use. The added value for the teaching process is clear for now and in the future. Points to consider: Ensure that the maintenance of the item bank is carried out, for example, by a functional administrator and the editor(s). Ensure that the financial administration functions as intended throughout the operating phase. Keep an eye on whether operation will provide an appropriate level of funding in the future. Ensure proper documentation is available, such as: a description of how the item bank will be used in a course. a description for testing and management of how the item bank will be maintained. a description for the coordination team, where applicable, and for each team of authors for each item bank of how the item bank will be maintained. See also § 3.7.2.3. a standard instruction programme for use of the item bank system for new employees. changes to the instruction programme when the item bank system is updated or amended. "],["going-deeper.html", "Part 3 Going deeper 3.1 Why create an item bank? And why collaborate? 3.2 Technology: Systems 3.3 Item bank size 3.4 Costs and benefits 3.5 Organisation: process steps, roles and privileges 3.6 Organising an item bank 3.7 Didactics: quality of items", " Part 3 Going deeper 3.1 Why create an item bank? And why collaborate? You describe the purpose of the item bank in the preparatory phase. In this chapter, we describe a number of possible ways in which an item bank can have an impact on teaching. We will also elaborate on the benefits of collaborating on an item bank. Finally, we will examine which factors within the professional field contribute to a successful joint item bank project. 3.1.1 Why create an item bank? Decide what role the item bank will play in the learning and assessment process. An item bank is often created with the aim of having an impact on education, such as improving quality or increasing efficiency. Examples of the intended impact are: Psychometric: increasing the reliability and validity of tests. Accountability and transparency: the launch of the item bank will provide more opportunities to account for the quality of assessment in a transparent manner. For instance, it will provide a good view of the entire item bank and the match with the test matrix. Assurance: a more verifiable and less error-prone production process of items, for example, by verifiable version management of items. Sustainability: items are managed within the item bank independently of a specific lecturer. So they will not disappear when a teacher leaves. ** Test security** it is easy to produce multiple test versions and therefore prevent cheating. ** Test security** it becomes easier to develop tests in a secure way, for example, because items are no longer forwarded through email systems. Efficiency: reusing items reduces costs. Items only need to be typed once, editing and improvement can be performed much faster, and items can be more easily selected for inclusion in a test. Educational quality: reusing items allows lecturers to spend more time teaching. Educational quality: there will be more practice opportunities and opportunities for level differentiation. Educational quality: resits and better feedback opportunities can be organised more quickly. Educational quality: the study success rate will be higher, as students will be able to practice and internalise the subject matter better. Utility: investments in items that have already developed are more effectively recouped. 3.1.2 Why create an item bank? And why collaborate? There is a lot to be gained from collaboration, both within and between institutions. The joint creation and use of item banks can even lead to a significant qualitative and quantitative benefit in the short term. Why? Firstly, lecturers who jointly set up an item bank will have to enter into a dialogue based on clear terms. They decide together how they will talk about item development and what minimum quality requirements they set. This already helps to raise the standard of quality. Secondly, the use of assessment experts in the collaboration is self-evident. They can contribute to the assessment content but also, for example, in the field of test competence, editing and layout. This will also help improve the quality of the items. See § 3.7.2 about the various roles in item development. Thirdly, when you collaborate, you will agree more rules about the item development process. This means that the item developers will become more aware of the various development phases, the responsibility and assurances in the process. If item developers adhere to these rules, this will lead to an increase in quality. This will allow for better control of quality because the process is assured. Finally, collaboration can lead to lower costs. Initially, collaborating will entail more costs, but once developed items are used more often and by more lecturers – and therefore used by more students – the cost of use of the items will reduce on balance. For a further explanation, see Hoofdstuk 3.4. 3.1.3 Potential fields for successful collaboration A shared vision within the field of study could help you achieve a successful item bank more quickly. The more people within a given field agree on the classification of the subject matter, the more self-evident it will become to develop item banks together. Fields of study with a common body of knowledge (BoK), such as teacher training programmes or the medical field, are good examples of this. Subject areas in which most study programmes provide introductory courses are also suitable for organising joint item banks. Examples include Introduction to Psychology, Introduction to Sociology, Introduction to Neurobiology, and Introduction to Programming. Stable domains lend themselves well to joint item banks that can be set up with little maintenance. Classical subjects such as logic, mathematics, statistics, mechanics, accounting and the fundamentals of microbiology lend themselves well to such collaboration. Once developed, questions will stay fresh for a long time. For disciplines that are constantly developing thanks to new understanding, it will be important to maintain items. Outdated or obsolete items must be identified and deleted or updated. This process must be structured systematically in order to ensure the quality of the questionnaire. For instance, a yearly review of all items to check for freshness. When you collaborate with others, it is essential to agree who will do this and when. Examples of subject areas to which this applies are the medical field, nursing, tourism and history. Collaboration to share costs and increase quality is an interesting driver here. Subject areas that are constantly developing in terms of content (volatile domains) require an even stronger focus on the design of the item bank system and the assurance process. Think of subjects where recent legislation is always important, such as Law, where adaptation to new protocols is always happening, such as Nursing, or where work is largely thematic. Ongoing attention is required for screening, editing, deletion or supplementation. A process has to be established to deal with this. In subject areas in which large numbers of students are taught, item banks are also more suitable for collaboration, because the investment costs and maintenance costs per student will then be much lower. 3.2 Technology: Systems You will conduct research into the item bank system to be used when preparing the project. In the case of a small project, this will probably be the institution’s central assessment system. Find out within the institution which department is responsible for managing educative ICT applications and, in particular, assessment systems, and discuss your wants and needs for the item bank. First and foremost, item bank systems must facilitate: proper entry and organisation of items, as well as search and selection activities relating to quality assurance of the items. Assessment systems come in many types and sizes and offer many features that meet these requirements. The core components of a digital assessment system are the authoring environment, the item bank, the playback environment and the analysis tool. In Hoofdstuk 3.5, we will take a deeper dive into the underlying processes of these components. 3.2.1 Commonly used assessment systems in education This section provides a run-down of the most commonly used assessment systems in higher education. The list has been compiled by experts involved in the authoring of this manual. The market for assessment system suppliers is dynamic, and for this reason the list is incomplete.Commonly used assessment systems in the Netherlands: TestVision RemindoToets MapleTA Cirrus Assessment QM iQualify Ans Delft Assessment systems within Learning Management Systems with limited item bank functionality: Canvas Blackboard Desire2Learn BrightSpace Moodle Cumlaude Item bank systems (stand-alone programs) with direct item export functionality: Respondus ExamView Assessment systems with questions: SOWISO (Mathematics) Grasple (Statistics) Zeer Actieve Psychologie R/Exams (Statistics) Drillster Quizlet ProProfs Item banks by publishing houses, often corresponding to a specific textbook or online learning environment (e.g. Pearson MyLab series, (McGraw Hill methods, WebAssign etc.) :::: 3.2.2 Sharing test items between systems Not every educational institution in the Netherlands uses the same assessment software. So how can you still collaborate on item banks? It is often possible to convert simple multiple-choice questions from one system to another. For example, institution A creates the questions and institutions B and C convert them for use in their own assessment system. Ideally, however, you will work in the same authoring environment, follow a joint item development process, and all the questions and data about the created items will flow back to a single item bank. 3.2.2.1 QTI Standard In recent years, research has been carried out in SURF’s own network into interoperability and the ability to share questions between the various systems. At the international level, development of the IMS QTI standard (Question and Test Interoperability)(https://www.imsglobal.org/question/index.html) is ongoing. This standard will not solve all interchange issues in the short term, because some systems have question types that others do not, such as pointing to multiple points in an image or unequal matching questions. At the same time, QTI already works in many cases for multiple-choice tests. In practice, therefore, a more complex collaborative venture between multiple institutions means that most of the participating partners would have to work in a system other than their home system. If you cannot work in a single item bank system during the collaboration, limit the question type to be used to the multiple-choice question. This form is so simple that exchange via QTI or plain text usually runs without a hitch. Assume that metadata and media (images, sound) will not transfer well with you when you export or import items. During the preparation, perform an item interchange test. This will allow you to establish whether the transfer is working or whether conversion tools need to be created. Try to use the same system wherever possible, even when working across multiple institutions. 3.2.2.2 LTI and SCORM standard Assessment professionals often use the IMS LTI (Learning Tools Interoperability) standard and SCORM (Sharable Content Object Reference Model) standards. However, these are not item interchange standards. These standards are used to link assessment systems to other systems, such as learning management systems (such as Blackboard or Canvas). Lecturer and student data, in particular, is automatically exchanged between the systems, assessments can be initiated from the LMS, and the test scores are fed back to the grading functions of the LMS. This creates a more distinctive experience of the LMS for the end-users and they do not need any extra logins. 3.2.3 Security and scalability of item banks Summative items, in particular, must be developed and stored in item banks in secure conditions. The greater the importance of the tests, the more thought needs to be given to security. You also want to prevent the distribution of items during the administration of exams, especially where questions have been calibrated and item banks are small. When administrating exams by computer, it is important to use a protected environment (secure browser), but also to check carefully for mobile phones, glasses, watches and other devices that may be able to take pictures of the screen or paper. Professional invigilators remain necessary. At the same time, during the coronavirus period of 2020-2021, a huge leap was made in administrating exams and tests using online invigilation (also widely known as online proctoring). Online proctoring allows students to be monitored remotely by collecting images of the student – using his or her webcam – and of the screen. For more information, see SURF’s publication Secure Assessment Workbook. Scalability is closely related to the desired size of the item bank. Be sure to consider the addition of any external authors, the performance of the authoring environment, the possible growth of the number of questions and subjects to be added, and metadata. 3.3 Item bank size How do you decide on the number of items in an item bank? During the preparation, you will already be giving some thought to the required size, because this will give you some idea of the amount of time and the resources you will need to create the item bank. Sometimes impressive numbers of items are quoted, but the required size actually depends on many factors. First of all, you will need to know how many items you want to include in a test. Can you and are you allowed to reuse items? Do you want to use the items in a summative or formative test? This can make a difference for the required number. In the sections below, we outline scenarios for various tests in which you might want to use the items. 3.3.1 Summative tests conducted simultaneously for all students Most tests in higher education are conducted as classical examinations. One lecturer or team puts together a test for a group of candidates for one course from a single institution. All students sit the exam at the same time. What will the situation be then? Test questions released Test questions not released Let’s say that the test questions will be released following the test, that you compile three different versions of the test and that the test consists of fifty items. In that case, you will need an item bank whose size will have to increase by 150 items for each test. After five years, there will be 750 items in the item bank. Institutions often stipulate in their assessment policy the rules relating to what is and what is not permitted with regard to the reuse of items. Let’s say that the test questions will not be released following the test, that you compile three different versions of the test and that the test consists of fifty items. In this case, an item bank of 150 items will be sufficient. Let’s say that the test questions will be released following the test, that the position of a question in the test is randomly selected from one of two positions, and that the test consists of fifty items. In that case, you will need an item bank whose size will have to increase by 100 items for each test. After five years, you will have an item bank of 500 items (Draaijer and Klinkenberg 2015). Let’s say that the test questions will not be released following the test, that the position of each question in the test is randomly selected from one of two positions, and that the test consists of fifty items. In this case, an item bank of 100 items will be sufficient. 3.3.2 Summative tests that can be taken at any time There are also tests that are administered whenever requested by the student. Here, there is a risk that students will collect items and share them among their peers. Or that students will enrol for the test multiple times in the hope of answering the same items to which they then know the correct answer. Some people argue that it is not necessarily a bad thing if test questions are known, as long as the item bank is large enough. The reasoning is that if students can practice using all the items, they will learn the subject matter by themselves. However, there will always be a type of student who will try to memorise the correct answers so that no real knowledge or understanding is acquired. If they pass this test, it will impair the validity of the test. How many items are needed to prevent this is impossible to ascertain. Nevertheless, item banks holding at least 1,000 to 2,000 items would seem necessary. In all cases, it is advisable to make sure that students answer new questions. A minimal variant is that alternatives from multiple-choice questions do vary in terms of content or their position in the test. In addition, parameterised test items could be used. These are questions in which numbers, objects or concepts are drawn from a collection (Fattoh 2014). At the start of the Dutch Open University’s item bank project, the guideline in use was that there should be fifteen times as many items in the bank as the number of items in a test. This requirement comes from the three examination opportunities per year and the assumption that students would complete the study programme within five years. For lecturers, it was almost impossible to create so many items, although about fifty item banks managed to succeed in doing so. The OU had the software modified so that students could only be presented with the same item again after five times. This made it possible to reduce the requirement of fifteen times to six times, which was a relief for the lecturers. Another requirement was that the item bank would be able to produce valuable psychometric analyses after one year. This would mean that each item would have to be answered at least thirty times. For some item banks, however, the requirement of fifteen times was already far too high; for example, when only ten students sat the exam for a particular specialisation on an annual basis. 3.3.3 Formative diagnostic tests Formative diagnostic tests that represent the classical examination correspond to the sitting one to three representative one-off tests. In this case, it does not matter if the test questions are leaked. The reason being that it is the student’s own responsibility to take a diagnostic test seriously or not. If the idea is that students can sit a diagnostic test very frequently, the most sensible choice seems to be to determine how many tests like this you want to offer. If two are available, students will usually check the first one at the beginning of the semester to assess the desired learning outcomes, and the second one just before the summative examination to perform a self-check of whether they have mastered enough of the subject matter. The number of items required for summative tests that can be administered at any time is not relevant here. 3.3.4 Formative practice tests Formative practice tests require fewer items for each topic. First and foremost, the student will need good instructional items on key, difficult topics. For each topic, twenty or so items will probably suffice. If a subject area consists of twenty topics, approximately 400 items will then be required. 3.4 Costs and benefits During the preparation, you will assess the financial feasibility of your project. You will make an overall estimate of the expected investment in the item bank in terms of investment costs and running costs. In addition, you will assess the quantitative and qualitative advantages or benefits that the item bank may provide. You will try to find answers to the questions ‘why invest in an item bank? ’and ‘do the benefits outweigh the costs?’ Performing a cost-benefit analysis will help you find the answers to these questions. In the case of large investment plans, a business case is something that is increasingly being requested. A business case answers the question ‘why do we actually want this?’ It underpins future decisions and helps everyone to understand why the project is important. The development of item banks can form part of a larger digital assessment implementation process within an institution. This will often be based on a business case. The costs and benefits of a proposed item bank will form one of the components of the business case. 9 For more information on the business case, see the publication Theme edition: Testing and question banks in education, Jan 2017 (chapter on Costs and Benefits of an Item Bank, page 12; only available in Dutch). 3.4.1 Costs Developing an item bank will require an investment. These are the project costs. Subsequently, you will need money for running costs and for ongoing development and administration. 3.4.1.1 Project costs The initial investment consists of creating a collaborative venture group, developing a plan, creating the organisation, consulting and negotiating on the structure of the item bank and its working methods, licencing and organising the administration of an item bank system, developing an initial number of items for the item bank and conducting the first tests using the items contained in the item bank. Different time budgets are used for the development of items. For simple knowledge questions in a multiple-choice format, especially if it is also easy to formulate distractors, it may sometimes be possible to develop four items per hour. Questions concerning case studies or application usually take more time to develop, up to an average of 60 minutes per question. This time can increase if an item also needs accompanying audiovisual material and feedback. If questions are to be used more than once, it is wise to review the quality after their first deployment in a test. Experience shows that after the first deployment, as many as half of the items will need to be modified. This can be a textual modification or may concern the subject matter itself. In the progress test for medical students, the development of a single item involves a large number of people. Despite this thoroughness, about one percent of the items are later found to be sub-standard in terms of content. To keep track of costs, log the hours of the project team members and multiply the total hours by their hourly rate. Include costs of external hires too, as well as the cost of resources used (such as systems, rooms, travel expenses, training, materials). Also include an amount for contingencies, which you can substantiate by referencing potential risks. A number of opportunities to finance the development of more complex item banks: ‘Closed exchanges’ are used for the development of the item banks. Decide who will create and review how many items and the time periods for doing so. No invoices will be sent between the team members. Item developers receive a fee per constructed and approved item. As a result, it will not matter which lecturer or institution creates the most items. In this case, however, out-of-pocket expenses will have to be available to fund item developers. The coordinating institution or lecturer will receive a separate fee because the coordinator will spend relatively more time and will have greater responsibility. Day-to-day running Following completion of the project, the item bank will have to be maintained and further developed. During the project, you will make a plan of what this will entail. Among other things, the day-to-day running plan will stipulate that workers will be given time to continue developing items, keep the item bank live and administrate it. Assume that all items will be reviewed every couple of years, for example, to recode them due to curriculum changes, new books, etc. Remember that without a joint item bank, the cost of developing items will in many cases be untransparent. It takes time and money each time you create items on an individual basis, and the results may be of varying quality. 3.4.2 Benefits WatWhat are the benefits of the item bank? § 3.1.1 sets out the possible types of impact the item bank may have. Try to formulate these quality goals in such a way that they can be expressed in financial terms. This is often difficult and will depend on the strength of arguments. Benefits may include long-term operation, where funding is secured, for example, through a lump sum, membership contributions or funding based on test administration by students or lecturers. Map out in advance the type of funding you want to secure. Make sure that the parties commit to this as early as in the preparation stage. Benefits may also include lower costs and quality improvements. Once developed, items are used by several lecturers and, more importantly, by several students. This means that although there are costs involved in developing an item bank, on balance, the ratio between cost and quality per test taken per student is better. 3.5 Organisation: process steps, roles and privileges You will arrange the development and administration in the design of the item bank. You decide who performs which tasks and how. Insight into the global process steps associated with an item bank can help you to gain a clear overview of these tasks. 3.5.1 Components and process steps Item bank systems often consist of disparate components with related functions. Core components are usually an authoring environment, an item bank, a playback environment and an analysis tool. Process steps within each of these core components can be identified. Take, for instance, the features of the item bank. We can distinguish between processes for entering, checking and modifying items, adding metadata, organising and selection. Figure 2 depicts a number of process steps that can be taken within the various components of an assessment system. Auteursomgeving Auteur schrijft item Review door collega reviewer Redactie door toetsexpert Specialist metadateert item Gatekeeper geeft items vrij Afname omgeving Toetsbureau zet toets klaar Analysetool Docent analyseert toets met toetsexpert 3.5.2 Roles and privileges Who can access which information in the item bank? Who can modify or add something? You decide who can do what by assigning roles and privileges. In most item bank systems, the possible roles and privileges have already been defined. It is then up to you to link people to the system. Some systems make it possible to define roles yourself. Preferably, start by deciding which roles and privileges you want to be able to distinguish before you choose a system. Examples of possible roles: Administration Super user: can access all items and can set privileges for all users. Coordinator: has access to and can make changes to a sub-set of the item bank and a sub-set of the users. Item development Author: may add and modify items. Controller: may modify and comment on items. Psychometric specialist: may add and edit metadata and comment on items. Editor: checks linguistic and spelling mistakes, uniform use of language and uniform formatting of items. Illustrator/multimedia specialist: creates uniform and useful images and adds them to the items. Copyright controller: ensures that the use of images and texts is legal. Gatekeeper: performs the final check of an item. Examinations Board member or Inspection Committee member *: can view the item bank to verify that any item bank policy incorporated in a study programme plan has been implemented. External validator: an individual from outside the development team who inspects the item bank. Examples of different privileges in an item bank system: Access to all items. Access to sub-sets of the item bank. Read privileges, modify privileges, delete privileges, move privileges, add privileges, comment privileges, metadata privileges. Privileges to create tests. Privileges to stage tests ready for administration. Privileges to perform test analyses. Tip Be frugal in assigning different roles and privileges. The more complex the number of roles and privileges, the more complex the workflow, the more time and energy the editorial team will have to spend on arranging and administrating them. A lack of good administration will reduce user-friendliness and stagnate the item development process. 3.5.3 Automated workflow The current assessment systems provide working procedure support — to varying degrees — for collaboration on items. This concerns the process that an item undergoes from conception to finished product and ultimately to deletion. The way the working procedure for item development is organised is therefore already largely static. When choosing a system, make sure that several people can work in the system at the same time and can even work on the same items at the same time. In web-based assessment systems, this is usually possible by default. A system of roles and privileges makes it possible for items to be made available sequentially to different persons for revision, checking or approval. This is the automated workflow. In a workflow, you can specify that an item can only progress to the next phase once the previous phase — for example, grammar checks — has been completed. Assessment systems keep a history of each item. This provides insight into who has worked on which item and when. If any errors have taken place during the process, it is possible to roll back to an earlier situation if needed. Tip If the quality is already high on deployment: organise a more comprehensive workflow process with a variety of check events. This will optimise the opportunities for raising the quality of the items. In § 3.7.2.2 you can read more about controlled item development. 3.6 Organising an item bank If an item bank is well organised, this will help provide insight into its structure and its suitability for composing tests. Good organisation also helps with the efficient management of items. At the same time, practical experience shows that organisation alone can never produce a fully watertight system. However, it is possible to create a watertight system for a single test. If you use a single item bank to select items for tests across multiple disciplines, it will be an almost impossible task. How should an item bank be organised? Establishing what everyone understand by the term is essential to avoid confusion. Make a solid distinction between the structure of an item bank and metadata. This chapter discusses these concepts and classification principles. 3.6.1 Metadata Metadata are pieces of information that are added to items. In the case of items within an item bank, metadata will be the main topic of the question, for instance, a sub-topic of the question, the status of the item (draft, pending revision, pending editorial control, approved, rejected), the degree of difficulty or discriminating capacity, the type of question, the taxonomic designation (e.g. knowledge, application, insight), the function of the question, who created or edited the item and the tests the item is used in. Figure 3.1: An item with some metadata fields added. 3.6.2 Structure The structure of an item bank is the division of the item bank into independent, often hierarchically ordered units. This is similar to the folder structure found in the file explorer of a computer. Generally speaking, users need this clear hierarchical structure to help them understand the structure of the item bank. This structure is more useful than a structure based solely on metadata. In practice, what works best is to first choose the hierarchical structure and then add metadata to the questions. The structure and metadata are often interwoven technically in different ways depending on the item bank system. Users can use the structure and metadata separately to classify and select items, but sometimes also in combination. This makes it even more difficult to comprehend structure and metadata. Some examples of existing item bank structures: The VUmc has a separate item bank within the item bank system of Questionmark Perception. Figure 3.2: The main structure of the VUmc follows the separate course-dependent tests (CAT) that are administered within the curriculum. Figure 3.3: The main structure for an item bank on statistics at the Faculty of Social and Behavioural Sciences at VU Amsterdam. This item bank is part of a course in Blackboard. The item bank follows the subject classification within statistics with a distinction between Advanced/Basic and level of difficulty. The Kennistoetsenbank contains item banks for users of Pedagogical Work (PW), Social Care (MZ) and Nursing and Care programmes within MBO Care and Welfare. Figure 3.4: These item banks are classified according to: question types, the structure of the body of knowledge (BoK), the qualification dossier (QD) and special attributes. The image shows how the item bank of Pedagogical Work (PW) is configured. Figure 3.5: A lecturer from the Microbiology Introduction course has created an item bank in QuestionmarkLive. The structure follows exactly the structure of the textbook that he uses. Structure of the LUMC questionnaire. Leiden University Medical Centre (LUMC) has an item bank based on the RemindoToets item bank system. Figure 3.6: Initially, LUMC opted for a thematic classification based on disciplines from the national Progress Test. This structure led to problems due to the poor findability of items for lecturers and also problems with the maintenance of the questions by lecturers. Figure 3.7: A more pragmatic organisation based on programmes and curricula has now been adopted. Structure of the Toets &amp; Leer item bank. Figure 3.8: Each item bank within the RemindoToets item bank system of Toets &amp; Leer offers three categories: Subject – Learning objective – Test objective. This means that you can only expand three categories to search for a test question, i.e.: subject, learning objective and test objective. The art has been to make the list of topics not too general, but at the same time not overly detailed. 3.6.3 Use structure and metadata How should you configure the structure and metadata of a specific item bank? First of all, the structure should help to achieve the purpose of the tests – which are composed of the items – as efficiently as possible. The test matrix is authoritative here. If there is no test matrix, the starting point should be a sequence of items that students can use to practice. The structure and metadata will help you to select the items, make them findable and add them to a test. They make it possible to organise the large number of items in such a way that the user can work effectively with them and, for example, compile a test with the desired attributes. Tip For metadata, the adage ‘garbage in, garbage out’ holds true. If there is too little time or money to maintain the metadata consistently and accurately, the metadata will lose its value. In that case, it’s better to have less metadata and make sure that the quality is high. A basic principle for organising the structure and the quantity of metadata is ‘less is more.’ Maintain a level of complexity that is just enough to achieve the purpose that the tests serve and to safeguard the basic steps needed for quality assurance. Secondly, organising the metadata facilitates the process of item development and quality assurance. It is therefore important that metadata follows the form of the development process steps. See also § 3.5.1. The same applies to metadata that allows you to identify outdated items or select items from a particular author. Thirdly, you must configure the metadata in such a way that it can form the basis for meaningful reports and feedback for students. If the purpose of a specific test is for students to receive feedback on how they score on each cell of a test matrix, structure the metadata in such a way to make this possible. If this purpose has not been established in advance, do not structure it in this way. Tip If there is a broad body of knowledge (BoK) or national or international standards that apply in your field, such as CanMeds or the Interuniversity Progress Test for Medicine (iVGT), it makes perfect sense to choose this classification as your main structure. Especially if this structure is already hierarchically organised. Its use can be even more effective if it is also closely aligned with the structure of the curriculum of a study programme. If it is not closely aligned to the curriculum, which is unfortunately often the case, choosing the BoK as your main structure will create problems in practice. It is often more efficient to follow the curriculum (course) structure as your main structure and to link BoK data to this in the form of metadata. Figure 3.9: An item bank with a set of chapters as its main structure. Questions can be filtered based on different categories of metadata. Finally, accept that there are different item banks that are differently organised and have differing metadata structures within a single item bank system. Higher education institutions often seek to achieve a uniform organisation and metadata structure for all items. Practical experience shows that this is not possible. But this is nothing to worry about. Item banks are used in higher education for various purposes and across diverse subject areas and should therefore be organised in different ways with varying metadata structures. However, do try to limit the number of variants. Tip It is very difficult to devise the structure needed to achieve a particular test objective ’on paper’. The best way to devise a structure is to carry out a pilot. Develop a matrix and a test using the item bank system. Then iterate until you find the optimal solution for the given context and the purpose. Make sure that the users of the system are aware of the advantages and disadvantages of the chosen structure so that you can build an appropriate support base. There is a ‘political’ aspect to this, because subdivisions within a field of study always like to see their subdivision represented in a test with the right ‘weighting.’ Especially if you want to compare test results over an extended period of time (such as the progress tests in medical studies), it is vital that the test matrix is not changed too often. Tip Often, lecturers want to be able to stipulate that a certain question within an item bank should not appear simultaneously in a test with certain other test items. The number of possibilities that have to be reviewed to achieve this purpose will increase exponentially as the item bank grows. The ongoing procedure needed to achieve this is a manual one. It is easier to examine whether the tests can be compiled and fine-tuned in such a way that potentially interdependent items can be removed manually after the initial selection of items. Tip It makes sense to assign each item in an item bank a unique code, which itself contains the hierarchical or metadata structure. Items can sometimes be moved accidentally. After a test is administered and only raw data remains, the codes will make it possible to retrieve individual items from the bank. The unique technical ID of an item at the system level is often meaningless. An example of this code: &lt;unique no.&gt; B2_AF12-3_RegcoefT is an item from the Bachelor, second year about Chapter 12 of the textbook by Agresti and Finlay on the subject of the regression coefficient. However, decoding this requires painstaking work by those who enter and check items. It underlines the importance of findability and the use of too many or too few tags.### Feedback and question types Depending on the purpose of your item bank, you will be making a choice about the type of items and the nature of the feedback on the items. 3.6.3.1 Feedback For formative tests, developing feedback for each item is very important. Feedback can provide the student with important pointers for their study. It makes sense to check early in the project what type of feedback you want to develop. This will allow you to make an accurate assessment of the development time required for each question. Especially in the case of practice tests, it is important that substantive feedback is given for each question. Feedback must be sufficiently specific and detailed and, if possible, contain pointers for the student or references to study resources to enable the student to revisit the subject matter. There is no way that you can second-guess why students will choose an incorrect answer over a correct one. So do not even try. Formulate feedback carefully and – preferably – using neutral language. You should not demotivate students, but invite them to continue their studies. 3.6.3.2 Question types You can create items in an item bank based on a variety of question types. Most test and item bank systems support more than ten types. See, for example, this overview of all QTI-defined question types. See also § 3.2.2.1. For practice tests, it is advisable to use a variety of question types. This will increase the appeal of the tests. For instance, these question types allow you to interrogate multiple sub-topics at the same time or to avail of the benefits of computer-based test administration, such as dragging or clicking objects. It is of little consequence in the case of practice tests that each question can result in a different score. For summative and diagnostic tests, it is advisable from a statistical point of view not to use a variety of question types. Choose multiple-choice questions and give each item an equal weighting (e.g. 1 point). This choice will ensure that no problems arise in respect to different scoring options or the manner in which they are answered. This will ensure greater flexibility in being able to pull items from an item bank at random. Varying the number of alternatives (1 out of 3, 1 out of 4 or 1 out of 5 questions) should not ordinarily pose any problems, because the scores are based on 1 point for the correct answer and 0 for a distractor. In multiple choice questions, five possible answers are popular in the United States. In the Netherlands, three or four possible responses are more common. The success of a stab in the dark varies depending on the number of possible responses, but sometimes it is difficult to devise sufficient distractors. Four responses are suitable for subject areas where certain answers are clearly wrong. Examples might include exact subjects such as mathematics or biology. In fields where connections are important or where things are not always necessarily wrong, three responses to a question may be preferable. Examples include communication and most courses in psychology. Be cautious with items describing a case study, including sub-questions. A question type like this makes it more difficult to draw on flexible items because the number of sub-questions can differ per case study. This in turn affects the maximum score that can be achieved. 3.7 Didactics: quality of items Improving the quality of assessment is usually one of the main reasons for collaborating on item banks. See § 3.1.2. The quality of assessment increases in line with an increase in the quality of the items. But how does a tool like a joint item bank help increase item quality? This is what this chapter is all about. There are many definitions to describe the quality of assessment (Joosten-ten Brinke and Draaijer 2014). The quality of an item bank system refers to the extent to which the system makes it possible to develop, manage and select items for inclusion in a test in an effective, efficient and user-friendly manner. The quality of the item bank increases as the items better represent the learning objectives from the test matrix. Quality also increases as the number of high-quality items in the bank increases. 3.7.1 Quality of items If you want to improve the quality of items, focus on two types of fundamental quality in the assessment process: validity and reliability. Item banks can help bring about improvements in both. This applies both to formative assessment and to summative assessment. The validity of a test concerns the extent to which it measures what you intended it to measure. To ensure validity, it is essential that the items are relevant for a particular topic or learning objective. Quality improvement occurs when lecturers become more proficient in creating these types of items. Training and collaboration benefits this process. Item bank systems support this process by providing opportunities, for instance, to provide direct comments, by facilitating collaboration and by structuring the improvement cycle in the form of a workflow. See § 3.5.1 and 3.5.2. Reliability concerns the extent to which the assessment outcome (the score obtained on a test) is reproducible and depends as little as possible on chance. To achieve high reliability, the ‘noise‘ or ’measurement error‘ must be as small as possible. Firstly, the reliability of tests can be increased by including more items in tests. You can increase the stock of items by collaborating on item banks. The number of items that a lecturer can use to compile tests will increase. Reliability is also increased by constructing items that better discriminate between students who have either a sufficient and or a deficient grasp of the source material. Having items that discriminate better will generally also increase validity: you will measure what you set out to measure better. Formulate items from the outset as clearly, efficiently and objectively as possible to achieve a better level of discrimination. There are many manuals and lists of rules of thumb to prevent the most common causes of ambiguous items (‘noise’). See, for example, Van Berkel, Bax, and Joosten-ten Brinke (2017). 3.7.2 How do you achieve quality improvements? You improve the quality of items by reducing fluctuations in the quality of items and improving the quality of questions. This insight comes about from the subject area of operations management, as illustrated in Figure 11. In the picture on the left, you can see that the quality of a product or service is quite low on average, sometimes increasing and sometimes decreasing, but no overall rising trend. The picture on the right shows that the quality is steadily increasing, and the amount of fluctuation is decreasing. A targeted approach is needed to achieve this kind of improvement in quality. Figure 3.10: Graphical representation of quality development. Left: the quality varies a lot; the average quality is relatively low. Right: the quality becomes more constant, and the quality systematically increases. There are two ways to improve the quality of new items: either by developing a greater capacity for constructing items, or by reviewing during the construction process. Item bank systems offer workflow support to help achieve this. In this way, you can systematically improve quality. In order to improve the quality of existing items in the item bank, you can continue to carry out analyses after administrating tests and make adjustments based on interpretations of those analyses. This is in fact a cyclical process, which forms part of the maintenance and administration of an item bank. We will elaborate on this in the sections below. 3.7.2.1 Information and training In order to be able to construct high-quality items, information, training and a good feedback loop in the item development process are indispensable. When you launch an item bank, offer training to the intended item developers. This will help to develop and grow a common sense of quality. Involve assessment experts in the design of the training. Once item development is underway, as many training courses as you like can take place, for example, to coincide with editorial meetings of the author team. See also § 3.7.2.3. In fact, the in-depth discussion of items based on assessment and item analyses constitutes training, albeit non-explicit. Training is therefore an ongoing and recurring part of the maintenance and administration of the item bank. 3.7.2.2 Controlled item development by review You can identify and resolve common pitfalls or ambiguities early by following a step-by-step item development process with built-in review points. It is widely known that it always helps to have a second pair of eyes look at your work. Reviews are performed by peers or experts. You do not have to define the review process or the division of roles in great detail for two collaborating lecturers, certainly not in a technical sense. Today, however, it is required that each item has been reviewed at least once by a fellow lecturer. The approver of an item must also be clear. Do not underestimate this: the consequences of a test developed by one lecturer are the same as those of a national test. A student who fails the exam will not graduate. Item banks for high-stakes tests will require more roles and privileges. Let’s say that a selection of tests are prepared using an item bank and that no resit option will be available. Stakeholders, such as politicians, the public, administrators and student unions, set high standards for transparency and the quality of assessment. Examples include national tests, such as the teacher training programme (‘PABO’) arithmetic test, the test in the context of the ‘10voordeleraar’ teacher training programme or the central test in senior secondary vocational education (MBO). A comprehensive, transparent review process is required. For a description of possible roles in this process, see § 3.5.1. The larger the team of stakeholders, the more important it will be to have a coordinator in place to manage teams and provide project support. 3.7.2.3 Item bank maintenance and administration The maintenance and administration of the item bank includes the discussion of items that perform less well than anticipated. Psychometric analysis of these items shows that they have low Rit values or high or low proportion values. For a detailed explanation of these terms, see § 3.7.3.2. Try to work together to find out what the cause is and what you can do to improve the items. You will regularly find yourself deciding to delete items and to start afresh. These discussions may offer an opportunity to clarify the rules and guidelines for creating items within the team. Sometimes it turns out that the problem is not with the items, but rather with the teaching. In that case, you will need to examine how the teaching and the items can be better aligned with each other. Sometimes, it may be necessary to reappraise items or modify the way items are organised. For example, there may be new understanding within the subject area, changes in topics or learning objectives, or in the metadata and the assessment matrix. You may then have to ask yourself if new sets of items are needed for new topics. If items need to be recoded, issue the item developers with specific development assignments. Editorial team The team of item developers should meet regularly during the development process to monitor the progress of item development. Assign one person responsibility for the item development process. Call this person the coordinator or editor-in-chief. Agree that this person has the mandate to hold people to account in respect of the rules agreed on for item development. The editor-in-chief should schedule regular meetings with all stakeholders. During these meetings, try to foster mutual trust and discuss any problems relating to the item development process. 3.7.3 Test data The data you collected from the administered tests provides you with a powerful tool to improve item quality. Combined with the quality increase that item banks generate, you will have a double improvement. This data is an indicator of the quality of the items and the performance of students perform, or of the quality of the teaching. It is important to distinguish between descriptive and psychometric data. They have very different characters, which must be aligned with the purpose for which you are using the item bank. 3.7.3.1 Descriptive data Simple descriptive data of items concerns attributes such as the number of times an item has been used in a test or answered. Looking at a test as a whole, descriptive data can include how often a student has attempted a test, when the attempts were, how the student has scored, what the pass rates are, etc. This data comes from assessment service systems. You can translate the data into a graphical dashboard for lecturers or students. Lecturers are presented with a summary of the progress of a group of students and the topics that they struggle with. Students will gain insight into their progress and their position relative to their peers. In fact, this is pure management information. The use of data in this way is also known as learning analytics. 3.7.3.2 Psychometric data Psychometric data provides you with insight on the level of difficulty of items and the extent to which they discriminate sufficiently between students who have fully internalised the subject matter and those who have not. However, there is a difference depending on whether you are using classical test theory (CTT) or item-response theory (IRT). In this section, we briefly explain both theories. Classical Test Theory (CTT) The classical test theory assumes that the score in a test will consist of the actual score and an error in the measurement (noise). Statistical operations can be used to calculate the reliability of one test, i.e. in which range the student’s actual score lies with a certain degree of certainty. The most important psychometric data for items within the concept of classical test theory are: Reliability: the extent to which the test as a whole, i.e. all items taken together, are good at discriminating in the extent to which the subject matter has been sufficiently internalised. This is a measure of measurement accuracy and the extent to which the score obtained on a test is not simply due to chance. According to the literature, examinations must have a value of 0.7 or higher and, for example, selection tests must have a value of 0.9 or higher. p-value: the proportion of correct answers of the student population. The p-value is often referred to as the level of difficulty of an item, but is in fact a level of ease. For open items, an optimum of 0.5 is considered reasonable and for multiple-choice questions with four alternatives, 0.67. The a-value is also worth mentioning here. This is the proportion of students in the group that chose a specific distractor. Rit value: the correlation between the score obtained by the students on the item and the score on the test as a whole. The Rit value is a measure of the discriminating capacity of a question. This is the extent to which the item discriminates between students who have a better or worse command of the subject matter. The Rir value is a somewhat stricter measure of discriminating capacity because the influence of the item itself on discriminating capacity is omitted. According to the literature, the aim is to have a Rit value of at least 0.3. Values below 0.1 are considered poor. Negative values deserve immediate attention. This is where the Rat value comes into its own. This is the measure of the extent to which the choice of an incorrect alternative correlates to the test score. It can be valuable to store psychometric data with the items. Some assessment systems even support an automatic update of this data after each student response. This sounds attractive, were it not for the fact that they can only be interpreted for each test. You can only compare them with the data from other tests if the tests are administered under identical conditions. Psychometric data per administered test is useful. 3.7.3.2.1 Item response theory (IRT) If you want to know the more absolute level of difficulty and the discriminating capacity of items, the item-response theory (IRT) offers a good solution. IRT generates “calibrated items.” In order to generate these, the items are administered among a large sample of a group of students with widely differing knowledge and abilities. The calibration process measures very precisely how items discriminate between students at specific levels of difficulty. Item banks with items developed in this way can be used in so-called adaptive assessment systems. The underlying techniques are so complex that they can only be created when development budgets are generous enough to allow. Examples of this are the teacher training programme arithmetic test (Wiscat), the Rekentuin test (primary education) and the computer-adaptive version of the iVTG (interuniversity progress test in medicine), which is currently being developed. What is also true is that the items must meet stricter requirements than in the case of classical tests, especially in terms of discriminating capacity. (Linden, Linden, and Glas 2000) In the preparation phase, lecturers sometimes opt for adaptive testing without fully realising that this requires complex and costly IRT techniques. Only opt for this if sufficient project funding will be available. The choice of IRT does not have so many consequences for how the item bank is organised. Building an adaptive item bank requires more knowledge and resources to perform the calibration, but the result can be included in an ‘ordinary‘ item bank. Sufficient quantities of items are needed, distributed across the various levels of difficulty, but with more difficult items than items with an average level of difficulty. For a more detailed and easier to understand explanation of classical test theory and item-response theory, see De Gruijter, D. N. M. (2008). Toetsing en toetsanalyse. 3.7.4 Legal aspects that need addressing If you want to develop an item bank on behalf of your institution, either alone or in collaboration with other institutions, you will have to think about a number of legal aspects. How will you assure ownership of the items and the item bank? What arrangements do you need to make in relation to personal data protection? And in relation to forms of collaboration? How will you prevent the distribution of items? This chapter discusses these matters. If you are unsure how things work within your institution, ask for more information from your Legal Affairs department or equivalent. If you are collaborating on a cross-institutional item bank, making the necessary legal arrangements can be daunting. Be sure to engage an experienced lawyer or legal adviser. Take your time to find someone with the right expertise. 3.7.5 Ownership of items and item bank Set out clear arrangements about the rights to the items. This will create assurances that the item bank will remain available to the institution and/or the collaborative venture. Items are protected by copyright. Copyright begins when a work is created. According to the Dutch Copyright Act, it is the author’s exclusive right to publish and duplicate the work. Be aware of the following aspects: For every lecturer with an employment contract, any copyright will belong to the employer if the work is created during the performance of the teaching tasks, unless the parties have agreed otherwise. In the case of a collaborative venture: ensure that the items are the property of the collaborative venture. Spell this out explicitly in the collaboration agreement or make the items available under a Creative Commons licence. Please note: if the items are used summatively, making the items available under a CC licence may not be desirable, because you will not want everyone to be able to use the items. If one of the partners leaves the collaborative venture, arrange for the ownership of the items to remain with the collaborative venture. Set this out in the collaboration agreement. If you use images within your items, you will have to follow the law governing the use of image libraries. The copying and use of any visual material is prohibited. The institution or the collaborative venture will have pay for its images either on a per-use basis or in the form of a subscription. Tip: define an additional step within your item development process in which the item reviewer also checks whether any included image or sound recording is permissible. Establish whether any database right will be created. The Dutch Databases Act protects against copying or the repeated retrieval of data from a database without the consent of its creator. The data contained in the database is protected as a ‘collection’ by database law. If you make an investment in the database when it is created, you will automatically have the right to use the database. The original investor may also exploit the database commercially. Consult with the relevant commercial market operator about how you can regain your content in an exit scenario. What kind of tool does the market operator offer? Secure good advice if you involve a commercial operator to ensure that your interests are covered. Do not allow some other party to take the valuable content of the item bank with them if they decide to walk away. 3.7.6 Abuse of item bank resources Institutions usually want to prevent the items from ending up ‘on the street,’ especially when it comes to summative items. Tests and test items are often posted on www.studeersnel.nl, www.stuvia.nl and www.knoowy.nl. Hold these websites to account by enforcing your copyright claims. They do not own the items and are therefore not allowed to publish them. If you ask them to take the items down, they must comply. 3.7.7 The use of third-party resources in item banks You cannot simply grab videos and images from elsewhere and include them in your test items. What you can do is covered by laws and regulations. However, there are many ways to legally use existing resources, but otherwise there is the option to recreate images or videos especially for your own item bank. Make sure you allow a budget for this in your project. Check whether your own institution or the institution you wish to collaborate with has an agreement with Stichting Pro (Publication and Reproduction Organisation). This body monitors copyright within the education sector and protects the publishing houses that it represents. Stichting Pro monitors the use and reuse of publications of commercial parties by educational institutions. Institutions may choose to pay Stichting Pro an annual buy-out fee. If they choose this option, these institutions will not have to keep records of short pieces of text that are copied. If your institution does not pay a buy-out fee, you must explicitly take copyright into account. Sometimes you will have to pay for a resource that you have acquired, but not always. Below, we list the most important rules of thumb for the use of images and sound recordings within your items. The information provided on the website comes from www.auteursrechten.nl. 3.7.7.1 Linking is always okay You can link to images, videos and sound clips on the Internet. Publication on the Internet must, however, be lawful. You stream a video or sound clip directly. Downloading or presenting is prohibited unless the licence allows it. For summative tests, streaming can be problematic. Tests often uses resources that must not be publicly accessible. Moreover, you do not want to discover that the resource has disappeared from the Internet at the time of the test. 3.7.7.2 Licence for re-use Images and audiovisual works with a licence that allows reuse, such as a Creative Commons licence, can be downloaded and presented without problems. Depending on the CC licence used, the options will be broader or more limited. Therefore, always check the meaning of the licence at https://creativecommons.nl/uitleg/. 3.7.7.3 The right to cite offers many possibilities The right to cite another work makes it possible to use images or audiovisual works. You must ensure the following conditions are met: The citation serves to support the content being taught. You may not use them for decorative purposes. You must also not make any changes to the excerpt being cited. The size of the quote is in proportion to the purpose for which you are using it. In practice, this means: always use short fragments. You can ‘cite‘ images in their entirety. You must cite sources. 3.7.7.4 Images or audiovisual works from your own library] Your institution may have concluded user licenses, for example for Academia, which allow the use of images and audiovisual works within the education process of your institution without you having to request separate consent and paying a fee for each use. Check within your institution if any such licence exists. 3.7.7.5 Entire audiovisual works You may play or show an entire audiovisual work for free without permission, provided that: playback serves an educational purpose during a lesson or class and is part of the teaching programme. playback takes place physically within the educational institution. playback takes place in the context of not-for-profit education. A copy of the work may not be included in an assessment application so that students can play it at home. You must ask permission from the creator for this. 3.7.7.6 Sharing audiovisual works You may include parts of audiovisual works in a test, provided that: presentation or playback is only for explanatory purposes in the case of non-commercial education. It is therefore supplementary and not a substitute for teaching. the presentation takes place in a confined environment, to which only students have access. If students have to log in for the test, you will already meet this condition. equitable remuneration is paid to the rights holders. You must contact them about this. The presentation does not have to take place physically in the classroom. Students may view or play the works at home. 3.7.7.7 Attribution If you use third-party resources for which you have to pay, you must contact the rights holders. Even if you will be recreating a resource. Contact can be made through the following organisations: VIDEMA BUMA/STEMRA PICTORIGHT More information: https://www.auteursrechten.nl/en http://www.onderwijsenauteursrecht.nl/ https://IUSmentis.com https://creativecommons.nl/ 3.7.8 Personal data protection Under the General Data Protection Regulation (GDPR), you need data processing agreements to safeguard the privacy of lecturers, employees and students. As soon as employees or students create an account for, for example, the assessment application and the institution or supplier will have access to this personal data, a data processing agreement must be signed between the institution and the supplier. Be aware of the following aspects: The use of software-as-a-service (SAAS) requires additional actions. The institution or the collaborative venture must make sure a data processing agreement is signed with the supplier. When logging in to the application, it will often say: ‘Terms and conditions of use apply.’ The users can the decide to accept or decline this, which guarantees the protection of privacy. The institution or the collaborative venture itself must take sufficient appropriate measures. It is imperative that arrangements with the supplier are set out in a contract and employees must know where they stand. If an application is hosted on the institution’s own servers, the institution or the collaborative venture itself will bear responsibility for the system and the data. In that case, do not forget to clearly communicate the arrangements to employees. Read more about the General Data Protection Regulation on the dedicated SURF page General Data Protection Regulation (GDPR). 3.7.9 Legal forms for collaborative ventures If you are planning to develop an item bank with other institutions, decide on the legal form and the relationships between the parties. Also decide who will deliver what and when. For example, you can set this out in the articles of association and/or a collaboration agreement. Literature "],["appendices.html", "Part 4 Appendices 4.1 Conceptual framework 4.2 Item development workflows", " Part 4 Appendices 4.1 Conceptual framework 4.1.1 Adaptive assessment A test in which the candidates have to perform assignments of varying difficulty depending on the candidate’s competence, which is estimated based on previous items. An IRT-based adaptive assessment involves calibrated items that can be assessed most accurately at a given level of competence of the candidates. In the case of a computer-based adaptive assessment, items will no longer be offered to the candidate once the level of competence has been determined at the desired level of accuracy. See also: item response theory. 4.1.2 Playback environment The component of a test system in which assessments and items are offered to the candidates and the responses are recorded. ensures that students can take the test after logging in; is often combined with a secure browser, which ensures that students cannot use unauthorised software to search the Internet, communicate with third parties, etc.; records the responses and sends them back to the item bank. 4.1.3 Authoring environment The component of an assessment system in which items can be entered. is an interface in which items can be developed and metadata added to them; offers a choice of various types of questions (multiple choice, matching questions, open questions, etc.); allows the inclusion of multimedia components; usually supports the workflow and records the review process; supports the revision of items (e.g. after the first deployment of an assessment). 4.1.4 Reliability The extent to which the score obtained on an assessment does not depend on chance. The extent to which an assessment yields the same result under identical conditions. In statistical terms, reliability is described in terms of the extent to which a measurement is free from measurement errors (also known as noise or error). See also: misclassifications. 4.1.5 Body of Knowledge (BoK) A Body of Knowledge (BoK) is a common knowledge base for a field of study or discipline from which a professional practitioner derives their theoretical and practical knowledge, insights and methods. This concerns not just theory, but also the proven insights and methods of the profession in question. 4.1.6 Businesscase A business case or a feasibility study describes the decision to either launch or discontinue a project or task. A business case weighs the costs against the benefits and considers the attendant risks. A business case does not intend to make any financial assessment. A good business case will explicitly also include considerations relating to quality. Aspects such as quality improvement can be very important for an institution, while the purely financial gain can be difficult to pinpoint. See also the whitepaper De businesscase van digitaal toetsen. 4.1.7 Cronbach’s alpha A psychometric measure used for non-repeated assessments (i.e. a one-off assessment such as an examination) that reflects the extent to which the measurement is subject to measurement errors. Cronbach’s alpha is suitable for polytomous scored items. Questions to which, in addition to correct-incorrect (0 or 1), points can also be given, for example 0, 1, 2, 3, etc. See also: KR20. 4.1.8 Diagnostic assessments (a subset of formative assessment) A formative assessment in which a candidate gains insight into their progress through the subject matter. A diagnostic assessment is representative in terms of content and level, possibly followed by a final summative assessment. See also: practice assessments. 4.1.9 Digital assessment system This is the cyclical process of implementing and continuously improving an assessment process, starting from the learning objectives and including test items, assessment, exam administration and analysis. The development and management of the item bank is often located at the centre of this cycle. Figure 4.1: Components digital assessment systems portrayed on the assessment cycle. 4.1.10 Formative feedback Feedback in which the benefit for the learning process is paramount. Feedback is intended to encourage students to deepen their knowledge of the subject matter. When an incorrect response is given, instructions are often given on how to ascertain the correct answer or else the correct response is simply provided. If the responses are correct, in-depth resources can be provided or another example is sometimes discussed. 4.1.11 Formative assessment Assessment in which learning for the exam is paramount. Based on the literature, it is recommended that you do not follow the practice of giving grades. The learning process benefits from taking test assignments and learning from your mistakes. No effort is made to achieve a certain minimum level. Sometimes also referred to as Assessment for Learning or Assessment as Learning. 4.1.12 Parameterised question An item consisting of a fixed main structure plus some variable elements (numbers, objects, concepts, principles), so that they constitute a new item each time. Some item bank systems generate a unique new item for the lecturer or a unique set of values for each student each time. 4.1.13 IMS An IMS standard is established by the IMS Global Learning Consortium, a community of higher education institutions, suppliers and government institutions that jointly develop standards to ensure interoperability. When IMS started out back in 1997, its official name was the Instructional Management Systems (IMS) project. 4.1.14 Item response theory (IRT) Item response theory (IRT) is a method based on the assumption that the probability of a candidate correctly answering an item is determined by the candidate’s skill as a function of the difficulty level of the item and the distinctiveness of the item. IRT is a theory based on the attributes of the items, while classical test theory is based on the attributes of an exam. Before you can deploy IRT in a meaningful way, these attributes must be established prior to an assessment based on a large number of administered tests among a population in which all skills levels are represented (known as calibration). See also: adaptive assessment. 4.1.15 Item An assignment/task in which the candidate must give the correct response. Also referred to as a question, test question or test item. 4.1.16 Item bank A collection of items for a specific test objective. An item bank has a certain structure, usually hierarchical and based on metadata. 4.1.17 Item bank system Software used for storing and editing item banks. Item bank systems may be separate systems or may be part of an assessment system. An assessment system will also include features to help you create assessments, administer tests, score them and carry out test analyses. 4.1.18 Classical test theory The basic assumption of classical test theory (CTT) is that the score observed in a test will consist of the actual score plus a measurement error. Values such as Cronbach’s alpha, which is an estimate of the magnitude of the measurement error, can be calculated as a function of these fundamental assumptions. 4.1.19 KR20 A psychometric measure used for non-repeated assessments (i.e. a one-off assessment such as an examination) that reflects the extent to which the measurement is subject to measurement errors. The only difference between KR20 and Cronbach’s alpha is that only dichotomous scored items, such as multiple-choice questions, can be counted in KR20. Items that are correct or incorrect and will return 0 or 1 point. See also: Cronbach’s alpha. 4.1.20 Learning analytics Learning analytics is the collection and analysis of educational data generated by students while learning online. The educational data will be converted into valuable information and can help to improve the quality of teaching. See also: https://www.surf.nl/en/expertises/learning-analytics. 4.1.21 LTI LTI (the Learning Tools Interoperability standard) is an IMS standard that facilitates the linking of assessment systems to other systems, such as learning management systems (such as Blackboard or Canvas). Lecturer and student data is automatically exchanged between the systems, assessments can be initiated from the LMS, and the test scores are fed back to the grading functions of the LMS. See also: SCORM. 4.1.22 Measurement error The measurement error in an assessment is the deviation from the score obtained on a test that cannot be explained. The actual score is the observed score less the measurement error. 4.1.23 Metadata Data added to items to describe additional attributes. This data can be used to structure the items (see: structure), search, filter and select. 4.1.24 Misclassifications The proportion of candidates who have either wrongly failed or passed an assessment. Assessments with a low degree of reliability have a high degree of misclassification. See also: reliability. 4.1.25 Practice assessments (a subset of formative assessments) An assessment in which a candidate can practice dealing with the subject matter. The candidate can choose the topics and level themselves. These tests are often used to practice dealing with concepts or ideas, especially if these are particularly difficult. Some adaptivity is sometimes built in so that the playback environment offers items to candidates at the most appropriate level. See also: diagnostic assessment. 4.1.26 Psychometric data Psychometric data refers to the data that is calculated from assessments that are administered and items that are indicative of the quality of the assessment. On the one hand, they are indicative of the reliability and the level of difficulty of assessments as a whole. Yet they are also indicative of the distinctiveness and the level of difficulty of individual items in those tests. See also: classical test theory, item response theory, Cronbach’s alpha, P-value, Rit value. 4.1.27 P-value The proportion of candidates who answered an item correctly. Also called the level of difficulty of an item. The higher the P-value, the easier the item is. The P-value is only used in classical test theory. 4.1.28 QTI QTI (Question and Test Interoperability) is an IMS standard for the interoperability of items and tests between assessment systems. 4.1.29 Rit-value The correlation between the score of the candidates on the item and the score on the test as a whole less the specific item in question. The Rir-value is a somewhat stricter measure of the distinctive capacity than the Rit-value because the Rir-value excludes the influence of the item itself on distinctive capacity. The Rir-value is only used in classical test theory. See also: Rit-value and classical test theory. 4.1.30 SCORM SCORM (Sharable Content Object Reference Model) is an IMS standard that facilitates the linking of assessment systems to other systems, such as learning management systems (like Blackboard or Canvas). Lecturer and student data, in particular, is automatically exchanged between the systems, assessments can be initiated from the LMS, and the test scores are fed back to the grading functions of the LMS. See also: LTI. 4.1.31 Stable domain A field of study that is subject to little change. See also: volatile domain. 4.1.32 Structure The way in which a collection of items is logically ordered, for example, in a tree structure. 4.1.33 Summative assessment Testing that focuses on measuring a certain level of competence as accurately as possible. The score obtained on an assessment of this type is used in the formal allocation of a study achievement, such as credits or a certificate. For example, following an exam. Sometimes also referred to as assessment or learning. 4.1.34 Test item See: item. 4.1.35 Test transparancy The entirety of aspects relating to assessment, including the degree of transparency, reliability and validity of the assessment. For instance, transparency can increase if insight is available into how items are created; reliability can increase where items are better at differentiating between students with differing degrees of competence in the subject matter or skill; validity can increase as the items better and more fully measure the intended knowledge or skill. 4.1.36 Test matrix A test matrix is a table showing how test assignments in a given test are distributed over the subject matter. The desired type of skill, such as knowledge, understanding or application, will often also be shown for each component of the subject matter. Sometimes also referred to as a test plan, specification table or blueprint. 4.1.37 Test safety The extent to which candidates provide correct responses to items and tests in a way which is compliant with the regulations (no fraud). For instance, because items have been stolen or have ended up in the public domain, or because candidates have access to unauthorised resources during the assessment, or because candidates can tamper with test results. See SURF’s publication Secure Assessment Workbook. 4.1.38 Test question See: [#item]. 4.1.39 Field of study A field of study is a field of professional knowledge and competencies in which people can specialise. The sharing of experiences and professional knowledge usually requires a specialist vocabulary. 4.1.40 Question database A system in which all items have been collected. 4.1.41 Volatile domain A field of study that is subject to many changes due to newly developing knowledge, methods or technologies. See also: stable domain. 4.1.42 Workflow A logical, fixed sequence of activities to be performed to obtain a predefined outcome. These steps may be sequential, but may also be carried out in parallel depending on the purpose of the process. 4.2 Item development workflows This appendix describes three practical examples of item development workflows. 4.2.1 An item development process by two lecturers A coordinator/examiner from the Faculty of Earth and Life Sciences at VU Amsterdam develops and maintains an item bank on the subject of Microbiology. Approximately 60 new items are created each year. The coordinator started the item bank based on the item bank of McGrawHill: Prescott’s Microbiology. The structure of the item bank follows the chapters from that book. The item bank is supplied by the publisher in Word format. The lecturer copies the items one by one to the online item bank system: QMLive. The coordinator performs a quality check during data entry: about 80-90 percent of the items are relevant and of good quality. The coordinator deletes the rest of the items. The lecturer first creates a new item in MS Word. One week before the course component is taught, the coordinator will invite the lecturer who will be teaching to review new items. The review is carried out in MS Word. The coordinator will modify the item based on the feedback from the lecturer. He or she then enters it into the item bank system. Following the test and based on the psychometric analysis, the lecturer will modify an item for future use directly in the item bank. It will take the lecturer about half an hour – excluding feedback – to develop a good-quality item. The coordinator will primarily create questions of any type, often with images, except multiple-choice questions. The time per item is relatively long. Figure 4.2: Itembank workflow 1. 4.2.2 Item development process with collaboration between universities of applied sciences Toets &amp; Leer was a collaborative venture between six universities of applied sciences (2012 – 2018) involving item banks for the subjects of Business Administration/Accounting, Business Economics, Tax Law, Marketing, Management and Law. The participating institutions provided lecturers to develop the items. They provided their time in kind. On an annual basis, they delivered 300 items per institution. The work process was as follows: A lecturer develops an item and then enters it into the item bank system. The lecturer informs a fellow lecturer from one of the other institutions, usually per group of items. The fellow lecturer checks the items and makes any changes or additions that may be needed. Any additions and comments are stored in the assessment application. The reviewing lecturer then notifies the original author, who then reviews the changes. The item is subject to testing by an external agency. If necessary, the original author receives comments or a change request. This is followed by a new check. The item in the assessment application receives final approval. Occasionally, feedback will be based on a psychometric analysis. In an ideal scenario, the process will include an editor. In this case, it took about half an hour on average to develop a good-quality item including feedback. Most items were multiple-choice questions (about 80 percent) and cloze test questions (short text or number, about 20 percent). Figure 4.3: Itembank workflow 2. 4.2.3 Item development process for MBO Knowledge Test Bank The vocational education and training schools involved provide lecturers to develop the items. Item developers are paid for four hours of item development per week. About six to ten item developers are active per item bank. The item developers work in a team to construct rough draft items with feedback, arranged according to both the qualification dossier and the Body of Knowledge (BoK). These rough draft items are circulated among multiple lecturers several times (internal validation). Labels on items clearly show which items still need to be viewed (meta tags). Additions and comments are tracked in the item bank system. Monthly face-to-face working sessions are held with assessment experts and lecturers from the participating schools. In these sessions, the teams will discuss new items for each qualification dossier. The resulting items are drafts. The item developer then modifies the draft items if needed. The resulting items are then ready for publication. Language editing will take place prior to publication. Modification is an ongoing process. Users can comment on items, and this may lead to more modifications. If major modifications are made, it is possible to resubmit the items for renewed internal and external validation. Feedback in the item bank project is only provided occasionally and is based on a psychometric analysis. On average, it will take about 1 hour – including feedback – to develop a good-quality item. Figure 4.4: Itembank workflow 3. "],["colofon.html", "Part 5 Colofon 5.1 Contributors to this revised edition: 5.2 The original publication was written by: 5.3 In close collaboration with various item bank experts: 5.4 With thanks to 5.5 Formatting 5.6 English translation 5.7 Copyright", " Part 5 Colofon This edition is an updated version of the 2018 edition. The special interest group (SIG) Digital assessment critically reviewed, supplemented and updated the edition, partly in the light of the corona crisis of 2020/2021. 5.1 Contributors to this revised edition: Silvester Draaijer, Vrije Universiteit Sharon Klinkenberg, Universiteit van Amsterdam Vincent Hendriks, Open Universiteit Annette Peet, SURF 5.2 The original publication was written by: Silvester Draaijer, Vrije Universiteit Jenny de Werk, SURFnet 5.2.1 Editor Marjolein van Trigt 5.3 In close collaboration with various item bank experts: Sander Schenk, Hogeschool Rotterdam Jane Groenendijk, Prove2Move Jeroen Donkers, Maastricht University Marjolein Wijnker, DAS Peter Roos, 10voordeleraar Angela Verschoor, CITO Wil de Groot-Bolluijt, GrootBolwerk 5.4 With thanks to Desiree Joosten-Ten Brinke, Fontys Hogescholen Ludo van Meeuwen, Technische Universiteit Eindhoven en SIG digitaal toetsen Rob Reichardt, NHL Stenden Hogeschool en SIG digitaal toetsen Kirsten Veelo, SURFnet 5.5 Formatting Rochelle Hurenkamp, Werkgroep Toetsen op Afstand 5.6 English translation Chris Hopley 5.7 Copyright Available under the Creative Commons Attribution 4.0 Netherlands License "],["literature.html", "Part 6 Literature", " Part 6 Literature "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
